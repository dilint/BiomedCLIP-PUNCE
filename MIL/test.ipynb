{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', eps=1e-8):\n",
    "        \"\"\"\n",
    "        alpha: 类别权重（平衡正负样本，建议 0.25 用于正样本少的场景）\n",
    "        gamma: 难易样本调节因子（越大，对难样本的关注越高）\n",
    "        reduction: 'mean'/'sum'/'none'\n",
    "        eps: 数值稳定性\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # 计算概率\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Focal Weight: (1 - p_t)^gamma\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)  # p if t=1 else 1-p\n",
    "        focal_weight = (1 - p_t).pow(self.gamma)\n",
    "        \n",
    "        # Alpha 权重\n",
    "        alpha_weight = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        \n",
    "        # 组合损失\n",
    "        loss = focal_weight * alpha_weight * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    ''' Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations'''\n",
    "\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False, ft_cls=None, num_classes=9):\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        self.flag = True\n",
    "\n",
    "        self.ft_cls = ft_cls\n",
    "        self.num_classes = num_classes\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x)\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "        \n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "\n",
    "            if self.ft_cls is not None:\n",
    "                # 需要按照微调需求手动更改\n",
    "                # 根据目前的测试结果看，漏的情况的原因：1）阳性类的得分不够；2）0类的得分高了\n",
    "                \n",
    "                # 由于1和0经常比较相近，因此我们还可以考虑不对1类动手的方案\n",
    "                gamma_neg = [1.0] + [1.0] + [10.] + [10.] + [10.] + [10.] + [1.]*3\n",
    "                gamma_pos = [self.gamma_pos] * 9\n",
    "                #weights = [0.] + [1.]*5 + [0.]*4\n",
    "                weights = [0.] + [1.] + [2.]*4 + [0.]*3\n",
    "            else:\n",
    "                gamma_neg = self.gamma_neg\n",
    "                gamma_pos = self.gamma_pos\n",
    "                weights = torch.tensor([1.]*9, device=x.device)\n",
    "\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          gamma_pos * self.targets + gamma_neg * self.anti_targets)\n",
    "\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss = self.loss * self.asymmetric_w\n",
    "        \n",
    "\n",
    "        if self.ft_cls is not None and 1==1:\n",
    "            assert self.loss.shape[-1] == 10\n",
    "            if self.ft_cls == 1:\n",
    "                print(\"移除阳性类的loss\")\n",
    "                self.loss *= torch.tensor([1.] + [0.]*5 + [0.]*4).to(x.device) # 移除阳性类的loss\n",
    "            elif self.ft_cls == 2: # 移除阴性类的loss:\n",
    "                print(\"移除阴性类的loss\")\n",
    "                self.loss = self.loss*weights\n",
    "\n",
    "        return -self.loss.sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10]) torch.Size([5, 10])\n",
      "tensor(0.2845) tensor(0.8440) tensor(1.6843)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 5\n",
    "logits = torch.randn(batch_size, num_classes)\n",
    "labels = torch.randint(0, num_classes, (batch_size,))\n",
    "labels_onehot = F.one_hot(labels, num_classes).type(torch.float32)\n",
    "print(logits.shape, labels_onehot.shape)\n",
    "criterion_focal = BinaryFocalLoss(alpha=0.25, gamma=2, reduction='mean', eps=1e-8)\n",
    "criterion_bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "criterion_asl = AsymmetricLossOptimized(gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8)\n",
    "loss_focal = criterion_focal(logits, labels_onehot)\n",
    "loss_bce = criterion_bce(logits, labels_onehot)\n",
    "loss_asl = criterion_asl(logits, labels_onehot)\n",
    "print(loss_focal, loss_bce, loss_asl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Multiheadattention(nn.Module):\n",
    "    def __init__(self,input_dim ,heads,d_model,dropout=0.1):\n",
    "        super(Multiheadattention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = self.d_model // heads\n",
    "        self.heads = heads\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.q_linear = nn.Linear(self.input_dim,self.d_model)  # batch_size,seq_len,d_model\n",
    "        self.k_linear = nn.Linear(self.input_dim,self.d_model)  # batch_size,seq_len,d_model\n",
    "        self.v_linear = nn.Linear(self.input_dim,self.d_model)  # batch_size,seq_len,d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(self.d_model,d_model)\n",
    "        self.init_weights_to_one()\n",
    "\n",
    "    def init_weights_to_one(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.constant_(param, 1.0)\n",
    "    \n",
    "    def attention(self,q,k,v,d_k,mask=None,dropout=None):\n",
    "        scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(self.d_k)  # batch_size,heads,seq_len,seq_len\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill_(mask==0,-1e9)\n",
    "\n",
    "        scores = F.softmax(scores,dim=-1)  # batch_size,heads,seq_len,seq_len\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores,v)  # batch_size,heads,seq_len,d_k\n",
    "        return output\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        bs = x.size(0)\n",
    "\n",
    "        q = self.q_linear(x).view(bs,-1,self.heads,self.d_k) # batch_size,seq_len,heads,d_k\n",
    "        k = self.k_linear(x).view(bs,-1,self.heads,self.d_k) # batch_size,seq_len,heads,d_k\n",
    "        v = self.v_linear(x).view(bs,-1,self.heads,self.d_k) # batch_size,seq_len,heads,d_k\n",
    "\n",
    "        q = q.transpose(1,2)  # batch_sie,heads,seq_len,d_k\n",
    "        k = k.transpose(1,2)  # batch_sie,heads,seq_len,d_k\n",
    "        v = v.transpose(1,2)  # batch_sie,heads,seq_len,d_k\n",
    "\n",
    "        # 计算attention\n",
    "        scores = self.attention(q,k,v,self.d_k,mask,self.dropout)  # batch_size,heads,seq_len,d_k\n",
    "\n",
    "        # 连接多个头并输入到最后的线性层\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs,-1,self.d_model) # batch_size,seq_len,d_model\n",
    "\n",
    "        output = self.out(concat)  # batch_size,seq_len,d_model\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MyMultiheadattention(nn.Module):\n",
    "    def __init__(self, input_dim, heads, d_model, dropout=0):\n",
    "        super(MyMultiheadattention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(input_dim, d_model)\n",
    "        self.k_linear = nn.Linear(input_dim, d_model)\n",
    "        self.v_linear = nn.Linear(input_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.init_weights_to_one()\n",
    "\n",
    "    def init_weights_to_one(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.constant_(param, 1.0)\n",
    "\n",
    "    \n",
    "    def attention(self, q, k, v, d_k, mask=None):\n",
    "        # q [batch_size, heads, seq_len, d_model]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if self.dropout is not None:\n",
    "            scores = self.dropout(scores)\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, xs, mask=None):\n",
    "        # xs:[batch_size, seq_len, input_dim]\n",
    "        bs = xs.size(0)\n",
    "        # q, k, v [batch_size, seq_len, d_model]\n",
    "        q = self.q_linear(xs).view(bs, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(xs).view(bs, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(xs).view(bs, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # q, k, v [batch_size, heads, seq_len, d_k]\n",
    "        attn_output = self.attention(q, k, v, self.d_k, mask)\n",
    "        concat = attn_output.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyMultiheadattention2(nn.Module):\n",
    "\tdef __init__(self, input_dim, heads, d_model, dropout=0.1):\n",
    "\t\tsuper(MyMultiheadattention2, self).__init__()\n",
    "\t\tself.input_dim = input_dim\n",
    "\t\tself.heads = heads\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.d_h =  d_model // heads\n",
    "\t\t\n",
    "\t\tself.q_linear = nn.Linear(input_dim, d_model)\n",
    "\t\tself.k_linear = nn.Linear(input_dim, d_model)\n",
    "\t\tself.v_linear = nn.Linear(input_dim, d_model)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.o_linear = nn.Linear(d_model, d_model)\n",
    "\t\tself._init_weights()\n",
    "        \n",
    "\tdef _init_weights(self):\n",
    "\t\tfor param in self.parameters():\n",
    "\t\t\tnn.init.constant_(param, 1.0)\n",
    "\t\t\t\n",
    "\tdef attention(self, q, k, v, mask):\n",
    "\t\tscores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_h)\n",
    "\t\tscores = F.softmax(scores, dim=-1)\n",
    "\t\tscores = self.dropout(scores)\n",
    "\t\toutput = torch.matmul(scores, v)\n",
    "\t\treturn output\n",
    "\n",
    "\tdef forward(self, xs, mask=None):\n",
    "\t\tbs = xs.shape[0]\n",
    "\t\tq = self.q_linear(xs).view(bs, -1, self.heads, self.d_h).transpose(1,2)\n",
    "\t\tk = self.k_linear(xs).view(bs, -1, self.heads, self.d_h).transpose(1,2)\n",
    "\t\tv = self.v_linear(xs).view(bs, -1, self.heads, self.d_h).transpose(1,2)\n",
    "\t\tattn_output = self.attention(q,k,v,mask)\n",
    "\t\toutput = attn_output.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "\t\toutput = self.o_linear(output)\n",
    "\t\treturn output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904,\n",
      "        146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904,\n",
      "        146.3904, 146.3904], grad_fn=<SelectBackward0>)\n",
      "tensor([146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904,\n",
      "        146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904,\n",
      "        146.3904, 146.3904], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 8\n",
    "seq_len = 10\n",
    "heads = 2\n",
    "d_model = 16\n",
    "attention1 = Multiheadattention(input_dim, heads, d_model, dropout=0.1)\n",
    "# attention2 = MyMultiheadattention(input_dim, heads, d_model, dropout=0.1)\n",
    "attention3 = MyMultiheadattention2(input_dim, heads, d_model, dropout=0.1)\n",
    "xs = torch.randn(2, seq_len, 8)\n",
    "attention1.eval()\n",
    "# attention2.eval()\n",
    "attention3.eval()\n",
    "y1 = attention1(xs)\n",
    "# y2 = attention2(xs)\n",
    "y3 = attention3(xs)\n",
    "# print(y1.shape, y2.shape, y3.shape)\n",
    "print(y1[0][1])\n",
    "# print(y2[0][1])\n",
    "print(y3[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "1: [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "4: [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "5: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "6: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "7: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "8: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "9: [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "id2label = {\n",
    "    0: 'nilm',\n",
    "    1: 'ascus',\n",
    "    2: 'asch',\n",
    "    3: 'lsil',\n",
    "    4: 'hsil',\n",
    "    5: 'agc',\n",
    "    6: 't',\n",
    "    7: 'm',\n",
    "    8: 'bv',}\n",
    "id2labelcode = {\n",
    "    0: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    1: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    2: [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    3: [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    4: [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    5: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "    6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "    7: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "    8: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "}\n",
    "# Generate random labels (0-8) for 10 samples\n",
    "label = np.random.randint(0, 9, size=(10,))\n",
    "# Or use sequential labels: label = np.arange(9)\n",
    "\n",
    "batch_size = label.shape[0]\n",
    "\n",
    "# Convert labels to one-hot encoding using numpy\n",
    "label_onehot = np.array([id2labelcode[l] for l in label])\n",
    "\n",
    "# Print results\n",
    "for i in range(len(label_onehot)):\n",
    "    print(f'{i}: {label_onehot[i].tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11538462 -0.12       -0.12352941 -0.12631579 -0.12857143]\n",
      "4\n",
      "Optimal FPR: 0.5, Optimal TPR: 0.6, Optimal Threshold: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def optimal_thresh(fpr, tpr, thresholds, p=0):\n",
    "    fpr = np.array(fpr)\n",
    "    tpr = np.array(tpr)\n",
    "    loss = (fpr - tpr) - p * tpr / (fpr + tpr + 1)\n",
    "    idx = np.argmin(loss, axis=0)\n",
    "    print(loss)\n",
    "    print(idx)\n",
    "    return fpr[idx], tpr[idx], thresholds[idx]\n",
    "\n",
    "fpr = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "tpr = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "fpr_opt, tpr_opt, thresh_opt = optimal_thresh(fpr, tpr, thresholds, p=0.1)\n",
    "print(f\"Optimal FPR: {fpr_opt}, Optimal TPR: {tpr_opt}, Optimal Threshold: {thresh_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件1 './output-model/testgc_10k/10.1/gigapath-abmil-1xb1-bce-multi0-drop0-pad0-epoch15-cosine/error_log.txt' 中有 130 个错误切片。\n",
      "文件2 './output-model/testgc_10k/10.1/gigapath-abmil-1xb1-bce-multi0-drop1-pad1-epoch15-cosine/error_log.txt' 中有 127 个错误切片。\n",
      "------------------------------\n",
      "两个文件共有 116 个相同的错误切片，它们是:\n",
      "006234382\n",
      "115577382-4-1\n",
      "192249946001-LSIL\n",
      "C02S3100\n",
      "C12S0941\n",
      "C12S2461\n",
      "C12S2585\n",
      "C22S0047\n",
      "CO2S2708\n",
      "CX21198932-LSIL\n",
      "CX21199799-ASC-US\n",
      "CX21211519-LSIL\n",
      "CX21211535-LSIL\n",
      "CX21212550-ASC-US\n",
      "CX21216263\n",
      "F3-2401006\n",
      "HBSY-15-T20-03867\n",
      "HBSY-16-T20-07196\n",
      "HBSY-D01-T18-00436\n",
      "L2003351\n",
      "L2003584\n",
      "L2005278\n",
      "L2006717\n",
      "L2007273\n",
      "L2008694\n",
      "L2009394\n",
      "L2009462\n",
      "L2009531\n",
      "L2009653\n",
      "L2009688\n",
      "L2010199\n",
      "L2010203\n",
      "L2011363\n",
      "L2015282\n",
      "L2015424-ASC-US\n",
      "L2015853-ASC-US\n",
      "L2016387\n",
      "S0283259-28000390\n",
      "S0291497\n",
      "T-SZ230106007\n",
      "T-SZ2307080135\n",
      "T-SZ2308050092\n",
      "T220104165-LSIL\n",
      "T220108376-ASC-H\n",
      "T220109180-ASC-US\n",
      "T220111186-ASC-US\n",
      "T220112125-ASC-US\n",
      "T220120245-ASC-US\n",
      "T220225084\n",
      "T220225225\n",
      "T220226049\n",
      "T220227053\n",
      "T220328067\n",
      "T220331302\n",
      "T220427041\n",
      "T220501009\n",
      "T220521014\n",
      "T220522055\n",
      "T220528356\n",
      "T220608230\n",
      "T220617110\n",
      "T220627052\n",
      "T220628211\n",
      "T220701044\n",
      "T220705061\n",
      "T220712314\n",
      "T220712320\n",
      "T220717296\n",
      "T220720701\n",
      "T2207240260\n",
      "T220725058\n",
      "T220809417\n",
      "T220914721\n",
      "T2209200207\n",
      "T2209200341\n",
      "T2209280404\n",
      "T221014718\n",
      "T2210160799\n",
      "T2211050860\n",
      "T2211110397\n",
      "T2211160457\n",
      "T2211210668\n",
      "T221216122\n",
      "T230101206\n",
      "T230107066\n",
      "T230108546\n",
      "T230113266\n",
      "T230202244\n",
      "T230202479\n",
      "T2302280930\n",
      "T2303190259\n",
      "chuxiong20231215_LSIL-51\n",
      "chuxiong20240112_241-0003\n",
      "chuxiong20240115_241-0097-US\n",
      "lanzhou_41004_LSIL\n",
      "mk_202310part2_Tm23067385\n",
      "mk_202310part2_Tm23067435\n",
      "nantong_T202104152\n",
      "nantong_T202303034\n",
      "xCR20017795-LSIL\n",
      "xCR20017872-ASC-US\n",
      "xCR20017874-ASC-US\n",
      "xCR20017937-ASC-US\n",
      "xCR20018001-LSIL\n",
      "xCR20018034-HSIL\n",
      "xCR20018105-LSIL\n",
      "xCR20018227-ASC-US\n",
      "xCR20018233-LSIL\n",
      "xCR20018561-LSIL\n",
      "xCR20018750-LSIL\n",
      "xCR20018912-ASC-H\n",
      "xCR20018939-LSIL\n",
      "xCY20005841-HSIL\n",
      "xCY20006690-LSIL\n",
      "xCY20006842-LSIL\n",
      "xCY20007631-LSIL\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_common_errors(file1_path, file2_path):\n",
    "    \"\"\"\n",
    "    比较两个文件，找出共同的错误切片。\n",
    "\n",
    "    参数:\n",
    "    file1_path (str): 第一个文件的路径。\n",
    "    file2_path (str): 第二个文件的路径。\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_error_slices(file_path):\n",
    "        \"\"\"从单个文件中提取所有错误切片的名称。\"\"\"\n",
    "        error_slices = set()\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    # 检查行是否以 \"Error:\" 开头\n",
    "                    if line.strip().startswith('Error:'):\n",
    "                        # 使用空格分割字符串，并取第二个元素作为切片ID\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) > 1:\n",
    "                            slice_name = parts[1]\n",
    "                            error_slices.add(slice_name)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"错误：找不到文件 {file_path}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"读取文件 {file_path} 时发生错误: {e}\")\n",
    "            return None\n",
    "            \n",
    "        return error_slices\n",
    "\n",
    "    # 从两个文件中分别提取错误切片名称\n",
    "    errors1 = extract_error_slices(file1_path)\n",
    "    errors2 = extract_error_slices(file2_path)\n",
    "\n",
    "    if errors1 is None or errors2 is None:\n",
    "        print(\"因文件读取错误，无法进行比较。\")\n",
    "        return\n",
    "\n",
    "    # 使用集合的 intersection 方法找到交集\n",
    "    common_errors = errors1.intersection(errors2)\n",
    "\n",
    "    # --- 输出结果 ---\n",
    "    print(f\"文件1 '{file1_path}' 中有 {len(errors1)} 个错误切片。\")\n",
    "    print(f\"文件2 '{file2_path}' 中有 {len(errors2)} 个错误切片。\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if common_errors:\n",
    "        print(f\"两个文件共有 {len(common_errors)} 个相同的错误切片，它们是:\")\n",
    "        # 排序后输出，方便查看\n",
    "        for slice_name in sorted(list(common_errors)):\n",
    "            print(slice_name)\n",
    "    else:\n",
    "        print(\"两个文件没有相同的错误切片。\")\n",
    "\n",
    "\n",
    "# --- 使用方法 ---\n",
    "# 1. 将下面的文件名替换为您自己的文件名\n",
    "file_path_1 = './output-model/testgc_10k/10.1/gigapath-abmil-1xb1-bce-multi0-drop0-pad0-epoch15-cosine/error_log.txt'  # 例如 'left_output.txt'\n",
    "file_path_2 = './output-model/testgc_10k/10.1/gigapath-abmil-1xb1-bce-multi0-drop1-pad1-epoch15-cosine/error_log.txt' # 例如 'right_output.txt'\n",
    "\n",
    "# 2. 运行函数\n",
    "find_common_errors(file_path_1, file_path_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

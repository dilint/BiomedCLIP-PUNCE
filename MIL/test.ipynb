{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', eps=1e-8):\n",
    "        \"\"\"\n",
    "        alpha: 类别权重（平衡正负样本，建议 0.25 用于正样本少的场景）\n",
    "        gamma: 难易样本调节因子（越大，对难样本的关注越高）\n",
    "        reduction: 'mean'/'sum'/'none'\n",
    "        eps: 数值稳定性\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # 计算概率\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Focal Weight: (1 - p_t)^gamma\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)  # p if t=1 else 1-p\n",
    "        focal_weight = (1 - p_t).pow(self.gamma)\n",
    "        \n",
    "        # Alpha 权重\n",
    "        alpha_weight = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        \n",
    "        # 组合损失\n",
    "        loss = focal_weight * alpha_weight * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    ''' Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations'''\n",
    "\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False, ft_cls=None, num_classes=9):\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        self.flag = True\n",
    "\n",
    "        self.ft_cls = ft_cls\n",
    "        self.num_classes = num_classes\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x)\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "        \n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "\n",
    "            if self.ft_cls is not None:\n",
    "                # 需要按照微调需求手动更改\n",
    "                # 根据目前的测试结果看，漏的情况的原因：1）阳性类的得分不够；2）0类的得分高了\n",
    "                \n",
    "                # 由于1和0经常比较相近，因此我们还可以考虑不对1类动手的方案\n",
    "                gamma_neg = [1.0] + [1.0] + [10.] + [10.] + [10.] + [10.] + [1.]*3\n",
    "                gamma_pos = [self.gamma_pos] * 9\n",
    "                #weights = [0.] + [1.]*5 + [0.]*4\n",
    "                weights = [0.] + [1.] + [2.]*4 + [0.]*3\n",
    "            else:\n",
    "                gamma_neg = self.gamma_neg\n",
    "                gamma_pos = self.gamma_pos\n",
    "                weights = torch.tensor([1.]*9, device=x.device)\n",
    "\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          gamma_pos * self.targets + gamma_neg * self.anti_targets)\n",
    "\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss = self.loss * self.asymmetric_w\n",
    "        \n",
    "\n",
    "        if self.ft_cls is not None and 1==1:\n",
    "            assert self.loss.shape[-1] == 10\n",
    "            if self.ft_cls == 1:\n",
    "                print(\"移除阳性类的loss\")\n",
    "                self.loss *= torch.tensor([1.] + [0.]*5 + [0.]*4).to(x.device) # 移除阳性类的loss\n",
    "            elif self.ft_cls == 2: # 移除阴性类的loss:\n",
    "                print(\"移除阴性类的loss\")\n",
    "                self.loss = self.loss*weights\n",
    "\n",
    "        return -self.loss.sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10]) torch.Size([5, 10])\n",
      "tensor(0.2845) tensor(0.8440) tensor(1.6843)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 5\n",
    "logits = torch.randn(batch_size, num_classes)\n",
    "labels = torch.randint(0, num_classes, (batch_size,))\n",
    "labels_onehot = F.one_hot(labels, num_classes).type(torch.float32)\n",
    "print(logits.shape, labels_onehot.shape)\n",
    "criterion_focal = BinaryFocalLoss(alpha=0.25, gamma=2, reduction='mean', eps=1e-8)\n",
    "criterion_bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "criterion_asl = AsymmetricLossOptimized(gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8)\n",
    "loss_focal = criterion_focal(logits, labels_onehot)\n",
    "loss_bce = criterion_bce(logits, labels_onehot)\n",
    "loss_asl = criterion_asl(logits, labels_onehot)\n",
    "print(loss_focal, loss_bce, loss_asl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Multiheadattention(nn.Module):\n",
    "    def __init__(self,input_dim ,heads,d_model,dropout=0.1):\n",
    "        super(Multiheadattention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = self.d_model // heads\n",
    "        self.heads = heads\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.q_linear = nn.Linear(self.input_dim,self.d_model)  # batch_size,seq_len,d_model\n",
    "        self.k_linear = nn.Linear(self.input_dim,self.d_model)  # batch_size,seq_len,d_model\n",
    "        self.v_linear = nn.Linear(self.input_dim,self.d_model)  # batch_size,seq_len,d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(self.d_model,d_model)\n",
    "        self.init_weights_to_one()\n",
    "\n",
    "    def init_weights_to_one(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.constant_(param, 1.0)\n",
    "    \n",
    "    def attention(self,q,k,v,d_k,mask=None,dropout=None):\n",
    "        scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(self.d_k)  # batch_size,heads,seq_len,seq_len\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill_(mask==0,-1e9)\n",
    "\n",
    "        scores = F.softmax(scores,dim=-1)  # batch_size,heads,seq_len,seq_len\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores,v)  # batch_size,heads,seq_len,d_k\n",
    "        return output\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        bs = x.size(0)\n",
    "\n",
    "        q = self.q_linear(x).view(bs,-1,self.heads,self.d_k) # batch_size,seq_len,heads,d_k\n",
    "        k = self.k_linear(x).view(bs,-1,self.heads,self.d_k) # batch_size,seq_len,heads,d_k\n",
    "        v = self.v_linear(x).view(bs,-1,self.heads,self.d_k) # batch_size,seq_len,heads,d_k\n",
    "\n",
    "        q = q.transpose(1,2)  # batch_sie,heads,seq_len,d_k\n",
    "        k = k.transpose(1,2)  # batch_sie,heads,seq_len,d_k\n",
    "        v = v.transpose(1,2)  # batch_sie,heads,seq_len,d_k\n",
    "\n",
    "        # 计算attention\n",
    "        scores = self.attention(q,k,v,self.d_k,mask,self.dropout)  # batch_size,heads,seq_len,d_k\n",
    "\n",
    "        # 连接多个头并输入到最后的线性层\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs,-1,self.d_model) # batch_size,seq_len,d_model\n",
    "\n",
    "        output = self.out(concat)  # batch_size,seq_len,d_model\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MyMultiheadattention(nn.Module):\n",
    "    def __init__(self, input_dim, heads, d_model, dropout=0):\n",
    "        super(MyMultiheadattention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(input_dim, d_model)\n",
    "        self.k_linear = nn.Linear(input_dim, d_model)\n",
    "        self.v_linear = nn.Linear(input_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.init_weights_to_one()\n",
    "\n",
    "    def init_weights_to_one(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.constant_(param, 1.0)\n",
    "\n",
    "    \n",
    "    def attention(self, q, k, v, d_k, mask=None):\n",
    "        # q [batch_size, heads, seq_len, d_model]\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if self.dropout is not None:\n",
    "            scores = self.dropout(scores)\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, xs, mask=None):\n",
    "        # xs:[batch_size, seq_len, input_dim]\n",
    "        bs = xs.size(0)\n",
    "        # q, k, v [batch_size, seq_len, d_model]\n",
    "        q = self.q_linear(xs).view(bs, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(xs).view(bs, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(xs).view(bs, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # q, k, v [batch_size, heads, seq_len, d_k]\n",
    "        attn_output = self.attention(q, k, v, self.d_k, mask)\n",
    "        concat = attn_output.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyMultiheadattention2(nn.Module):\n",
    "\tdef __init__(self, input_dim, heads, d_model, dropout=0.1):\n",
    "\t\tsuper(MyMultiheadattention2, self).__init__()\n",
    "\t\tself.input_dim = input_dim\n",
    "\t\tself.heads = heads\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.d_h =  d_model // heads\n",
    "\t\t\n",
    "\t\tself.q_linear = nn.Linear(input_dim, d_model)\n",
    "\t\tself.k_linear = nn.Linear(input_dim, d_model)\n",
    "\t\tself.v_linear = nn.Linear(input_dim, d_model)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.o_linear = nn.Linear(d_model, d_model)\n",
    "\t\tself._init_weights()\n",
    "        \n",
    "\tdef _init_weights(self):\n",
    "\t\tfor param in self.parameters():\n",
    "\t\t\tnn.init.constant_(param, 1.0)\n",
    "\t\t\t\n",
    "\tdef attention(self, q, k, v, mask):\n",
    "\t\tscores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_h)\n",
    "\t\tscores = F.softmax(scores, dim=-1)\n",
    "\t\tscores = self.dropout(scores)\n",
    "\t\toutput = torch.matmul(scores, v)\n",
    "\t\treturn output\n",
    "\n",
    "\tdef forward(self, xs, mask=None):\n",
    "\t\tbs = xs.shape[0]\n",
    "\t\tq = self.q_linear(xs).view(bs, -1, self.heads, self.d_h).transpose(1,2)\n",
    "\t\tk = self.k_linear(xs).view(bs, -1, self.heads, self.d_h).transpose(1,2)\n",
    "\t\tv = self.v_linear(xs).view(bs, -1, self.heads, self.d_h).transpose(1,2)\n",
    "\t\tattn_output = self.attention(q,k,v,mask)\n",
    "\t\toutput = attn_output.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "\t\toutput = self.o_linear(output)\n",
    "\t\treturn output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904,\n",
      "        146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904,\n",
      "        146.3904, 146.3904], grad_fn=<SelectBackward0>)\n",
      "tensor([146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904,\n",
      "        146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904, 146.3904,\n",
      "        146.3904, 146.3904], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 8\n",
    "seq_len = 10\n",
    "heads = 2\n",
    "d_model = 16\n",
    "attention1 = Multiheadattention(input_dim, heads, d_model, dropout=0.1)\n",
    "# attention2 = MyMultiheadattention(input_dim, heads, d_model, dropout=0.1)\n",
    "attention3 = MyMultiheadattention2(input_dim, heads, d_model, dropout=0.1)\n",
    "xs = torch.randn(2, seq_len, 8)\n",
    "attention1.eval()\n",
    "# attention2.eval()\n",
    "attention3.eval()\n",
    "y1 = attention1(xs)\n",
    "# y2 = attention2(xs)\n",
    "y3 = attention3(xs)\n",
    "# print(y1.shape, y2.shape, y3.shape)\n",
    "print(y1[0][1])\n",
    "# print(y2[0][1])\n",
    "print(y3[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "1: [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "4: [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "5: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "6: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "7: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "8: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "9: [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "id2label = {\n",
    "    0: 'nilm',\n",
    "    1: 'ascus',\n",
    "    2: 'asch',\n",
    "    3: 'lsil',\n",
    "    4: 'hsil',\n",
    "    5: 'agc',\n",
    "    6: 't',\n",
    "    7: 'm',\n",
    "    8: 'bv',}\n",
    "id2labelcode = {\n",
    "    0: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    1: [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    2: [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    3: [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    4: [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    5: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "    6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "    7: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "    8: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "}\n",
    "# Generate random labels (0-8) for 10 samples\n",
    "label = np.random.randint(0, 9, size=(10,))\n",
    "# Or use sequential labels: label = np.arange(9)\n",
    "\n",
    "batch_size = label.shape[0]\n",
    "\n",
    "# Convert labels to one-hot encoding using numpy\n",
    "label_onehot = np.array([id2labelcode[l] for l in label])\n",
    "\n",
    "# Print results\n",
    "for i in range(len(label_onehot)):\n",
    "    print(f'{i}: {label_onehot[i].tolist()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

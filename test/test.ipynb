{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.],\n",
      "         [ 3.,  4.]],\n",
      "\n",
      "        [[ 5.,  6.],\n",
      "         [ 7.,  8.]],\n",
      "\n",
      "        [[ 9., 10.],\n",
      "         [11., 12.]]]) torch.Size([3, 2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangjialong/miniconda3/envs/dist-pu/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.return_types.max' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37915/3695196815.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.return_types.max' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "b = torch.range(1,12,1).reshape([3,2,2])\n",
    "print(b,b.shape)\n",
    "\n",
    "d = torch.max(b,dim=0,keepdim=True)\n",
    "d.view(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260,)\n",
      "(260,)\n",
      "------Weighted------\n",
      "Weighted precision 0.6314062748845358\n",
      "Weighted recall 0.5576923076923077\n",
      "Weighted f1-score 0.575114540631782\n",
      "------Macro------\n",
      "Macro precision 0.5193926846100759\n",
      "Macro recall 0.589781746031746\n",
      "Macro f1-score 0.5233019853709507\n",
      "------Micro------\n",
      "Micro precision 0.5576923076923077\n",
      "Micro recall 0.5576923076923077\n",
      "Micro f1-score 0.5576923076923077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score\n",
    "\n",
    "# create confusion matrix\n",
    "y_true = np.array([-1]*70 + [0]*160 + [1]*30)\n",
    "y_pred = np.array([-1]*40 + [0]*20 + [1]*20 + \n",
    "                  [-1]*30 + [0]*80 + [1]*30 + \n",
    "                  [-1]*5 + [0]*15 + [1]*20)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix = pd.DataFrame(cm, index=['Cat','Dog','Pig'], columns=['Cat','Dog','Pig'])\n",
    "\n",
    "print(y_true.shape)\n",
    "print(y_pred.shape)\n",
    "print('------Weighted------')\n",
    "print('Weighted precision', precision_score(y_true, y_pred, average='weighted'))\n",
    "print('Weighted recall', recall_score(y_true, y_pred, average='weighted'))\n",
    "print('Weighted f1-score', f1_score(y_true, y_pred, average='weighted'))\n",
    "print('------Macro------')\n",
    "print('Macro precision', precision_score(y_true, y_pred, average='macro'))\n",
    "print('Macro recall', recall_score(y_true, y_pred, average='macro'))\n",
    "print('Macro f1-score', f1_score(y_true, y_pred, average='macro'))\n",
    "print('------Micro------')\n",
    "print('Micro precision', precision_score(y_true, y_pred, average='micro'))\n",
    "print('Micro recall', recall_score(y_true, y_pred, average='micro'))\n",
    "print('Micro f1-score', f1_score(y_true, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = a + [4]\n",
    "b = tuple(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckp_path = '/home/huangjialong/projects/BiomedCLIP-PUNCE/mil-methods/output-model/output-test/resnet1-meanmil-ngc-customsplit/fold_0_model_best_auc.pt'\n",
    "torch.load(ckp_path)\n",
    "ckp_dict = torch.load(ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BJXFK-LA-591455', 'c00567195', 'BJXFK-XXBA-579873', 'BJXFK-XA-575822', 'c00564281', 'BJXK-XIMEA-FFK-P-599781', 'BJXFK-YX577236', 'F22', 'BJXFK-XA576163', 'BJFFK-SCLC574253', 'C00564517', 'BJFFK-XA574864', 'BJFFK-XA573779', 'BJFFK-XA574570', 'BJXK-XIMEA-FFK-P-592670', 'BJXFK-YX575769', 'BJFFK-XA575058', 'F2', 'C00570399', 'BJXK-XIMEA-FFK-P-601186', 'BJXK-XIMEA-FFK-P-599581', 'BJXFK-XA577091', 'BJXFK-XA575834', 'BJXFK-XA583281', 'c00568834', 'F25', 'BJXFK-YX-576622', 'BJXFK-XA576692', 'BJFFK-XA571457', 'BJXFK-XA577014', 'c00569478', 'BJXFK-YX583088', 'BJXFK-XA577016', 'BJFFK-XA571327', 'C00569905', 'c00568308', 'BJFFK-SCLC572374', 'BJFFK-XA571332', 'c00567186', 'c00568523', 'BJFFK-XA572378', 'BJFFK-XA575640', 'BJXFK-YX584459', 'BJFFK-XA575740', 'BJXK-XIMEA-FFK-P-601001', 'BJXFK-XA-581051', 'BJXK-XIMEA-FFK-P-593609', 'c00568309', 'BJFFK-XA571956', 'BJFFK-XA570908', 'BJXK-XIMEA-FFK-P-602284', 'C00562488', 'BJXFK-XA579315', 'BJXK-XIMEA-FFK-P-598709', 'BJFFK-XA573780', 'BJFFK-XA571951', 'F8', 'BJXFK-EXZLWFX-586914', 'BJXK-XIMEA-FFK-P-592163', 'BJFFK-XA574012', 'BJFFK-XA573815', 'BJFFK-XA574061', 'BJXFK-YX577852', 'F7', 'BJXFK-XA583062', 'BJXFK-XA576184', 'BJXK-XIMEA-FFK-P-592757', 'BJXFK-YX590065', 'BJXFK-EXZLWFX-588182', 'BJXK-XIMEA-FFK-P-593826', 'BJXFK-YX-575866', 'BJFFK-XA574126', 'BJXK-XIMEA-FFK-P-599123', 'BJXFK-XA-590263', 'BJXFK-XXBA-575861', 'BJXFK-XA581050', 'BJXFK-XA576102', 'BJXFK-XA586392', 'BJFFK-XA572478', 'BJXK-XIMEA-FFK-P-593722', 'BJXFK-XA576734', 'BJXK-XIMEA-FFK-P-599508', 'BJFFK-XA573481', 'F27', 'BJXFK-XA588179', 'BJXFK-XXBA-591060', 'BJFFK-XA575619', 'BJXFK-EXZLWFX-581977', 'BJXK-XIMEA-FFK-P-601996', 'BJXFK-XA586645', 'C00569479', 'BJXFK-XA576250', 'BJXFK-XA-591134', 'BJFFK-XA571885', 'c00570276', 'BJXFK-XA-XXBA-576162', 'BJXFK-YX-575750', 'BJFFK-XA570999', 'BJFFK-XA571608', 'BJFFK-XA572293', 'BJXK-XIMEA-FFK-P-593127', 'BJXFK-JPL-585186', 'C0056281f', 'c00570667', 'BJXFK-JPL-576255', 'BJXK-XIMEA-FFK-P-591855', 'BJXFK-XA576932', 'F24', 'BJXFK-YX585201', 'BJFFK-XA571134', 'c00566112', 'C00569807', 'BJFFK-XA572862', 'BJXFK-XXBA-576604', 'BJFFK-XA572375', 'BJXFK-XA576924', 'BJXFK-XA587998', 'BJFFK-JPL571326', 'c00567896', 'BJXK-XIMEA-FFK-P-593715', 'BJXFK-JPL-577915', 'c00570171', 'BJXFK-LA-587831', 'BJXFK-XA589465', 'BJFFK-XA572708', 'C00562664', 'BJXK-XIMEA-FFK-P-598831', 'BJFFK-SCLC572373', 'BJXK-XIMEA-FFK-P-600723', 'c00563357', 'c00570398', 'BJFFK-XA573501', 'BJFFK-XA575215', 'BJXFK-YX576249', 'BJXK-XIMEA-FFK-P-599505', 'C00569706', 'BJXFK-XA575786', 'BJFFK-JPL570703', 'BJFFK-XA574333', 'BJXK-XIMEA-FFK-P-599731', 'BJXFK-XA-XXBA-578232', 'BJXK-XIMEA-FFK-P-601297', 'BJXFK-JPL-582473', 'BJFFK-XA571953', 'F23', 'BJFFK-XA573781', 'BJXFK-XA587308', 'BJXFK-EXZLWFX-580084', 'c00570277', 'C00568307', 'BJXK-XIMEA-FFK-P-598610', 'BJXFK-XA591299', 'BJXFK-YX590265', 'BJFFK-XA574584', 'BJFFK-JPL571450', 'C00569142', 'BJFFK-XA575743', 'BJFFK-XA570907', 'C00562663', 'BJXK-XIMEA-FFK-P-598555', 'C00567317', 'BJXK-XIMEA-FFK-P-599220', 'BJXK-XIMEA-FFK-P-600407', 'BJFFK-XA575445', 'BJXFK-EXZLWFX-585294', 'c00567894', 'BJXFK-XA589233', 'BJXFK-JPL-584243', 'BJXFK-XA585881', 'BJXFK-XA576164', 'BJFFK-XA575703', 'BJXFK-JPL-586018', 'BJFFK-XA573000', 'F1', 'BJFFK-XA573710', 'BJXFK-XXBA-590356', 'BJFFK-XA573708', 'BJXFK-XA583282', 'BJXFK-XA-590514', 'BJXFK-YX581895', 'BJFFK-XA573638', 'BJFFK-XA574865', 'BJXFK-XA-XXBA-576103', 'BJXFK-XA586728', 'BJXFK-YX576670', 'BJXFK-YX577400', 'BJFFK-XA573776', 'BJFFK-JPL570906', 'C00565462', 'BJXK-XIMEA-FFK-P-593212', 'BJFFK-XA571508', 'BJXFK-XA580423', 'BJFFK-XA571452', 'BJFFK-XA573974', 'BJXFK-EXZLWFX-580664', 'BJXFK-XA591086', 'BJFFK-XA571510', 'BJFFK-XA574583', 'BJFFK-XA574842', 'BJXK-XIMEA-P-C601031', 'BJFFK-XA573871', 'BJXK-XIMEA-FFK-P-593309', 'BJFFK-XA571994', 'BJXFK-EXZLWFX-580048', 'BJXFK-XXBA-575862', 'BJFFK-XA572332', 'BJXFK-JPL-585628', 'BJXK-XIMEA-FFK-P-592511', 'BJXK-XIMEA-FFK-P-594531', 'BJXFK-XA577093', 'BJXFK-XA591363', 'BJFFK-XA575354', 'BJXFK-XA578049', 'BJXFK-XA578780', 'BJFFK-XA571606', 'BJXFK-XA586013', 'BJXFK-XA-589881', 'BJXFK-EXZLWFX-582096', 'BJXFK-XA577621', 'BJXK-XIMEA-FFK-P-599699', 'c00567767', 'BJXFK-XA-576006', 'C00562668', 'BJXFK-LA-587550', 'c00570619', 'BJXFK-LA-589256', 'BJXFK-XA-579871', 'BJXFK-XA-580054', 'BJXFK-XA-579925', 'BJXFK-YX577427', 'BJFFK-JPL571605', 'BJFFK-XA572376', 'BJXFK-JPL-584457', 'BJXK-XIMEA-FFK-P-593304', 'BJFFK-JPL570998', 'BJFFK-XA573909', 'c00570400', 'BJFFK-XA570823', 'F14', 'BJXFK-NILM-589516', 'F65', 'BJXFK-NILM-591007', 'BJXFK-NILM-590323', 'BJXFK-NILM-591132', 'BJXFK-NILM-590606', 'F12', 'F69', 'F78', 'F72', 'BJXFK-NILM-591545', 'F67', 'F77', 'F61', 'F82', 'F93', 'F66', 'BJXFK-NILM-590508', 'F92', 'BJXK-XIMEA-FFK-N-598911', 'F68', 'F85', 'BJXFK-NILM-591201', 'F79', 'BJXFK-NILM-590322', 'c00569477', 'F80', 'F98', 'F83', 'BJXFK-NILM-589662', 'BJXFK-NILM-591230', 'BJXFK-NILM-591304', 'c00569144', 'BJXFK-NILM-589580', 'BJXFK-NILM-589508', 'BJXFK-NILM-591089', 'BJXFK-NILM-591253', 'F15', 'BJXFK-NILM-591607', 'BJXFK-NILM-590688', 'BJXFK-NILM-591624', 'F13', 'F97', 'BJXFK-NILM-591294', 'BJXFK-NILM-590757', 'BJXK-XIMEA-FFK-N-598823', 'F88', 'BJTCT563861', 'c00563160', 'F81', 'BJXFK-NILM-590672', 'F90', 'F94', 'F95', 'F75', 'F84', 'F74', 'BJXFK-NILM-589730', 'BJTCT563940', 'BJXFK-NILM-590687', 'F99', 'BJXFK-NILM-591084', 'F11', 'BJXFK-NILM-591444', 'BJXFK-NILM-591276', 'F86', 'F73', 'BJXFK-NILM-590673', 'F71', 'BJXFK-NILM-591313', 'BJXFK-NILM-591311', 'C00567187', 'BJFFK-SQ571250', 'BJXK-XIMEA-FFK-P-594801', 'BJXK-XIMEA-FFK-P-599858', 'BJFFK-XA571456', 'BJXK-XIMEA-FFK-P-602787', 'C00568218', 'BJXK-XIMEA-FFK-P-602501', 'BJXK-XIMEA-FFK-P-594934', 'F40', 'BJXK-XIMEA-FFK-P-595754', 'BJXK-XIMEA-FFK-P-596250', 'BJXK-XIMEA-FFK-P-595753', 'F4', 'BJXK-XIMEA-FFK-P-595253', 'BJXK-XIMEA-FFK-P-597796', 'BJXK-XIMEA-FFK-P-593818', 'BJXK-XIMEA-FFK-P-595303', 'BJXK-XIMEA-FFK-P-599895', 'BJXK-XIMEA-FFK-P-598413', 'BJXK-XIMEA-FFK-P-599867', 'BJXK-XIMEA-FFK-P-602793', 'C00564166', 'BJXK-XIMEA-FFK-P-594333', 'BJXK-XIMEA-FFK-P-593136', 'F43', 'BJXK-XIMEA-FFK-P-602157', 'BJXK-XIMEA-FFK-P-591724', 'BJFFK-JPL572707', 'BJXK-XIMEA-FFK-P-598817', 'BJXK-XIMEA-FFK-P-595158', 'F28', 'BJXK-XIMEA-FFK-P-593142', 'F45', 'BJFFK-SCLC574043', 'BJXK-XIMEA-P-599587', 'F35', 'BJXK-XIMEA-FFK-P-601790', 'C00562410', 'BJXK-XIMEA-FFK-P-601882', 'BJFFK-SQ570830', 'BJXK-XIMEA-FFK-P-599292', 'BJXK-XIMEA-FFK-P-597495', 'BJXK-XIMEA-FFK-P-599223', 'BJXK-XIMEA-FFK-P-599495', 'BJXK-XIMEA-FFK-P-598439', 'BJXK-XIMEA-FFK-P-592517', 'BJXK-XIMEA-FFK-P-600069', 'BJXK-XIMEA-FFK-P-595006', 'BJXK-XIMEA-FFK-P-601787', 'C00569138', 'F58', 'BJXK-XIMEA-FFK-P-597670', 'BJXK-XIMEA-FFK-P-601791', 'C00568110', 'BJXK-XIMEA-FFK-P-602399', 'BJXK-XIMEA-FFK-P-598304', 'BJXK-XIMEA-FFK-P-596341', 'BJXK-XIMEA-P-595010', 'BJXK-XIMEA-FFK-P-596783', 'BJXK-XIMEA-FFK-P-596785', 'BJXK-XIMEA-FFK-P-602900', 'C00567184', 'c00568222', 'BJXK-XIMEA-FFK-P-597001', 'BJXK-XIMEA-FFK-P-599857', 'F39', 'BJXK-XIMEA-FFK-P-602661', 'c00563158', 'C00569903', 'F36', 'c00566002', 'F29', 'F60', 'BJXK-XIMEA-FFK-P-593015', 'BJXK-XIMEA-FFK-P-593128', 'BJXK-XIMEA-FFK-P-601393', 'BJXK-XIMEA-FFK-P-592779', 'BJXK-XIMEA-FFK-P-596249', 'BJXK-XIMEA-FFK-P-601185', 'BJXK-XIMEA-FFK-P-601411', 'C00569715', 'BJXK-XIMEA-FFK-P-598926', 'BJXK-XIMEA-FFK-P-597134', 'F46', 'c00566207', 'BJXK-XIMEA-P-595259', 'F33', 'F41', 'BJXK-XIMEA-FFK-P-595871', 'F59', 'BJXK-XIMEA-FFK-P-595752', 'BJXK-XIMEA-FFK-P-592778', 'c00565704', 'BJFFK-SQ573819', 'F3', 'BJXK-XIMEA-FFK-P-594552', 'C00567183', 'BJXK-XIMEA-FFK-P-599694', 'BJXK-XIMEA-FFK-P-597669', 'BJXK-XIMEA-FFK-P-597069', 'BJFFK-SQ574677', 'BJXK-XIMEA-FFK-P-595757', 'F10', 'F32', 'BJXK-XIMEA-FFK-P-595041', 'BJFFK-SQ571266', 'BJFFK-JPL572747', 'F57', 'C00568109', 'F38', 'BJXK-XIMEA-FFK-P-596254', 'BJFFK-SCLC574011', 'BJXK-XIMEA-FFK-P-601989', 'BJFFK-SCLC571451', 'BJXK-XIMEA-FFK-P-598504', 'c00565709', 'C00564974', 'BJXK-XIMEA-FFK-P-602889', 'BJXK-XIMEA-FFK-P-602386', 'c00566203', 'F42', 'BJXK-XIMEA-FFK-P-598833', 'F34', 'BJFFK-JPL573004', 'BJXK-XIMEA-FFK-P-599695', 'BJXK-XIMEA-FFK-P-595224', 'F44', 'BJXK-XIMEA-FFK-N-602653', 'BJXFK-NILM-607487', 'BJXK-XIMEA-FFK-N-601554', 'BJTCT564874', 'BJXFK-NILM-608646', 'BJTCT569901', 'BJXFK-NILM-607389', 'BJXK-XIMEA-FFK-N-602654', 'BJXFK-NILM-603193', 'BJTCT568024', 'BJTCT563942', 'BJTCT570623', 'BJXFK-NILM-603605', 'BJTCT569371', 'BJFFK-ZS573972', 'BJXK-XIMEA-N-599025', 'BJXK-XIMEA-FFK-N-601555', 'BJXFK-NILM-608428', 'BJXK-XIMEA-FFK-N-600497', 'BJXFK-NILM-608247', 'BJXFK-NILM-590675', 'BJXFK-NILM-590582', 'BJXFK-NILM-590755', 'BJXFK-NILM-590900', 'BJTCT568022', 'BJTCT570617', 'BJXK-XIMEA-N-601392', 'BJXK-XIMEA-FFK-N-601988', 'F52', 'BJXK-XIMEA-FFK-N-599859', 'BJXFK-NILM-591329', 'BJXK-XIMEA-FFK-N-602293', 'BJXK-XIMEA-FFK-N-601283', 'BJXFK-NILM-609152', 'BJTCT570620', 'BJXFK-NILM-606591', 'BJXFK-NILM-590677', 'BJXFK-NILM-608703', 'BJXK-XIMEA-FFK-N-600716', 'BJTCT569806', 'BJFFK-ZS572911', 'BJXK-XIMEA-FFK-N-599776', 'BJXK-XIMEA-N-601314', 'BJXFK-NILM-606611', 'BJXFK-NILM-590793', 'BJXFK-NILM-606968', 'BJXFK-NILM-608939', 'BJXFK-NILM-607496', 'BJXFK-NILM-603434', 'BJXFK-NILM-607483', 'BJXFK-NILM-608168', 'BJXK-XIMEA-N-601412', 'BJXFK-NILM-591353', 'BJXFK-NILM-603301', 'BJXFK-NILM-608169', 'BJXFK-NILM-609043', 'BJXFK-NILM-591356', 'BJFFK-ZS573463', 'BJXFK-NILM-607486', 'BJXFK-NILM-601200', 'BJXFK-NILM-608873', 'BJXFK-NILM-608417', 'BJXFK-NILM-607271', 'BJXK-XIMEA-N-602285', 'BJXK-XIMEA-FFK-N-602655', 'BJXK-XIMEA-FFK-N-600714', 'BJTCT569714', 'BJXK-XIMEA-N-600972', 'BJXFK-NILM-603511', 'BJXFK-NILM-590440', 'BJXK-XIMEA-FFK-N-599715', 'BJXFK-NILM-607920', 'BJXK-XIMEA-FFK-N-600720', 'BJTCT568026', 'BJXFK-NILM-591448', 'BJFFK-ZS572458', 'BJXFK-NILM-608795', 'BJXK-XIMEA-FFK-N-599866', 'BJXFK-NILM-591074', 'BJXFK-NILM-603304', 'BJXFK-NILM-589809', 'BJXFK-NILM-607599', 'BJTCT570828', 'BJXFK-NILM-591131', 'BJXK-XIMEA-FFK-N-602185', 'BJXFK-NILM-603446', 'BJXK-XIMEA-N-602504', 'BJXFK-NILM-608777', 'BJXK-XIMEA-FFK-N-602290', 'BJXFK-NILM-607812', 'BJXK-XIMEA-N-598910', 'BJXK-XIMEA-FFK-N-598927', 'BJXFK-NILM-590462', 'BJTCT571330', 'BJXFK-NILM-607258', 'F54', 'BJXK-XIMEA-N-601391', 'BJXFK-NILM-589796', 'BJFFK-ZS575685', 'BJXFK-NILM-607598', 'BJXK-XIMEA-N-599153', 'BJXFK-NILM-607192', 'BJXFK-NILM-606991', 'BJXFK-NILM-607270', 'BJTCT570273', 'BJXK-XIMEA-N-600292', 'BJXFK-NILM-607022', 'BJXK-XIMEA-FFK-N-599629', 'BJXK-XIMEA-N-599252', 'BJXK-XIMEA-N-599088', 'BJXFK-NILM-591091', 'BJXFK-NILM-607485', 'BJXFK-NILM-591371', 'BJXFK-NILM-591229', 'BJXFK-NILM-591204', 'BJXK-XIMEA-FFK-N-601816', 'BJXFK-NILM-607513', 'BJXK-XIMEA-FFK-N-600803', 'BJTCT570826', 'BJFFK-ZS573778', 'BJXK-XIMEA-FFK-N-600218', 'BJTCT570032', 'BJXFK-NILM-603142', 'F56', 'BJXFK-NILM-609235', 'BJXFK-NILM-606705', 'BJXK-XIMEA-N-599154', 'F50', 'BJXFK-NILM-591071', 'BJXK-XIMEA-N-599226', 'BJTCT571139', 'BJXFK-NILM-608773', 'BJXK-XIMEA-FFK-N-601280', 'BJXFK-NILM-607350', 'BJXK-XIMEA-FFK-N-599865', 'BJXFK-NILM-606895', 'BJTCT570030', 'BJFFK-ZS573634', 'BJXFK-NILM-589509', 'BJFFK-ZS575657', 'BJXFK-NILM-609234', 'BJTCT570004', 'BJXFK-NILM-603690', 'BJTCT571334', 'BJTCT568030', 'BJXFK-NILM-591254', 'BJXK-XIMEA-FFK-N-600068', 'BJXFK-NILM-606929', 'BJXFK-NILM-608796', 'BJXK-XIMEA-FFK-N-600789', 'BJXFK-NILM-590510', 'BJTCT570827', 'BJTCT563941', 'BJTCT570618', 'BJXFK-NILM-590922', 'BJTCT564059', 'BJXFK-NILM-608109', 'F49', 'BJXK-XIMEA-FFK-N-599777', 'BJTCT570170', 'BJXFK-NILM-603750', 'BJTCT567895', 'BJXFK-NILM-608108', 'BJXFK-NILM-589656', 'BJXFK-NILM-603770', 'BJXFK-NILM-590974', 'BJTCT564871', 'BJXFK-NILM-609262', 'BJXFK-NILM-606832', 'BJXFK-NILM-606707', 'BJXFK-NILM-606752', 'BJXFK-NILM-607057', 'BJXFK-NILM-603487', 'BJTCT570392', 'BJXK-XIMEA-FFK-N-603331', 'BJTCT570275', 'BJXFK-NILM-608416', 'BJXFK-NILM-591443', 'BJXK-XIMEA-FFK-N-599716', 'BJXFK-NILM-590894', 'BJXK-XIMEA-FFK-N-600729', 'BJXFK-NILM-590903', 'BJXK-XIMEA-FFK-N-598914', 'BJXK-XIMEA-FFK-N-600790', 'BJTCT570622', 'BJXK-XIMEA-N-599333', 'BJXK-XIMEA-N-599229', 'BJXK-XIMEA-FFK-N-601279', 'BJXFK-NILM-603032', 'BJTCT570913', 'BJXFK-NILM-590583', 'BJXFK-NILM-608321', 'BJFFK-ZS571537', 'BJTCT571269', 'BJXFK-NILM-590756', 'BJXFK-NILM-609258', 'BJTCT570167', 'BJXFK-NILM-607719', 'BJTCT570027', 'BJXFK-NILM-606709', 'BJXK-XIMEA-FFK-N-601808', 'BJTCT569904', 'BJTCT569711', 'BJXFK-NILM-603143', 'BJFFK-ZS571909', 'F53', 'BJXFK-NILM-591481', 'BJXK-XIMEA-FFK-N-602082', 'BJXFK-NILM-606756', 'BJTCT570914', 'BJXK-XIMEA-FFK-N-599881', 'BJTCT570029', 'BJTCT570391', 'BJTCT570911', 'BJTCT567688', 'BJXFK-NILM-590526', 'BJXK-XIMEA-FFK-N-601985', 'BJTCT570396', 'BJFFK-ZS573546', 'BJTCT564058', 'BJXFK-NILM-589729', 'BJXK-XIMEA-N-599122', 'BJXFK-NILM-606927', 'BJTCT569475', 'BJXFK-NILM-606971', 'BJTCT569584', 'BJXFK-NILM-603445', 'BJXFK-NILM-603588', 'BJFFK-ZS572711', 'BJTCT569805', 'BJXK-XIMEA-FFK-N-600715', 'BJTCT568025', 'BJXFK-NILM-590461', 'BJTCT570912', 'BJXFK-NILM-608724', 'BJXFK-NILM-590465', 'BJXK-XIMEA-FFK-N-599779', 'BJXFK-NILM-590904', 'BJTCT564164', 'BJXFK-NILM-608772', 'BJXFK-NILM-591065', 'BJXFK-NILM-590321', 'BJXFK-NILM-608415', 'BJXK-XIMEA-FFK-N-598824', 'BJTCT571333', 'BJFFK-ZS572891', 'BJTCT569369', 'BJTCT569582', 'BJXFK-NILM-590460', 'BJXFK-NILM-591225', 'BJXFK-NILM-608704', 'BJTCT570829', 'BJTCT567687', 'BJXFK-NILM-591355', 'BJXFK-NILM-603507', 'BJTCT564168', 'BJXFK-NILM-603191', 'BJXFK-NILM-607495', 'BJTCT570397', 'BJXFK-NILM-608702', 'BJXFK-NILM-609039', 'BJTCT569710', 'BJXFK-NILM-591256', 'BJXFK-NILM-601203', 'BJXK-XIMEA-FFK-N-599532', 'BJXFK-NILM-608774', 'BJXK-XIMEA-FFK-N-602196', 'BJFFK-ZS573914', 'BJXFK-NILM-603195', 'BJTCT571136', 'BJXK-XIMEA-FFK-N-600804', 'BJXK-XIMEA-FFK-N-600291', 'F51', 'BJXK-XIMEA-FFK-N-603314', 'BJXFK-NILM-609295', 'BJXK-XIMEA-FFK-N-600578', 'BJXFK-NILM-608701', 'BJXFK-NILM-601184', 'BJXFK-NILM-608775', 'BJXK-XIMEA-FFK-N-598948', 'BJXK-XIMEA-N-599228', 'BJXFK-NILM-591354', 'BJXK-XIMEA-FFK-N-601987', 'BJXFK-NILM-607349', 'BJXFK-NILM-589819', 'BJXFK-NILM-606803', 'BJTCT564873', 'BJXFK-NILM-607921', 'BJFFK-ZS572890', 'BJTCT569581', 'BJTCT564055', 'BJTCT569476', 'BJXFK-NILM-591606', 'BJXFK-NILM-607489', 'BJTCT571138', 'BJXK-XIMEA-N-599253', 'BJXK-XIMEA-FFK-N-601556', 'BJTCT564056', 'BJXFK-NILM-607351', 'BJXFK-NILM-591059', 'BJXK-XIMEA-N-603136', 'BJXFK-NILM-589612', 'BJXFK-NILM-607491', 'BJTCT564162', 'BJXFK-NILM-591255', 'BJTCT568031', 'BJTCT571135', 'BJXK-XIMEA-FFK-N-601990', 'BJTCT571137', 'BJTCT568023', 'BJXK-XIMEA-FFK-N-600053', 'F55', 'BJXFK-NILM-607096', 'BJXFK-NILM-603364', 'BJXFK-NILM-608934', 'BJXFK-NILM-608246', 'BJTCT568032', 'BJXFK-NILM-589657', 'BJTCT570915', 'BJXFK-NILM-591546', 'BJTCT564870', 'BJXFK-NILM-590459', 'BJXFK-NILM-591139', 'BJXFK-NILM-607257', 'BJXK-XIMEA-FFK-N-599024', 'BJFFK-ZS573868']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "wsis = []\n",
    "file_name = './simclr/train_label.csv'\n",
    "with open(file_name, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for row in lines:\n",
    "        row = row.strip().split(',')\n",
    "        wsis.append(row[0])\n",
    "print(wsis)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class CIFAR10Pair(CIFAR10):\n",
    "    def __init__(self,\n",
    "                 transform,\n",
    "                 labeled: int = 10000,\n",
    "                 unlabeled: int = 40000,\n",
    "                 **kargs):\n",
    "        super().__init__(**kargs)        \n",
    "        self.transform = transform\n",
    "        self.labeled, self.unlabeled = labeled, unlabeled\n",
    "        self.targets = self._binarize_cifar10_class(self.targets)\n",
    "        self.data, self.targets, self.prior = self._make_pu_label_from_binary_label(self.data, self.targets)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        img = Image.fromarray(img)  # .convert('RGB')\n",
    "        imgs = [self.transform(img), self.transform(img)]\n",
    "        return torch.stack(imgs), target  # stack a positive pair\n",
    "    \n",
    "    ## add function ##\n",
    "    def _binarize_cifar10_class(self, y):\n",
    "        \"\"\"将类别分为animal和vehicle\"\"\"\n",
    "        # 先转化为numpy\n",
    "        y = np.array(y)\n",
    "        y_bin = np.ones(len(y), dtype=int)\n",
    "        y_bin[(y == 2) | (y == 3) | (y == 4) | (y == 5) | (y == 6) | (y == 7)] = 0\n",
    "        return y_bin\n",
    "    \n",
    "    def _make_pu_label_from_binary_label(self, x, y):\n",
    "        \"\"\"挑选出一定的正样本数作为已标注标签\"\"\"\n",
    "        \"\"\"from https://github.com/kiryor/nnPUlearning\"\"\"\n",
    "        y = np.array(y)\n",
    "        labels = np.unique(y)\n",
    "        positive, negative = labels[1], labels[0]\n",
    "        labeled, unlabeled = self.labeled, self.unlabeled\n",
    "        assert(len(x) == len(y))\n",
    "        perm = np.random.permutation(len(y))\n",
    "        x, y = x[perm], y[perm]\n",
    "        n_p = (y == positive).sum()\n",
    "        n_lp = labeled\n",
    "        n_n = (y == negative).sum()\n",
    "        n_u = unlabeled\n",
    "        if labeled + unlabeled == len(x):\n",
    "            n_up = n_p - n_lp\n",
    "        elif unlabeled == len(x):\n",
    "            n_up = n_p\n",
    "        else:\n",
    "            raise ValueError(\"Only support |P|+|U|=|X| or |U|=|X|.\")\n",
    "        _prior = float(n_up) / float(n_u)\n",
    "        xlp = x[y == positive][:n_lp]\n",
    "        xup = np.concatenate((x[y == positive][n_lp:], xlp), axis=0)[:n_up]\n",
    "        xun = x[y == negative]\n",
    "        x = np.asarray(np.concatenate((xlp, xup, xun), axis=0))\n",
    "        print(x.shape)\n",
    "        y = np.asarray(np.concatenate((np.ones(n_lp), -np.ones(n_u))))\n",
    "        perm = np.random.permutation(len(y))\n",
    "        y[y==-1]=negative\n",
    "        x, y = x[perm], y[perm]\n",
    "        return x, y, _prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Whole_Slide_Patchs_Ngc(Dataset):\n",
    "    # pos is 0 and neg is 1, because all patches of the neg wsi are neg,\n",
    "    # but pos wsi includes pos and neg patches \n",
    "    def __init__(self,\n",
    "                 data_dir,\n",
    "                 train_label_path,\n",
    "                 transform):\n",
    "        # get img_path\n",
    "        sub_paths = [\n",
    "            'Unannotated_KSJ/Unannotated-KSJ-TCTNGC-NILM',\n",
    "            'Unannotated_KSJ/Unannotated-KSJ-TCTNGC-POS',\n",
    "            'Unannotated_XIMEA/Unannotated-XIMEA-TCTNGC-NILM',\n",
    "            'Unannotated_XIMEA/Unannotated-XIMEA-TCTNGC-POS'\n",
    "        ]\n",
    "        data_roots = list(map(lambda x: os.path.join(data_dir, x), sub_paths)) \n",
    "        wsi_dirs = []\n",
    "        train_wsi_lists = []\n",
    "        img_paths = []\n",
    "        with open(train_label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for row in lines:\n",
    "                row = row.strip().split(',')\n",
    "                train_wsi_lists.append(row[0])\n",
    "        for data_root in data_roots:\n",
    "            wsi_dirs.extend([os.path.join(data_root, subdir) for subdir in os.listdir(data_root)])\n",
    "        \n",
    "        for wsi_path in wsi_dirs:\n",
    "            wsi_name = os.path.basename(wsi_path)\n",
    "            if wsi_name not in train_wsi_lists:\n",
    "                continue\n",
    "            img_paths.extend(glob.glob(os.path.join(wsi_path, '*.jpg')))\n",
    "        self.img_paths = img_paths\n",
    "        # the size is too big\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx])\n",
    "        target = 0 \n",
    "        if 'NILM' in str(self.img_paths[idx]):\n",
    "            target = 1\n",
    "        imgs =  [self.transform(img), self.transform(img)]\n",
    "        return torch.stack(imgs), target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'the length of patchs in {self.img_paths} is {self.__len__()}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import glob\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                          transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                          transforms.ToTensor()])\n",
    "    \n",
    "\n",
    "train_set = Whole_Slide_Patchs_Ngc(\n",
    "            data_dir='/home1/wsi/ngc-2023-1333/',\n",
    "            train_label_path='/home/huangjialong/projects/BiomedCLIP-PUNCE/datatools/ngc_labels/ngc_train_label.csv',\n",
    "            transform=train_transform\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "/home1/wsi/ngc-2023-1333/Unannotated_KSJ/Unannotated-KSJ-TCTNGC-POS/c00570398/19_9.jpg\n"
     ]
    }
   ],
   "source": [
    "idx = 200000\n",
    "print(train_set[idx][1])\n",
    "print(train_set.img_paths[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class args_class():\n",
    "    def __init__(self,):\n",
    "        self.data_dir = '/home/huangjialong/projects/SimCLR-CIFAR10/data'\n",
    "        \n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.ToTensor()])\n",
    "args = args_class()\n",
    "train_set = CIFAR10Pair(root=args.data_dir,\n",
    "                        train=True,\n",
    "                        transform=train_transform,\n",
    "                        download=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[10][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1., -1.,  1., -1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([1.0, -2.0, 3.0, -4.0])\n",
    "sign_tensor = torch.sign(tensor)\n",
    "\n",
    "print(sign_tensor)  # 输出: tensor([1., -1., 1., -1.])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

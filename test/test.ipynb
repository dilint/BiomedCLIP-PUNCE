{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/sklearn/__init__.py:82\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[1;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[1;32m     85\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcovariance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcross_decomposition\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     86\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdecomposition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mensemble\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mexceptions\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     87\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mexperimental\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mexternals\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfeature_extraction\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mclone\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mget_config\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mset_config\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mconfig_context\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     97\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/sklearn/base.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     22\u001b[0m _DEFAULT_TAGS \u001b[39m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnon_deterministic\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrequires_positive_X\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbinary_only\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mrequires_fit\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclone\u001b[39m(estimator, safe\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/sklearn/utils/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m np_version\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m (as_float_array,\n\u001b[1;32m     29\u001b[0m                          assert_all_finite,\n\u001b[1;32m     30\u001b[0m                          check_random_state, column_or_1d, check_array,\n\u001b[1;32m     31\u001b[0m                          check_consistent_length, check_X_y, indexable,\n\u001b[1;32m     32\u001b[0m                          check_symmetric, check_scalar)\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/sklearn/utils/fixes.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msp\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m lsqr \u001b[39mas\u001b[39;00m sparse_lsqr  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_parse_version\u001b[39m(version_string):\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/scipy/stats/__init__.py:485\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[1;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/scipy/stats/_stats_py.py:46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[0;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[1;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[1;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/scipy/stats/distributions.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m comb, entr\n\u001b[1;32m     20\u001b[0m \u001b[39m# for root finding for continuous distribution ppf, and max likelihood\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# estimation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m optimize\n\u001b[1;32m     24\u001b[0m \u001b[39m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m integrate\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/scipy/__init__.py:200\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(name):\n\u001b[1;32m    199\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m submodules:\n\u001b[0;32m--> 200\u001b[0m         \u001b[39mreturn\u001b[39;00m _importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mscipy.\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    201\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/anaconda3/envs/biomed/lib/python3.8/site-packages/scipy/optimize/__init__.py:419\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_linprog\u001b[39;00m \u001b[39mimport\u001b[39;00m linprog, linprog_verbose_callback\n\u001b[1;32m    418\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lsap\u001b[39;00m \u001b[39mimport\u001b[39;00m linear_sum_assignment\n\u001b[0;32m--> 419\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_differentialevolution\u001b[39;00m \u001b[39mimport\u001b[39;00m differential_evolution\n\u001b[1;32m    420\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lsq\u001b[39;00m \u001b[39mimport\u001b[39;00m least_squares, lsq_linear\n\u001b[1;32m    421\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_constraints\u001b[39;00m \u001b[39mimport\u001b[39;00m (NonlinearConstraint,\n\u001b[1;32m    422\u001b[0m                            LinearConstraint,\n\u001b[1;32m    423\u001b[0m                            Bounds)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:971\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:640\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score\n",
    "\n",
    "# create confusion matrix\n",
    "y_true = np.array([-1]*70 + [0]*160 + [1]*30)\n",
    "y_pred = np.array([-1]*40 + [0]*20 + [1]*20 + \n",
    "                  [-1]*30 + [0]*80 + [1]*30 + \n",
    "                  [-1]*5 + [0]*15 + [1]*20)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix = pd.DataFrame(cm, index=['Cat','Dog','Pig'], columns=['Cat','Dog','Pig'])\n",
    "\n",
    "print(y_true.shape)\n",
    "print(y_pred.shape)\n",
    "print('------Weighted------')\n",
    "print('Weighted precision', precision_score(y_true, y_pred, average='weighted'))\n",
    "print('Weighted recall', recall_score(y_true, y_pred, average='weighted'))\n",
    "print('Weighted f1-score', f1_score(y_true, y_pred, average='weighted'))\n",
    "print('------Macro------')\n",
    "print('Macro precision', precision_score(y_true, y_pred, average='macro'))\n",
    "print('Macro recall', recall_score(y_true, y_pred, average='macro'))\n",
    "print('Macro f1-score', f1_score(y_true, y_pred, average='macro'))\n",
    "print('------Micro------')\n",
    "print('Micro precision', precision_score(y_true, y_pred, average='micro'))\n",
    "print('Micro recall', recall_score(y_true, y_pred, average='micro'))\n",
    "print('Micro f1-score', f1_score(y_true, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = a + [4]\n",
    "b = tuple(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ckp_path = '/home/huangjialong/projects/BiomedCLIP-PUNCE/mil-methods/output-model/output-test/resnet1-meanmil-ngc-customsplit/fold_0_model_best_auc.pt'\n",
    "torch.load(ckp_path)\n",
    "ckp_dict = torch.load(ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BJXFK-LA-591455', 'c00567195', 'BJXFK-XXBA-579873', 'BJXFK-XA-575822', 'c00564281', 'BJXK-XIMEA-FFK-P-599781', 'BJXFK-YX577236', 'F22', 'BJXFK-XA576163', 'BJFFK-SCLC574253', 'C00564517', 'BJFFK-XA574864', 'BJFFK-XA573779', 'BJFFK-XA574570', 'BJXK-XIMEA-FFK-P-592670', 'BJXFK-YX575769', 'BJFFK-XA575058', 'F2', 'C00570399', 'BJXK-XIMEA-FFK-P-601186', 'BJXK-XIMEA-FFK-P-599581', 'BJXFK-XA577091', 'BJXFK-XA575834', 'BJXFK-XA583281', 'c00568834', 'F25', 'BJXFK-YX-576622', 'BJXFK-XA576692', 'BJFFK-XA571457', 'BJXFK-XA577014', 'c00569478', 'BJXFK-YX583088', 'BJXFK-XA577016', 'BJFFK-XA571327', 'C00569905', 'c00568308', 'BJFFK-SCLC572374', 'BJFFK-XA571332', 'c00567186', 'c00568523', 'BJFFK-XA572378', 'BJFFK-XA575640', 'BJXFK-YX584459', 'BJFFK-XA575740', 'BJXK-XIMEA-FFK-P-601001', 'BJXFK-XA-581051', 'BJXK-XIMEA-FFK-P-593609', 'c00568309', 'BJFFK-XA571956', 'BJFFK-XA570908', 'BJXK-XIMEA-FFK-P-602284', 'C00562488', 'BJXFK-XA579315', 'BJXK-XIMEA-FFK-P-598709', 'BJFFK-XA573780', 'BJFFK-XA571951', 'F8', 'BJXFK-EXZLWFX-586914', 'BJXK-XIMEA-FFK-P-592163', 'BJFFK-XA574012', 'BJFFK-XA573815', 'BJFFK-XA574061', 'BJXFK-YX577852', 'F7', 'BJXFK-XA583062', 'BJXFK-XA576184', 'BJXK-XIMEA-FFK-P-592757', 'BJXFK-YX590065', 'BJXFK-EXZLWFX-588182', 'BJXK-XIMEA-FFK-P-593826', 'BJXFK-YX-575866', 'BJFFK-XA574126', 'BJXK-XIMEA-FFK-P-599123', 'BJXFK-XA-590263', 'BJXFK-XXBA-575861', 'BJXFK-XA581050', 'BJXFK-XA576102', 'BJXFK-XA586392', 'BJFFK-XA572478', 'BJXK-XIMEA-FFK-P-593722', 'BJXFK-XA576734', 'BJXK-XIMEA-FFK-P-599508', 'BJFFK-XA573481', 'F27', 'BJXFK-XA588179', 'BJXFK-XXBA-591060', 'BJFFK-XA575619', 'BJXFK-EXZLWFX-581977', 'BJXK-XIMEA-FFK-P-601996', 'BJXFK-XA586645', 'C00569479', 'BJXFK-XA576250', 'BJXFK-XA-591134', 'BJFFK-XA571885', 'c00570276', 'BJXFK-XA-XXBA-576162', 'BJXFK-YX-575750', 'BJFFK-XA570999', 'BJFFK-XA571608', 'BJFFK-XA572293', 'BJXK-XIMEA-FFK-P-593127', 'BJXFK-JPL-585186', 'C0056281f', 'c00570667', 'BJXFK-JPL-576255', 'BJXK-XIMEA-FFK-P-591855', 'BJXFK-XA576932', 'F24', 'BJXFK-YX585201', 'BJFFK-XA571134', 'c00566112', 'C00569807', 'BJFFK-XA572862', 'BJXFK-XXBA-576604', 'BJFFK-XA572375', 'BJXFK-XA576924', 'BJXFK-XA587998', 'BJFFK-JPL571326', 'c00567896', 'BJXK-XIMEA-FFK-P-593715', 'BJXFK-JPL-577915', 'c00570171', 'BJXFK-LA-587831', 'BJXFK-XA589465', 'BJFFK-XA572708', 'C00562664', 'BJXK-XIMEA-FFK-P-598831', 'BJFFK-SCLC572373', 'BJXK-XIMEA-FFK-P-600723', 'c00563357', 'c00570398', 'BJFFK-XA573501', 'BJFFK-XA575215', 'BJXFK-YX576249', 'BJXK-XIMEA-FFK-P-599505', 'C00569706', 'BJXFK-XA575786', 'BJFFK-JPL570703', 'BJFFK-XA574333', 'BJXK-XIMEA-FFK-P-599731', 'BJXFK-XA-XXBA-578232', 'BJXK-XIMEA-FFK-P-601297', 'BJXFK-JPL-582473', 'BJFFK-XA571953', 'F23', 'BJFFK-XA573781', 'BJXFK-XA587308', 'BJXFK-EXZLWFX-580084', 'c00570277', 'C00568307', 'BJXK-XIMEA-FFK-P-598610', 'BJXFK-XA591299', 'BJXFK-YX590265', 'BJFFK-XA574584', 'BJFFK-JPL571450', 'C00569142', 'BJFFK-XA575743', 'BJFFK-XA570907', 'C00562663', 'BJXK-XIMEA-FFK-P-598555', 'C00567317', 'BJXK-XIMEA-FFK-P-599220', 'BJXK-XIMEA-FFK-P-600407', 'BJFFK-XA575445', 'BJXFK-EXZLWFX-585294', 'c00567894', 'BJXFK-XA589233', 'BJXFK-JPL-584243', 'BJXFK-XA585881', 'BJXFK-XA576164', 'BJFFK-XA575703', 'BJXFK-JPL-586018', 'BJFFK-XA573000', 'F1', 'BJFFK-XA573710', 'BJXFK-XXBA-590356', 'BJFFK-XA573708', 'BJXFK-XA583282', 'BJXFK-XA-590514', 'BJXFK-YX581895', 'BJFFK-XA573638', 'BJFFK-XA574865', 'BJXFK-XA-XXBA-576103', 'BJXFK-XA586728', 'BJXFK-YX576670', 'BJXFK-YX577400', 'BJFFK-XA573776', 'BJFFK-JPL570906', 'C00565462', 'BJXK-XIMEA-FFK-P-593212', 'BJFFK-XA571508', 'BJXFK-XA580423', 'BJFFK-XA571452', 'BJFFK-XA573974', 'BJXFK-EXZLWFX-580664', 'BJXFK-XA591086', 'BJFFK-XA571510', 'BJFFK-XA574583', 'BJFFK-XA574842', 'BJXK-XIMEA-P-C601031', 'BJFFK-XA573871', 'BJXK-XIMEA-FFK-P-593309', 'BJFFK-XA571994', 'BJXFK-EXZLWFX-580048', 'BJXFK-XXBA-575862', 'BJFFK-XA572332', 'BJXFK-JPL-585628', 'BJXK-XIMEA-FFK-P-592511', 'BJXK-XIMEA-FFK-P-594531', 'BJXFK-XA577093', 'BJXFK-XA591363', 'BJFFK-XA575354', 'BJXFK-XA578049', 'BJXFK-XA578780', 'BJFFK-XA571606', 'BJXFK-XA586013', 'BJXFK-XA-589881', 'BJXFK-EXZLWFX-582096', 'BJXFK-XA577621', 'BJXK-XIMEA-FFK-P-599699', 'c00567767', 'BJXFK-XA-576006', 'C00562668', 'BJXFK-LA-587550', 'c00570619', 'BJXFK-LA-589256', 'BJXFK-XA-579871', 'BJXFK-XA-580054', 'BJXFK-XA-579925', 'BJXFK-YX577427', 'BJFFK-JPL571605', 'BJFFK-XA572376', 'BJXFK-JPL-584457', 'BJXK-XIMEA-FFK-P-593304', 'BJFFK-JPL570998', 'BJFFK-XA573909', 'c00570400', 'BJFFK-XA570823', 'F14', 'BJXFK-NILM-589516', 'F65', 'BJXFK-NILM-591007', 'BJXFK-NILM-590323', 'BJXFK-NILM-591132', 'BJXFK-NILM-590606', 'F12', 'F69', 'F78', 'F72', 'BJXFK-NILM-591545', 'F67', 'F77', 'F61', 'F82', 'F93', 'F66', 'BJXFK-NILM-590508', 'F92', 'BJXK-XIMEA-FFK-N-598911', 'F68', 'F85', 'BJXFK-NILM-591201', 'F79', 'BJXFK-NILM-590322', 'c00569477', 'F80', 'F98', 'F83', 'BJXFK-NILM-589662', 'BJXFK-NILM-591230', 'BJXFK-NILM-591304', 'c00569144', 'BJXFK-NILM-589580', 'BJXFK-NILM-589508', 'BJXFK-NILM-591089', 'BJXFK-NILM-591253', 'F15', 'BJXFK-NILM-591607', 'BJXFK-NILM-590688', 'BJXFK-NILM-591624', 'F13', 'F97', 'BJXFK-NILM-591294', 'BJXFK-NILM-590757', 'BJXK-XIMEA-FFK-N-598823', 'F88', 'BJTCT563861', 'c00563160', 'F81', 'BJXFK-NILM-590672', 'F90', 'F94', 'F95', 'F75', 'F84', 'F74', 'BJXFK-NILM-589730', 'BJTCT563940', 'BJXFK-NILM-590687', 'F99', 'BJXFK-NILM-591084', 'F11', 'BJXFK-NILM-591444', 'BJXFK-NILM-591276', 'F86', 'F73', 'BJXFK-NILM-590673', 'F71', 'BJXFK-NILM-591313', 'BJXFK-NILM-591311', 'C00567187', 'BJFFK-SQ571250', 'BJXK-XIMEA-FFK-P-594801', 'BJXK-XIMEA-FFK-P-599858', 'BJFFK-XA571456', 'BJXK-XIMEA-FFK-P-602787', 'C00568218', 'BJXK-XIMEA-FFK-P-602501', 'BJXK-XIMEA-FFK-P-594934', 'F40', 'BJXK-XIMEA-FFK-P-595754', 'BJXK-XIMEA-FFK-P-596250', 'BJXK-XIMEA-FFK-P-595753', 'F4', 'BJXK-XIMEA-FFK-P-595253', 'BJXK-XIMEA-FFK-P-597796', 'BJXK-XIMEA-FFK-P-593818', 'BJXK-XIMEA-FFK-P-595303', 'BJXK-XIMEA-FFK-P-599895', 'BJXK-XIMEA-FFK-P-598413', 'BJXK-XIMEA-FFK-P-599867', 'BJXK-XIMEA-FFK-P-602793', 'C00564166', 'BJXK-XIMEA-FFK-P-594333', 'BJXK-XIMEA-FFK-P-593136', 'F43', 'BJXK-XIMEA-FFK-P-602157', 'BJXK-XIMEA-FFK-P-591724', 'BJFFK-JPL572707', 'BJXK-XIMEA-FFK-P-598817', 'BJXK-XIMEA-FFK-P-595158', 'F28', 'BJXK-XIMEA-FFK-P-593142', 'F45', 'BJFFK-SCLC574043', 'BJXK-XIMEA-P-599587', 'F35', 'BJXK-XIMEA-FFK-P-601790', 'C00562410', 'BJXK-XIMEA-FFK-P-601882', 'BJFFK-SQ570830', 'BJXK-XIMEA-FFK-P-599292', 'BJXK-XIMEA-FFK-P-597495', 'BJXK-XIMEA-FFK-P-599223', 'BJXK-XIMEA-FFK-P-599495', 'BJXK-XIMEA-FFK-P-598439', 'BJXK-XIMEA-FFK-P-592517', 'BJXK-XIMEA-FFK-P-600069', 'BJXK-XIMEA-FFK-P-595006', 'BJXK-XIMEA-FFK-P-601787', 'C00569138', 'F58', 'BJXK-XIMEA-FFK-P-597670', 'BJXK-XIMEA-FFK-P-601791', 'C00568110', 'BJXK-XIMEA-FFK-P-602399', 'BJXK-XIMEA-FFK-P-598304', 'BJXK-XIMEA-FFK-P-596341', 'BJXK-XIMEA-P-595010', 'BJXK-XIMEA-FFK-P-596783', 'BJXK-XIMEA-FFK-P-596785', 'BJXK-XIMEA-FFK-P-602900', 'C00567184', 'c00568222', 'BJXK-XIMEA-FFK-P-597001', 'BJXK-XIMEA-FFK-P-599857', 'F39', 'BJXK-XIMEA-FFK-P-602661', 'c00563158', 'C00569903', 'F36', 'c00566002', 'F29', 'F60', 'BJXK-XIMEA-FFK-P-593015', 'BJXK-XIMEA-FFK-P-593128', 'BJXK-XIMEA-FFK-P-601393', 'BJXK-XIMEA-FFK-P-592779', 'BJXK-XIMEA-FFK-P-596249', 'BJXK-XIMEA-FFK-P-601185', 'BJXK-XIMEA-FFK-P-601411', 'C00569715', 'BJXK-XIMEA-FFK-P-598926', 'BJXK-XIMEA-FFK-P-597134', 'F46', 'c00566207', 'BJXK-XIMEA-P-595259', 'F33', 'F41', 'BJXK-XIMEA-FFK-P-595871', 'F59', 'BJXK-XIMEA-FFK-P-595752', 'BJXK-XIMEA-FFK-P-592778', 'c00565704', 'BJFFK-SQ573819', 'F3', 'BJXK-XIMEA-FFK-P-594552', 'C00567183', 'BJXK-XIMEA-FFK-P-599694', 'BJXK-XIMEA-FFK-P-597669', 'BJXK-XIMEA-FFK-P-597069', 'BJFFK-SQ574677', 'BJXK-XIMEA-FFK-P-595757', 'F10', 'F32', 'BJXK-XIMEA-FFK-P-595041', 'BJFFK-SQ571266', 'BJFFK-JPL572747', 'F57', 'C00568109', 'F38', 'BJXK-XIMEA-FFK-P-596254', 'BJFFK-SCLC574011', 'BJXK-XIMEA-FFK-P-601989', 'BJFFK-SCLC571451', 'BJXK-XIMEA-FFK-P-598504', 'c00565709', 'C00564974', 'BJXK-XIMEA-FFK-P-602889', 'BJXK-XIMEA-FFK-P-602386', 'c00566203', 'F42', 'BJXK-XIMEA-FFK-P-598833', 'F34', 'BJFFK-JPL573004', 'BJXK-XIMEA-FFK-P-599695', 'BJXK-XIMEA-FFK-P-595224', 'F44', 'BJXK-XIMEA-FFK-N-602653', 'BJXFK-NILM-607487', 'BJXK-XIMEA-FFK-N-601554', 'BJTCT564874', 'BJXFK-NILM-608646', 'BJTCT569901', 'BJXFK-NILM-607389', 'BJXK-XIMEA-FFK-N-602654', 'BJXFK-NILM-603193', 'BJTCT568024', 'BJTCT563942', 'BJTCT570623', 'BJXFK-NILM-603605', 'BJTCT569371', 'BJFFK-ZS573972', 'BJXK-XIMEA-N-599025', 'BJXK-XIMEA-FFK-N-601555', 'BJXFK-NILM-608428', 'BJXK-XIMEA-FFK-N-600497', 'BJXFK-NILM-608247', 'BJXFK-NILM-590675', 'BJXFK-NILM-590582', 'BJXFK-NILM-590755', 'BJXFK-NILM-590900', 'BJTCT568022', 'BJTCT570617', 'BJXK-XIMEA-N-601392', 'BJXK-XIMEA-FFK-N-601988', 'F52', 'BJXK-XIMEA-FFK-N-599859', 'BJXFK-NILM-591329', 'BJXK-XIMEA-FFK-N-602293', 'BJXK-XIMEA-FFK-N-601283', 'BJXFK-NILM-609152', 'BJTCT570620', 'BJXFK-NILM-606591', 'BJXFK-NILM-590677', 'BJXFK-NILM-608703', 'BJXK-XIMEA-FFK-N-600716', 'BJTCT569806', 'BJFFK-ZS572911', 'BJXK-XIMEA-FFK-N-599776', 'BJXK-XIMEA-N-601314', 'BJXFK-NILM-606611', 'BJXFK-NILM-590793', 'BJXFK-NILM-606968', 'BJXFK-NILM-608939', 'BJXFK-NILM-607496', 'BJXFK-NILM-603434', 'BJXFK-NILM-607483', 'BJXFK-NILM-608168', 'BJXK-XIMEA-N-601412', 'BJXFK-NILM-591353', 'BJXFK-NILM-603301', 'BJXFK-NILM-608169', 'BJXFK-NILM-609043', 'BJXFK-NILM-591356', 'BJFFK-ZS573463', 'BJXFK-NILM-607486', 'BJXFK-NILM-601200', 'BJXFK-NILM-608873', 'BJXFK-NILM-608417', 'BJXFK-NILM-607271', 'BJXK-XIMEA-N-602285', 'BJXK-XIMEA-FFK-N-602655', 'BJXK-XIMEA-FFK-N-600714', 'BJTCT569714', 'BJXK-XIMEA-N-600972', 'BJXFK-NILM-603511', 'BJXFK-NILM-590440', 'BJXK-XIMEA-FFK-N-599715', 'BJXFK-NILM-607920', 'BJXK-XIMEA-FFK-N-600720', 'BJTCT568026', 'BJXFK-NILM-591448', 'BJFFK-ZS572458', 'BJXFK-NILM-608795', 'BJXK-XIMEA-FFK-N-599866', 'BJXFK-NILM-591074', 'BJXFK-NILM-603304', 'BJXFK-NILM-589809', 'BJXFK-NILM-607599', 'BJTCT570828', 'BJXFK-NILM-591131', 'BJXK-XIMEA-FFK-N-602185', 'BJXFK-NILM-603446', 'BJXK-XIMEA-N-602504', 'BJXFK-NILM-608777', 'BJXK-XIMEA-FFK-N-602290', 'BJXFK-NILM-607812', 'BJXK-XIMEA-N-598910', 'BJXK-XIMEA-FFK-N-598927', 'BJXFK-NILM-590462', 'BJTCT571330', 'BJXFK-NILM-607258', 'F54', 'BJXK-XIMEA-N-601391', 'BJXFK-NILM-589796', 'BJFFK-ZS575685', 'BJXFK-NILM-607598', 'BJXK-XIMEA-N-599153', 'BJXFK-NILM-607192', 'BJXFK-NILM-606991', 'BJXFK-NILM-607270', 'BJTCT570273', 'BJXK-XIMEA-N-600292', 'BJXFK-NILM-607022', 'BJXK-XIMEA-FFK-N-599629', 'BJXK-XIMEA-N-599252', 'BJXK-XIMEA-N-599088', 'BJXFK-NILM-591091', 'BJXFK-NILM-607485', 'BJXFK-NILM-591371', 'BJXFK-NILM-591229', 'BJXFK-NILM-591204', 'BJXK-XIMEA-FFK-N-601816', 'BJXFK-NILM-607513', 'BJXK-XIMEA-FFK-N-600803', 'BJTCT570826', 'BJFFK-ZS573778', 'BJXK-XIMEA-FFK-N-600218', 'BJTCT570032', 'BJXFK-NILM-603142', 'F56', 'BJXFK-NILM-609235', 'BJXFK-NILM-606705', 'BJXK-XIMEA-N-599154', 'F50', 'BJXFK-NILM-591071', 'BJXK-XIMEA-N-599226', 'BJTCT571139', 'BJXFK-NILM-608773', 'BJXK-XIMEA-FFK-N-601280', 'BJXFK-NILM-607350', 'BJXK-XIMEA-FFK-N-599865', 'BJXFK-NILM-606895', 'BJTCT570030', 'BJFFK-ZS573634', 'BJXFK-NILM-589509', 'BJFFK-ZS575657', 'BJXFK-NILM-609234', 'BJTCT570004', 'BJXFK-NILM-603690', 'BJTCT571334', 'BJTCT568030', 'BJXFK-NILM-591254', 'BJXK-XIMEA-FFK-N-600068', 'BJXFK-NILM-606929', 'BJXFK-NILM-608796', 'BJXK-XIMEA-FFK-N-600789', 'BJXFK-NILM-590510', 'BJTCT570827', 'BJTCT563941', 'BJTCT570618', 'BJXFK-NILM-590922', 'BJTCT564059', 'BJXFK-NILM-608109', 'F49', 'BJXK-XIMEA-FFK-N-599777', 'BJTCT570170', 'BJXFK-NILM-603750', 'BJTCT567895', 'BJXFK-NILM-608108', 'BJXFK-NILM-589656', 'BJXFK-NILM-603770', 'BJXFK-NILM-590974', 'BJTCT564871', 'BJXFK-NILM-609262', 'BJXFK-NILM-606832', 'BJXFK-NILM-606707', 'BJXFK-NILM-606752', 'BJXFK-NILM-607057', 'BJXFK-NILM-603487', 'BJTCT570392', 'BJXK-XIMEA-FFK-N-603331', 'BJTCT570275', 'BJXFK-NILM-608416', 'BJXFK-NILM-591443', 'BJXK-XIMEA-FFK-N-599716', 'BJXFK-NILM-590894', 'BJXK-XIMEA-FFK-N-600729', 'BJXFK-NILM-590903', 'BJXK-XIMEA-FFK-N-598914', 'BJXK-XIMEA-FFK-N-600790', 'BJTCT570622', 'BJXK-XIMEA-N-599333', 'BJXK-XIMEA-N-599229', 'BJXK-XIMEA-FFK-N-601279', 'BJXFK-NILM-603032', 'BJTCT570913', 'BJXFK-NILM-590583', 'BJXFK-NILM-608321', 'BJFFK-ZS571537', 'BJTCT571269', 'BJXFK-NILM-590756', 'BJXFK-NILM-609258', 'BJTCT570167', 'BJXFK-NILM-607719', 'BJTCT570027', 'BJXFK-NILM-606709', 'BJXK-XIMEA-FFK-N-601808', 'BJTCT569904', 'BJTCT569711', 'BJXFK-NILM-603143', 'BJFFK-ZS571909', 'F53', 'BJXFK-NILM-591481', 'BJXK-XIMEA-FFK-N-602082', 'BJXFK-NILM-606756', 'BJTCT570914', 'BJXK-XIMEA-FFK-N-599881', 'BJTCT570029', 'BJTCT570391', 'BJTCT570911', 'BJTCT567688', 'BJXFK-NILM-590526', 'BJXK-XIMEA-FFK-N-601985', 'BJTCT570396', 'BJFFK-ZS573546', 'BJTCT564058', 'BJXFK-NILM-589729', 'BJXK-XIMEA-N-599122', 'BJXFK-NILM-606927', 'BJTCT569475', 'BJXFK-NILM-606971', 'BJTCT569584', 'BJXFK-NILM-603445', 'BJXFK-NILM-603588', 'BJFFK-ZS572711', 'BJTCT569805', 'BJXK-XIMEA-FFK-N-600715', 'BJTCT568025', 'BJXFK-NILM-590461', 'BJTCT570912', 'BJXFK-NILM-608724', 'BJXFK-NILM-590465', 'BJXK-XIMEA-FFK-N-599779', 'BJXFK-NILM-590904', 'BJTCT564164', 'BJXFK-NILM-608772', 'BJXFK-NILM-591065', 'BJXFK-NILM-590321', 'BJXFK-NILM-608415', 'BJXK-XIMEA-FFK-N-598824', 'BJTCT571333', 'BJFFK-ZS572891', 'BJTCT569369', 'BJTCT569582', 'BJXFK-NILM-590460', 'BJXFK-NILM-591225', 'BJXFK-NILM-608704', 'BJTCT570829', 'BJTCT567687', 'BJXFK-NILM-591355', 'BJXFK-NILM-603507', 'BJTCT564168', 'BJXFK-NILM-603191', 'BJXFK-NILM-607495', 'BJTCT570397', 'BJXFK-NILM-608702', 'BJXFK-NILM-609039', 'BJTCT569710', 'BJXFK-NILM-591256', 'BJXFK-NILM-601203', 'BJXK-XIMEA-FFK-N-599532', 'BJXFK-NILM-608774', 'BJXK-XIMEA-FFK-N-602196', 'BJFFK-ZS573914', 'BJXFK-NILM-603195', 'BJTCT571136', 'BJXK-XIMEA-FFK-N-600804', 'BJXK-XIMEA-FFK-N-600291', 'F51', 'BJXK-XIMEA-FFK-N-603314', 'BJXFK-NILM-609295', 'BJXK-XIMEA-FFK-N-600578', 'BJXFK-NILM-608701', 'BJXFK-NILM-601184', 'BJXFK-NILM-608775', 'BJXK-XIMEA-FFK-N-598948', 'BJXK-XIMEA-N-599228', 'BJXFK-NILM-591354', 'BJXK-XIMEA-FFK-N-601987', 'BJXFK-NILM-607349', 'BJXFK-NILM-589819', 'BJXFK-NILM-606803', 'BJTCT564873', 'BJXFK-NILM-607921', 'BJFFK-ZS572890', 'BJTCT569581', 'BJTCT564055', 'BJTCT569476', 'BJXFK-NILM-591606', 'BJXFK-NILM-607489', 'BJTCT571138', 'BJXK-XIMEA-N-599253', 'BJXK-XIMEA-FFK-N-601556', 'BJTCT564056', 'BJXFK-NILM-607351', 'BJXFK-NILM-591059', 'BJXK-XIMEA-N-603136', 'BJXFK-NILM-589612', 'BJXFK-NILM-607491', 'BJTCT564162', 'BJXFK-NILM-591255', 'BJTCT568031', 'BJTCT571135', 'BJXK-XIMEA-FFK-N-601990', 'BJTCT571137', 'BJTCT568023', 'BJXK-XIMEA-FFK-N-600053', 'F55', 'BJXFK-NILM-607096', 'BJXFK-NILM-603364', 'BJXFK-NILM-608934', 'BJXFK-NILM-608246', 'BJTCT568032', 'BJXFK-NILM-589657', 'BJTCT570915', 'BJXFK-NILM-591546', 'BJTCT564870', 'BJXFK-NILM-590459', 'BJXFK-NILM-591139', 'BJXFK-NILM-607257', 'BJXK-XIMEA-FFK-N-599024', 'BJFFK-ZS573868']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "wsis = []\n",
    "file_name = './simclr/train_label.csv'\n",
    "with open(file_name, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for row in lines:\n",
    "        row = row.strip().split(',')\n",
    "        wsis.append(row[0])\n",
    "print(wsis)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home1/wsi/tct/NILM/xCR20018818-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005897-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006000-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006569-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179985-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166624-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175637-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018330-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008034-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006621-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018882-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005843-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179988-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018485-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018268-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006943-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152317-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007765-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171594-NILM',\n",
       " '/home1/wsi/tct/NILM/202000822',\n",
       " '/home1/wsi/tct/NILM/CX20176960-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006719-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006691-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007597-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174926-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007709-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018261-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152310-NILM',\n",
       " '/home1/wsi/tct/NILM/202000779',\n",
       " '/home1/wsi/tct/NILM/xCR20018855-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152324-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167394-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006568-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166714-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018862-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007954-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152312-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179970-NILM',\n",
       " '/home1/wsi/tct/NILM/202000878',\n",
       " '/home1/wsi/tct/NILM/202000844',\n",
       " '/home1/wsi/tct/NILM/xCY20005811-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010722-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174991-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017708-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018342-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180074-NILM',\n",
       " '/home1/wsi/tct/NILM/202000908',\n",
       " '/home1/wsi/tct/NILM/CX20152372-NILM',\n",
       " '/home1/wsi/tct/NILM/202000842',\n",
       " '/home1/wsi/tct/NILM/xCY20006615-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174958-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176083-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176087-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177011-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010692-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018970-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006651-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018661-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018363-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166674-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166625-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018509-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177067-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174893-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006863-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20011063-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176096-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177908-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174917-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018382-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006908-NILM',\n",
       " '/home1/wsi/tct/NILM/202000774',\n",
       " '/home1/wsi/tct/NILM/xCR20018491-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180044-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007844-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018061-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006710-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018441-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174985-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152410-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166629-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176092-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175576-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018152-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018573-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152390-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006599-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018248-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176151-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152295-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018522-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176124-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177859-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007628-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006896-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167393-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167375-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006783-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010709-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005815-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177837-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180936-NILM',\n",
       " '/home1/wsi/tct/NILM/202000940',\n",
       " '/home1/wsi/tct/NILM/202000857',\n",
       " '/home1/wsi/tct/NILM/xCY20006792-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006782-NILM',\n",
       " '/home1/wsi/tct/NILM/202000795',\n",
       " '/home1/wsi/tct/NILM/xCY20007592-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180061-NILM',\n",
       " '/home1/wsi/tct/NILM/202000860',\n",
       " '/home1/wsi/tct/NILM/CX20172446-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006609-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005820-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018294-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006588-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005857-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177899-NILM',\n",
       " '/home1/wsi/tct/NILM/202000948',\n",
       " '/home1/wsi/tct/NILM/xCY20007964-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175566-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010727-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017996-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005792-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018314-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018902-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175640-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176112-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018675-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006595-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007835-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006551-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176115-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018319-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017869-NILM',\n",
       " '/home1/wsi/tct/NILM/202000828',\n",
       " '/home1/wsi/tct/NILM/CX20174896-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018906-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005916-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179979-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174961-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005995-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018512-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008015-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006618-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017730-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152384-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018529-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018460-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005826-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006772-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176127-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007961-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008014-NILM',\n",
       " '/home1/wsi/tct/NILM/202000816',\n",
       " '/home1/wsi/tct/NILM/CX20152395-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166724-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152397-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018937-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010696-NILM',\n",
       " '/home1/wsi/tct/NILM/202000893',\n",
       " '/home1/wsi/tct/NILM/xCY20010663-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177034-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174883-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20172449-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177888-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006857-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171626-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005917-NILM',\n",
       " '/home1/wsi/tct/NILM/202000768',\n",
       " '/home1/wsi/tct/NILM/xCR20018638-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175577-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167379-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008010-NILM',\n",
       " '/home1/wsi/tct/NILM/202000938',\n",
       " '/home1/wsi/tct/NILM/CX20176126-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174897-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166641-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018226-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175586-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018422-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008031-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005817-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006581-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005844-NILM',\n",
       " '/home1/wsi/tct/NILM/202000926',\n",
       " '/home1/wsi/tct/NILM/xCR20018513-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175649-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006580-NILM',\n",
       " '/home1/wsi/tct/NILM/202000933',\n",
       " '/home1/wsi/tct/NILM/202000787',\n",
       " '/home1/wsi/tct/NILM/CX20152399-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018517-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005922-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018088-NILM',\n",
       " '/home1/wsi/tct/NILM/202000868',\n",
       " '/home1/wsi/tct/NILM/xCR20018128-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006005-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005847-NILM',\n",
       " '/home1/wsi/tct/NILM/CX202012383-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176122-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175650-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175612-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008030-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018156-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005834-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152364-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018616-NILM',\n",
       " '/home1/wsi/tct/NILM/202000794',\n",
       " '/home1/wsi/tct/NILM/xCY20005754-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166677-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006600-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175583-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152347-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005906-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005937-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167406-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007617-NILM',\n",
       " '/home1/wsi/tct/NILM/202000845',\n",
       " '/home1/wsi/tct/NILM/xCR20017825-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008062-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166713-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006608-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180055-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177833-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174994-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017709-NILM',\n",
       " '/home1/wsi/tct/NILM/202000861',\n",
       " '/home1/wsi/tct/NILM/xCY20006667-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175636-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166711-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174978-NILM',\n",
       " '/home1/wsi/tct/NILM/202000796',\n",
       " '/home1/wsi/tct/NILM/CX20174916-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152344-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006634-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018537-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010717-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010669-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007817-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007603-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017844-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006867-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005980-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20019002-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006584-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180926-NILM',\n",
       " '/home1/wsi/tct/NILM/202000830',\n",
       " '/home1/wsi/tct/NILM/xCR20018929-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176973-NILM',\n",
       " '/home1/wsi/tct/NILM/202000864',\n",
       " '/home1/wsi/tct/NILM/202000866',\n",
       " '/home1/wsi/tct/NILM/CX20152362-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171623-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006830-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005863-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018146-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005876-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175638-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152337-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171612-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018078-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152366-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20019006-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018705-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166639-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174886-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167402-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174932-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018989-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167382-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018356-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175002-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018166-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166701-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167380-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007826-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175658-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006885-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010668-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018796-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179991-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177901-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007777-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007776-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176077-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177038-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171609-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174975-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174999-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007613-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152401-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177910-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174931-NILM',\n",
       " '/home1/wsi/tct/NILM/202000935',\n",
       " '/home1/wsi/tct/NILM/xCY20007767-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006795-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167377-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010732-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007834-NILM',\n",
       " '/home1/wsi/tct/NILM/202000953',\n",
       " '/home1/wsi/tct/NILM/xCY20008048-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018630-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152335-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171622-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180064-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017943-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018444-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152409-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010708-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010664-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175611-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166686-NILM',\n",
       " '/home1/wsi/tct/NILM/202000942',\n",
       " '/home1/wsi/tct/NILM/CX20177850-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010693-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010729-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010714-NILM',\n",
       " '/home1/wsi/tct/NILM/202000848',\n",
       " '/home1/wsi/tct/NILM/xCR20018810-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018979-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006591-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152287-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006763-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176132-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152325-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006638-NILM',\n",
       " '/home1/wsi/tct/NILM/202000865',\n",
       " '/home1/wsi/tct/NILM/CX20174927-NILM',\n",
       " '/home1/wsi/tct/NILM/202000783',\n",
       " '/home1/wsi/tct/NILM/xCY20005763-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007842-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176966-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175603-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018923-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005945-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176098-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166649-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006589-NILM',\n",
       " '/home1/wsi/tct/NILM/202000767',\n",
       " '/home1/wsi/tct/NILM/CX20180060-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166702-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006656-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018217-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166726-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167361-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176097-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177861-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007783-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176986-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180944-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166675-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006843-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166645-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175606-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008043-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006821-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006706-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005994-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018402-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018301-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177042-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166728-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174937-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005948-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005830-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018674-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176063-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017767-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018652-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171602-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176133-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176053-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177073-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152293-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152284-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175664-NILM',\n",
       " '/home1/wsi/tct/NILM/202000859',\n",
       " '/home1/wsi/tct/NILM/xCY20010716-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006842-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010700-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175622-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006882-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152302-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006672-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006676-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018602-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018800-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174948-NILM',\n",
       " '/home1/wsi/tct/NILM/202000927',\n",
       " '/home1/wsi/tct/NILM/CX20166690-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177848-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152353-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006001-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166717-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005765-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018044-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008024-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167363-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177827-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167395-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175659-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166623-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167409-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175633-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007782-NILM',\n",
       " '/home1/wsi/tct/NILM/202000786',\n",
       " '/home1/wsi/tct/NILM/CX20171645-NILM',\n",
       " '/home1/wsi/tct/NILM/202000903',\n",
       " '/home1/wsi/tct/NILM/xCR20018120-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152357-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018530-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006808-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010713-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171606-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005866-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166648-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006715-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174903-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152331-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180040-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177851-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010655-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005766-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007811-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018129-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018553-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005942-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017796-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174954-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006873-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018684-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017921-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152354-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010688-NILM',\n",
       " '/home1/wsi/tct/NILM/202000890',\n",
       " '/home1/wsi/tct/NILM/CX20177025-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180016-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166700-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177894-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006630-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017973-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018792-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177914-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018299-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152341-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010675-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167372-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018668-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005814-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017818-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152391-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152342-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007830-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006787-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006410-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152321-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008053-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018998-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010731-NILM',\n",
       " '/home1/wsi/tct/NILM/202000928',\n",
       " '/home1/wsi/tct/NILM/xCR20017905-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018071-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010721-NILM',\n",
       " '/home1/wsi/tct/NILM/202000835',\n",
       " '/home1/wsi/tct/NILM/CX20171642-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017792-NILM',\n",
       " '/home1/wsi/tct/NILM/202000955',\n",
       " '/home1/wsi/tct/NILM/xCR20018932-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006845-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176996-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007831-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152323-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010680-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010699-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010690-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010720-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018988-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006917-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005981-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171613-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007632-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152373-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152356-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006948-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166636-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176962-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008011-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010694-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152307-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167407-NILM',\n",
       " '/home1/wsi/tct/NILM/202000791',\n",
       " '/home1/wsi/tct/NILM/CX20167355-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174941-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152352-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152315-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167378-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180002-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007636-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006866-NILM',\n",
       " '/home1/wsi/tct/NILM/202000836',\n",
       " '/home1/wsi/tct/NILM/CX20174968-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006747-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018721-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006831-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018516-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007808-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010723-NILM',\n",
       " '/home1/wsi/tct/NILM/202000770',\n",
       " '/home1/wsi/tct/NILM/xCR20018634-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152333-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166729-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018396-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018610-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166721-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152377-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005880-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177845-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007819-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176992-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152380-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006594-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152378-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007627-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005783-NILM',\n",
       " '/home1/wsi/tct/NILM/202000884',\n",
       " '/home1/wsi/tct/NILM/xCR20018536-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018985-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180037-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005750-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005912-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007773-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018649-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007720-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152340-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017859-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177015-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174892-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007724-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008020-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007637-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017809-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018971-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006721-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175632-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179999-NILM',\n",
       " '/home1/wsi/tct/NILM/170657627-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017870-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006693-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152407-NILM',\n",
       " '/home1/wsi/tct/NILM/202000824',\n",
       " '/home1/wsi/tct/NILM/CX20176982-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007602-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018698-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174969-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018669-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006743-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006657-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007797-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008009-NILM',\n",
       " '/home1/wsi/tct/NILM/202000952',\n",
       " '/home1/wsi/tct/NILM/xCR20018007-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018847-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006819-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180027-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180071-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167354-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152406-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006628-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006624-NILM',\n",
       " '/home1/wsi/tct/NILM/202000781',\n",
       " '/home1/wsi/tct/NILM/CX20167357-NILM',\n",
       " '/home1/wsi/tct/NILM/202000843',\n",
       " '/home1/wsi/tct/NILM/xCY20006576-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175563-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174965-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018681-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177024-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20178936-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006620-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010711-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167367-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175594-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20172452-NILM',\n",
       " '/home1/wsi/tct/NILM/202000918',\n",
       " '/home1/wsi/tct/NILM/CX20174902-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010710-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152332-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180068-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018134-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176146-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010670-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152370-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180937-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008067-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005809-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179995-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175623-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017880-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167401-NILM',\n",
       " '/home1/wsi/tct/NILM/202000855',\n",
       " '/home1/wsi/tct/NILM/xCR20018528-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175587-NILM',\n",
       " '/home1/wsi/tct/NILM/202000924',\n",
       " '/home1/wsi/tct/NILM/CX20174907-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005804-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177072-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018857-NILM',\n",
       " '/home1/wsi/tct/NILM/202000833',\n",
       " '/home1/wsi/tct/NILM/xCY20005892-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018197-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166705-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018309-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166635-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174945-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167391-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006847-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018615-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006610-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152411-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006683-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007801-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180049-NILM',\n",
       " '/home1/wsi/tct/NILM/202000789',\n",
       " '/home1/wsi/tct/NILM/xCY20006932-NILM',\n",
       " '/home1/wsi/tct/NILM/202000931',\n",
       " '/home1/wsi/tct/NILM/xCR20018856-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152369-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152328-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006883-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010677-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005818-NILM',\n",
       " '/home1/wsi/tct/NILM/202000817',\n",
       " '/home1/wsi/tct/NILM/xCY20006804-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006738-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152313-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152289-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005998-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018664-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018657-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174984-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018958-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166613-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166698-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171592-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018677-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175648-NILM',\n",
       " '/home1/wsi/tct/NILM/202000945',\n",
       " '/home1/wsi/tct/NILM/xCY20010672-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176066-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166719-NILM',\n",
       " '/home1/wsi/tct/NILM/202000932',\n",
       " '/home1/wsi/tct/NILM/CX20180041-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006762-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005777-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152351-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152412-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166646-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176144-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018864-NILM',\n",
       " '/home1/wsi/tct/NILM/202000906',\n",
       " '/home1/wsi/tct/NILM/202000854',\n",
       " '/home1/wsi/tct/NILM/CX20166633-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018700-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018406-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166707-NILM',\n",
       " '/home1/wsi/tct/NILM/202000793',\n",
       " '/home1/wsi/tct/NILM/202000874',\n",
       " '/home1/wsi/tct/NILM/xCR20018032-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175645-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152355-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018907-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007824-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006601-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167398-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006877-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018090-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018075-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018904-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176071-NILM',\n",
       " '/home1/wsi/tct/NILM/202000902',\n",
       " '/home1/wsi/tct/NILM/CX20166689-NILM',\n",
       " '/home1/wsi/tct/NILM/202000939',\n",
       " '/home1/wsi/tct/NILM/202000880',\n",
       " '/home1/wsi/tct/NILM/CX20167369-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20011067-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006575-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005985-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176963-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008019-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006614-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007601-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152371-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018611-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018232-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152334-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010703-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006839-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018699-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018990-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018270-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010654-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166716-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152396-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174918-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005797-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180948-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179994-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017763-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018459-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018918-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167396-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017845-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152394-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166684-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006835-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018135-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018412-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167374-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006834-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018577-NILM',\n",
       " '/home1/wsi/tct/NILM/202000889',\n",
       " '/home1/wsi/tct/NILM/CX20174955-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167366-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006409-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006921-NILM',\n",
       " '/home1/wsi/tct/NILM/202000826',\n",
       " '/home1/wsi/tct/NILM/CX20166653-NILM',\n",
       " '/home1/wsi/tct/NILM/202000792',\n",
       " '/home1/wsi/tct/NILM/CX20166622-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018718-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171603-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171637-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018325-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176067-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005907-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152314-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20172447-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176082-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177881-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177868-NILM',\n",
       " '/home1/wsi/tct/NILM/202000773',\n",
       " '/home1/wsi/tct/NILM/CX20171649-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177879-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006411-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006003-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167359-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174995-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007711-NILM',\n",
       " '/home1/wsi/tct/NILM/202000775',\n",
       " '/home1/wsi/tct/NILM/xCY20005977-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152375-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006663-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005988-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007593-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018682-NILM',\n",
       " '/home1/wsi/tct/NILM/202000896-',\n",
       " '/home1/wsi/tct/NILM/CX20166615-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177063-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018298-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176057-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167400-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171641-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166699-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017909-NILM',\n",
       " '/home1/wsi/tct/NILM/202000772',\n",
       " '/home1/wsi/tct/NILM/xCY20007800-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018666-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018582-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175593-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176993-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166691-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018413-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018772-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018678-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152403-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166709-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174906-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20178937-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005969-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152301-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006702-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006820-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177890-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017783-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171655-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018346-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174981-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017805-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176069-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018656-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008035-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006897-NILM',\n",
       " '/home1/wsi/tct/NILM/202000916',\n",
       " '/home1/wsi/tct/NILM/xCY20006714-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175641-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180059-NILM',\n",
       " '/home1/wsi/tct/NILM/202000944',\n",
       " '/home1/wsi/tct/NILM/xCY20007715-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176977-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007728-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152350-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167389-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018124-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006944-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018130-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175627-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006579-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010686-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152368-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166619-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174944-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166667-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008039-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177865-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006949-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171596-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007762-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180026-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20017986-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177028-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166614-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006793-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20008066-NILM',\n",
       " '/home1/wsi/tct/NILM/202000797',\n",
       " '/home1/wsi/tct/NILM/CX20180949-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152318-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006578-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180013-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174949-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006876-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018373-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007612-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152382-NILM',\n",
       " '/home1/wsi/tct/NILM/202000907',\n",
       " '/home1/wsi/tct/NILM/xCY20006811-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005887-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176072-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177000-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174960-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177855-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166669-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010679-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018838-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175655-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176149-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20178926-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177021-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177891-NILM',\n",
       " '/home1/wsi/tct/NILM/202000832',\n",
       " '/home1/wsi/tct/NILM/xCR20018544-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018926-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006731-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174912-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166671-NILM',\n",
       " '/home1/wsi/tct/NILM/202000946',\n",
       " '/home1/wsi/tct/NILM/CX20177918-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010683-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018892-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005960-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175596-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010687-NILM',\n",
       " '/home1/wsi/tct/NILM/202000841',\n",
       " '/home1/wsi/tct/NILM/CX20174959-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176155-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005869-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007950-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176113-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006916-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005838-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166666-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005810-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007606-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006686-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167364-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006570-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166672-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010658-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176141-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005829-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179978-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177874-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166685-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005753-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018730-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177008-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179941-NILM',\n",
       " '/home1/wsi/tct/NILM/202000919',\n",
       " '/home1/wsi/tct/NILM/xCY20005821-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005971-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006664-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010735-NILM',\n",
       " '/home1/wsi/tct/NILM/202000943',\n",
       " '/home1/wsi/tct/NILM/CX20175642-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018510-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20179968-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005936-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177864-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176136-NILM',\n",
       " '/home1/wsi/tct/NILM/202000862',\n",
       " '/home1/wsi/tct/NILM/xCY20006810-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175609-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018629-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20169246-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010667-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018696-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005796-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175607-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20177841-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174988-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171593-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167385-NILM',\n",
       " '/home1/wsi/tct/NILM/202000929',\n",
       " '/home1/wsi/tct/NILM/CX20152404-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174980-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007839-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010673-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176076-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166632-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006678-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005842-NILM',\n",
       " '/home1/wsi/tct/NILM/202000788',\n",
       " '/home1/wsi/tct/NILM/202000785',\n",
       " '/home1/wsi/tct/NILM/xCY20010682-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018026-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018118-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171650-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175616-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171597-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010674-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005853-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005877-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005862-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018825-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20175617-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166697-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018824-NILM',\n",
       " '/home1/wsi/tct/NILM/202000847',\n",
       " '/home1/wsi/tct/NILM/CX20176967-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018643-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166655-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006720-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007960-NILM',\n",
       " '/home1/wsi/tct/NILM/202000950',\n",
       " '/home1/wsi/tct/NILM/CX20180941-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176052-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166706-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018703-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171636-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018511-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174979-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176106-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005875-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005989-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176145-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20167362-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018658-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166681-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166650-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20176102-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007725-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018290-NILM',\n",
       " '/home1/wsi/tct/NILM/202000819',\n",
       " '/home1/wsi/tct/NILM/202000790',\n",
       " '/home1/wsi/tct/NILM/CX20174887-NILM',\n",
       " '/home1/wsi/tct/NILM/202000887',\n",
       " '/home1/wsi/tct/NILM/CX20167397-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174998-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152319-NILM',\n",
       " '/home1/wsi/tct/NILM/202000914',\n",
       " '/home1/wsi/tct/NILM/xCY20006910-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180017-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171651-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180058-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166627-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006587-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018284-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018835-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20180033-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018618-NILM',\n",
       " '/home1/wsi/tct/NILM/202000954',\n",
       " '/home1/wsi/tct/NILM/202000891',\n",
       " '/home1/wsi/tct/NILM/xCY20006666-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20007818-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018637-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018991-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018254-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20152398-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20171627-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20174989-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166670-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20006907-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20010724-NILM',\n",
       " '/home1/wsi/tct/NILM/202000776',\n",
       " '/home1/wsi/tct/NILM/xCY20005913-NILM',\n",
       " '/home1/wsi/tct/NILM/CX20166688-NILM',\n",
       " '/home1/wsi/tct/NILM/xCY20005902-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018279-NILM',\n",
       " '/home1/wsi/tct/NILM/202000831',\n",
       " '/home1/wsi/tct/NILM/CX20171654-NILM',\n",
       " '/home1/wsi/tct/NILM/xCR20018566-NILM',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "wsi_root = '/home1/wsi/tct/'\n",
    "sub_paths = [\n",
    "            'NILM',\n",
    "            'POS'\n",
    "        ]\n",
    "wsi_dirs = []\n",
    "for sub_path in sub_paths:\n",
    "    wsi_dirs.extend([os.path.join(wsi_root, sub_path, wsi_name) for wsi_name in os.listdir(os.path.join(wsi_root, sub_path))])\n",
    "# data_roots = list(map(lambda x: os.path.join(wsi_root, x), sub_paths))\n",
    "# data_roots\n",
    "# wsi_dirs = [os.path.join(wsi_root, sub_dir) for sub_dir in os.listdir(data_roots) ]\n",
    "wsi_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import glob\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                          transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                          transforms.ToTensor()])\n",
    "    \n",
    "\n",
    "train_set = Whole_Slide_Patchs_Ngc(\n",
    "            data_dir='/home1/wsi/ngc-2023-1333/',\n",
    "            train_label_path='/home/huangjialong/projects/BiomedCLIP-PUNCE/datatools/ngc_labels/ngc_train_label.csv',\n",
    "            transform=train_transform\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]] tensor([[0.2765, 0.1852, 0.2274, 0.1506, 0.1602],\n",
      "        [0.2159, 0.1409, 0.1849, 0.2888, 0.1695],\n",
      "        [0.1244, 0.1427, 0.2141, 0.2983, 0.2204],\n",
      "        [0.1451, 0.2369, 0.2650, 0.2379, 0.1151],\n",
      "        [0.1354, 0.1046, 0.2688, 0.2269, 0.2644],\n",
      "        [0.2930, 0.1570, 0.2331, 0.1355, 0.1814],\n",
      "        [0.2257, 0.3268, 0.1634, 0.1427, 0.1414],\n",
      "        [0.2123, 0.1062, 0.2431, 0.2417, 0.1966],\n",
      "        [0.2173, 0.2060, 0.2000, 0.2304, 0.1462],\n",
      "        [0.1517, 0.2533, 0.1651, 0.2723, 0.1577]], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37846/3189169087.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_label_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mroc_auc_macro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dist-pu/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         )\n\u001b[1;32m    574\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dist-pu/lib/python3.7/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dist-pu/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         raise ValueError(\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;34m\"Only one class present in y_true. ROC AUC score \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"is not defined in that case.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "n_classes = 5\n",
    "y_label = np.random.randint(0,n_classes, (10))\n",
    "y_pred = np.random.rand(10, n_classes)\n",
    "y_pred = torch.softmax(torch.tensor(y_pred), dim=1)\n",
    "y_label_one_hot = np.eye(n_classes)[y_label]\n",
    "print(y_label_one_hot, y_pred)\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    roc_auc[i] = roc_auc_score(y_label_one_hot[:, i], y_pred[:, i])\n",
    "roc_auc = list(roc_auc.values())\n",
    "roc_auc_macro = np.mean(roc_auc)\n",
    "roc_auc_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[10][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565720\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root_dir = '/home1/wsi/ngc-2023-224/'\n",
    "sub_dirs = [\n",
    "    'Unannotated_KSJ/Unannotated-KSJ-TCTNGC-NILM',\n",
    "    'Unannotated_KSJ/Unannotated-KSJ-TCTNGC-POS',\n",
    "    'Unannotated_XIMEA/Unannotated-XIMEA-TCTNGC-NILM',\n",
    "    'Unannotated_XIMEA/Unannotated-XIMEA-TCTNGC-POS'\n",
    "]\n",
    "sum_wsi = 0\n",
    "for sub_dir in sub_dirs:\n",
    "    sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "    sum_wsi += len(os.listdir(sub_dir_path))\n",
    "sum_wsi\n",
    "sum_images = 0\n",
    "\n",
    "def count_fun(sub_dir):\n",
    "    sum_images = 0\n",
    "    input_dir_tmp = os.path.join(root_dir, sub_dir)\n",
    "    for root, dirs, files in os.walk(input_dir_tmp):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png') or filename.endswith('.jpeg'):\n",
    "                sum_images += 1\n",
    "    return sum_images \n",
    "for sub_dir in sub_dirs:\n",
    "    sum_images += count_fun(sub_dir)\n",
    "print(sum_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "from torchmetrics.classification import BinarySpecificity\n",
    "target = tensor([0, 1, 0, 1, 0, 1])\n",
    "preds = tensor([0, 0, 1, 1, 0, 1])\n",
    "metric = BinarySpecificity()\n",
    "metric(preds, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adapter': OrderedDict([('hide_layer.weight',\n",
       "               tensor([[ 0.0109, -0.0350,  0.0311,  ...,  0.0307,  0.0095,  0.0083],\n",
       "                       [-0.0034,  0.0327,  0.0025,  ..., -0.0300,  0.0202,  0.0268],\n",
       "                       [ 0.0229,  0.0451,  0.0338,  ...,  0.0130, -0.0130,  0.0236],\n",
       "                       ...,\n",
       "                       [-0.0213,  0.0386,  0.0194,  ..., -0.0484, -0.0293, -0.0139],\n",
       "                       [ 0.0197,  0.0509,  0.0287,  ..., -0.0375,  0.0119,  0.0043],\n",
       "                       [ 0.0115,  0.0238,  0.0194,  ...,  0.0040, -0.0067, -0.0480]],\n",
       "                      device='cuda:0')),\n",
       "              ('hide_layer.bias',\n",
       "               tensor([-0.2269,  0.2048,  0.1236,  0.0651,  0.0013,  0.0025,  0.1936,  0.0200,\n",
       "                        0.2058,  0.2394,  0.2005,  0.4795,  0.3440, -0.0141,  0.6812,  0.0355,\n",
       "                        0.5717, -0.5975,  0.0265, -0.0436, -0.1840, -0.1434,  0.1415, -0.0168,\n",
       "                       -0.2090,  0.0154,  0.0908,  0.0705, -0.1890,  0.5126, -0.1928,  0.2626,\n",
       "                        0.0113,  0.3986,  0.2094, -0.0591, -0.2312, -0.3872, -0.0144,  0.2650,\n",
       "                        0.0693, -0.1298, -0.0531,  0.0235,  0.1087, -0.2287,  0.1161,  0.1306,\n",
       "                        0.1457, -0.3340,  0.1522, -0.0369, -0.2120, -0.4814,  0.1612,  0.2128,\n",
       "                        0.0832,  0.1036,  0.2788, -0.0563, -0.4531, -0.2633,  0.0558,  0.2704,\n",
       "                       -0.3633,  0.0507, -0.3508, -0.3760, -0.0176, -0.1021, -0.3568, -0.1752,\n",
       "                        0.1245,  0.4145, -0.1757,  0.2691,  0.1051,  0.2463, -0.0182,  0.2083,\n",
       "                       -0.1582, -0.0884,  0.1760, -0.4594, -0.2763, -0.3737, -0.0862,  0.4208,\n",
       "                       -0.4323, -0.0222,  0.2747,  0.3981, -0.2772, -0.0369, -0.2640,  0.6869,\n",
       "                       -0.3667,  0.0197,  0.3617, -0.2209,  0.5264, -0.2027, -0.0075, -0.1217,\n",
       "                        0.1318,  0.2203,  0.0984, -0.1157,  0.2255, -0.1033,  0.2651, -0.0802,\n",
       "                        0.0296,  0.0707, -0.5421,  0.1179, -0.2282, -0.1173,  0.1497, -0.0624,\n",
       "                        0.1941, -0.1322, -0.1754,  0.0166,  0.1966,  0.3354,  0.0746, -0.0333,\n",
       "                        0.4619, -0.1859, -0.3292, -0.3565, -0.2024, -0.1192,  0.3430,  0.3034,\n",
       "                        0.0639, -0.3540, -0.0054,  0.3323,  0.0845,  0.3982, -0.1563,  0.0612,\n",
       "                        0.3903,  0.9396,  0.5902,  0.5811,  0.0223,  0.0028, -0.3297, -0.3580,\n",
       "                        0.1655, -0.2856,  0.4666, -0.1440,  0.0088, -0.2315, -0.1788,  0.2399,\n",
       "                       -0.2923,  0.0402,  0.1058,  0.3623, -0.0758,  0.4662,  0.0860,  0.8003,\n",
       "                       -0.2788,  0.3541, -0.0210,  0.0374, -0.3666,  0.2507, -0.3797,  0.0043,\n",
       "                       -0.2281, -0.8224, -0.1736,  0.2285,  0.2384, -0.4163, -0.0742, -0.1056,\n",
       "                       -0.0657, -0.0952,  0.2854, -0.1497,  0.2760,  0.2234, -0.1087, -0.2329,\n",
       "                        0.4587, -0.4567,  0.1523, -0.6213,  0.0477, -0.6065, -0.0673,  0.2349,\n",
       "                        0.0032,  0.1735,  0.3396, -0.0803,  0.3485, -0.0051,  0.4782, -0.0375,\n",
       "                       -0.5675,  0.0589, -0.1845, -0.3379,  0.1217, -0.0546,  0.5778,  0.1087,\n",
       "                       -0.0691,  0.5391,  0.3023,  0.2132, -0.0567,  0.7811, -0.4872,  0.6100,\n",
       "                       -0.1444,  0.4493, -0.1104,  0.0063, -0.3918, -0.1295,  0.0748, -0.0829,\n",
       "                       -0.2680, -0.0254,  0.2536,  0.2818,  0.2583,  0.0946, -0.5029, -0.1781,\n",
       "                        0.4593, -0.1316, -0.2107,  0.0117,  0.2376,  0.5711, -0.0815, -0.3324,\n",
       "                        0.4939,  0.3245,  0.0472,  0.0903,  0.1290,  0.5142,  0.1938,  0.1522,\n",
       "                        0.3270,  0.2809,  0.1220, -0.2182, -0.0668,  0.1894, -0.0716, -0.4396,\n",
       "                        0.1663, -0.1671,  0.0144,  0.0385,  0.0150, -0.1736, -0.1175, -0.5836,\n",
       "                       -0.2708, -0.3119,  0.1161, -0.1064, -0.1811, -0.2274, -0.0108, -0.2891,\n",
       "                        0.2244,  0.0542,  0.2477, -0.6498, -0.2817, -0.0585,  0.0395,  0.1392,\n",
       "                        0.4983, -0.1630, -0.2354, -0.0081,  0.0274, -0.1583,  0.0020,  0.4802,\n",
       "                        0.0230,  0.2770, -0.1400, -0.0157,  0.5189, -0.1199,  0.0190,  0.1366,\n",
       "                        0.1621,  0.2588, -0.3376, -0.4225,  0.0843,  0.1093,  0.5585, -0.3215,\n",
       "                        0.3545,  0.0761, -0.1998, -0.2854, -0.2995,  0.0615, -0.1016,  0.2463,\n",
       "                       -0.3563,  0.0596, -0.1932,  0.0418,  0.1075, -0.0057, -0.5165, -0.1357,\n",
       "                       -0.1910,  0.0992, -0.2330,  0.1906, -0.3124, -0.1056,  0.0060, -0.4374,\n",
       "                       -0.1837,  0.3391,  0.6745, -0.1972,  0.1070, -0.0888,  0.1992, -0.3762,\n",
       "                       -0.2623,  0.5184, -0.0256,  0.1372,  0.1878,  0.0744,  0.0663, -0.0699,\n",
       "                       -0.0826, -0.2583,  0.4387,  0.3041,  0.1807, -0.4288,  0.0693, -0.2645,\n",
       "                       -0.2894,  0.1126,  0.0961,  0.1002,  0.0756,  0.3180,  0.1088,  0.0215,\n",
       "                        0.5383, -0.5326,  0.2845, -0.2799, -0.4761,  0.4583, -0.0900, -0.1140,\n",
       "                       -0.3022, -0.0885, -0.0407, -0.2089, -0.0289, -0.1899,  0.0514, -0.3177,\n",
       "                       -0.4575, -0.0521,  0.1387,  0.3800, -0.0326, -0.0864,  0.0156,  0.1973,\n",
       "                       -0.7044,  0.0412, -0.1542,  0.2891, -0.1770, -0.2625, -0.0884, -0.0536,\n",
       "                       -0.3185,  0.1105, -0.2542,  0.3613,  0.0294, -0.2507, -0.2512,  0.2473,\n",
       "                        0.1659,  0.0827,  0.2904, -0.0322, -0.3003, -0.4083,  0.3434, -0.0357,\n",
       "                       -0.1626, -0.0378,  0.1634,  0.3323, -0.1051, -0.7092, -0.2208,  0.1961,\n",
       "                       -0.1371,  0.2236, -0.2635,  0.1897, -0.0382, -0.1176, -0.2170, -0.4185,\n",
       "                        0.1733, -0.0455,  0.0434,  0.0769, -0.1712,  0.2825, -0.2794, -0.0397,\n",
       "                        0.2314, -0.0999, -0.1710, -0.1840,  0.4951,  0.1137, -0.0351,  0.5067,\n",
       "                       -0.3249,  0.0232,  0.4402, -0.4521,  0.1085,  0.2530,  0.1988, -0.2003,\n",
       "                       -0.4291, -0.6581, -0.9080, -0.0604, -0.2161,  0.0160, -0.1562,  0.2735,\n",
       "                       -0.1837,  0.5959, -0.0527, -0.4692,  0.0795,  0.3432,  0.0339,  0.1346,\n",
       "                        0.0715, -0.1385, -0.0902,  0.0775,  0.1433, -0.0036,  0.1951,  0.0725,\n",
       "                       -0.3238,  0.1095, -0.1654, -0.0014,  0.2745,  0.2242, -0.0986,  0.2521,\n",
       "                       -0.1787,  0.1251, -0.3883, -0.1318,  0.3003,  0.1465,  0.1173, -0.3092,\n",
       "                       -0.1188, -0.4468, -0.3603,  0.0592,  0.0835,  0.2086,  0.2901,  0.0843,\n",
       "                        0.1136,  0.1666,  0.1529, -0.2756,  0.0332,  0.5293,  0.1529, -0.0862],\n",
       "                      device='cuda:0'))]),\n",
       " 'lr_sche': {'base_lrs': [0.6],\n",
       "  'last_epoch': 720,\n",
       "  '_step_count': 721,\n",
       "  'verbose': False,\n",
       "  '_get_lr_called_within_step': False,\n",
       "  '_last_lr': [0.3577875948049462],\n",
       "  'lr_lambdas': [None]},\n",
       " 'optimizer': {'state': {351: {'momentum_buffer': tensor([[-9.6155e-09, -4.9006e-07,  1.8814e-07,  ...,  3.2616e-07,\n",
       "              1.7614e-07,  8.7010e-08],\n",
       "            [-2.9472e-07,  3.4179e-07, -2.3733e-07,  ..., -4.2360e-07,\n",
       "              3.2304e-07,  3.7904e-07],\n",
       "            [ 4.3299e-07,  9.6216e-07,  7.1367e-07,  ...,  2.2082e-07,\n",
       "             -2.5709e-07,  4.3126e-07],\n",
       "            ...,\n",
       "            [-9.6058e-08,  4.6412e-07,  2.4750e-07,  ..., -3.6569e-07,\n",
       "             -3.1287e-07, -1.2151e-07],\n",
       "            [ 2.5308e-07,  7.5232e-07,  3.2036e-07,  ..., -2.9803e-07,\n",
       "              6.3135e-08,  7.5661e-08],\n",
       "            [ 1.1903e-07,  3.9151e-08,  2.9201e-07,  ...,  7.3205e-08,\n",
       "             -2.9050e-08, -4.8761e-07]], device='cuda:0')},\n",
       "   352: {'momentum_buffer': tensor([-7.6008e-07,  5.9048e-06,  5.4530e-07, -2.4583e-06,  1.6694e-06,\n",
       "            -6.4717e-07,  8.5348e-06, -5.0561e-06,  5.9507e-07,  2.2916e-06,\n",
       "             1.9506e-06,  4.6825e-06,  1.1100e-06, -3.9229e-06,  1.0176e-05,\n",
       "             9.8119e-07,  4.3748e-06, -8.4628e-06,  1.8967e-06, -2.6914e-06,\n",
       "            -3.1895e-06,  2.2926e-06,  5.3072e-06,  1.6075e-06, -1.9448e-06,\n",
       "            -8.0508e-08, -1.4245e-07, -9.2364e-07, -3.4997e-07,  6.0493e-06,\n",
       "            -2.4002e-06,  8.0998e-06, -4.8284e-07,  3.7855e-06,  3.6845e-06,\n",
       "             1.3616e-06, -1.6459e-06, -6.8433e-06, -6.9066e-07,  5.9602e-07,\n",
       "            -4.5503e-06,  3.5312e-06, -7.1978e-07,  1.8086e-06, -3.3674e-06,\n",
       "            -5.2062e-06,  2.3971e-06,  1.0893e-06, -2.1207e-06,  2.6373e-06,\n",
       "             6.6015e-06,  1.8940e-06, -6.9278e-06, -3.4608e-06,  4.7955e-06,\n",
       "             4.4937e-07, -2.9908e-06, -8.4022e-07,  4.8860e-06, -3.0405e-06,\n",
       "            -5.9551e-06,  7.9072e-07,  3.1547e-06,  3.9513e-06, -6.6117e-06,\n",
       "            -1.5609e-06, -3.6434e-06, -6.9248e-06,  2.8475e-06,  4.4561e-07,\n",
       "            -1.4068e-06,  1.3815e-06, -1.9796e-06,  2.9835e-06, -6.9674e-06,\n",
       "             6.0633e-06,  2.9264e-06,  2.6556e-06, -1.0190e-06,  3.0707e-06,\n",
       "            -6.3505e-06, -4.2636e-06,  2.3937e-07, -1.3421e-06, -2.3514e-06,\n",
       "            -3.4034e-06,  3.7929e-06,  3.4684e-06, -5.7002e-06, -2.7330e-06,\n",
       "             2.9402e-06,  3.0663e-06, -3.4841e-06,  6.7887e-07, -3.5868e-06,\n",
       "             8.5973e-06, -2.7025e-06,  2.2203e-06,  3.5336e-06,  9.2189e-07,\n",
       "             9.1695e-06,  1.9150e-07,  2.2168e-06,  1.5881e-06,  2.1761e-06,\n",
       "             1.6084e-06,  2.4624e-06,  1.6839e-06,  1.4202e-06, -8.1408e-07,\n",
       "             4.6973e-06, -1.9213e-06, -3.5128e-08, -6.4682e-07, -5.3371e-06,\n",
       "            -1.0090e-06,  1.4102e-07, -6.3531e-06,  1.7898e-06,  4.4881e-06,\n",
       "             3.5418e-06, -1.3405e-06, -3.5922e-06,  4.5245e-06,  5.7384e-07,\n",
       "             4.7120e-06,  1.4462e-06, -3.2853e-06,  5.1975e-06, -3.6343e-06,\n",
       "            -4.7691e-06, -3.3722e-06, -3.5433e-06, -2.5440e-07,  2.4818e-06,\n",
       "             3.4769e-06, -1.9233e-06, -3.3880e-06,  1.1895e-06,  4.3176e-06,\n",
       "             2.2100e-06,  3.5951e-06,  1.9883e-06, -2.2948e-07,  2.8880e-06,\n",
       "             1.4970e-05,  6.8282e-06,  9.1724e-06,  1.0916e-06, -4.1615e-06,\n",
       "            -6.7305e-06, -2.1152e-06,  5.9995e-07, -5.7517e-06,  7.5708e-06,\n",
       "             1.4914e-08, -2.1415e-07,  1.0840e-07, -7.6903e-07,  6.7795e-06,\n",
       "            -3.1556e-06, -3.1928e-07,  5.0433e-07,  4.4296e-06, -5.7160e-06,\n",
       "             3.1871e-06,  1.5453e-06,  7.8120e-06,  4.7727e-07,  5.1652e-06,\n",
       "             3.4633e-06,  2.9999e-06, -1.6317e-06, -1.6575e-06, -2.6933e-07,\n",
       "             9.3962e-07, -6.5393e-06, -9.3635e-06, -2.0556e-08,  5.0062e-06,\n",
       "             4.1020e-06, -7.1038e-06, -3.1984e-06, -3.0634e-06, -3.6426e-06,\n",
       "             1.3395e-06, -2.9781e-08, -4.7358e-08,  3.7664e-06,  6.4053e-07,\n",
       "            -2.5546e-06, -2.0324e-06,  4.8552e-06, -4.7506e-06,  2.1353e-06,\n",
       "            -8.6043e-06,  2.5424e-06, -5.7797e-06,  2.7140e-06,  7.0291e-06,\n",
       "             1.8500e-06, -1.2584e-06,  7.5110e-06,  2.1589e-06,  7.0872e-07,\n",
       "             2.6555e-06,  4.2371e-06,  2.0595e-06, -6.3425e-06,  2.3240e-06,\n",
       "             6.0419e-07, -3.5323e-06,  4.7454e-06,  2.0728e-06,  6.1493e-06,\n",
       "            -2.7682e-06,  2.9348e-07,  8.9880e-06,  7.3193e-06,  4.5633e-06,\n",
       "             4.6197e-06,  1.1701e-05, -6.2452e-06,  1.0312e-05,  2.6428e-06,\n",
       "             1.9517e-06,  6.4264e-07,  4.3357e-06, -6.0062e-06, -1.7539e-06,\n",
       "             1.2734e-06, -4.6734e-06, -5.0530e-06,  1.9002e-06,  7.9796e-07,\n",
       "             6.2478e-06,  7.2592e-06,  2.3641e-06, -3.2862e-06, -2.1832e-06,\n",
       "             8.8903e-06,  3.5509e-06, -3.3993e-06,  7.3519e-07,  1.6286e-06,\n",
       "             6.0808e-06, -4.6233e-06, -5.0541e-06,  3.7770e-06,  6.5285e-06,\n",
       "            -1.4377e-06,  3.6904e-06,  5.6386e-07,  2.7840e-06,  2.5336e-06,\n",
       "            -2.2153e-06,  3.4871e-06,  1.4737e-06,  4.5675e-06,  3.4417e-07,\n",
       "            -3.2799e-06,  1.9502e-06, -3.6492e-06, -3.7302e-06,  2.1621e-08,\n",
       "            -4.6698e-06, -1.3314e-06,  1.4567e-06, -9.1943e-07, -6.3985e-06,\n",
       "            -2.1769e-06, -5.3112e-06, -7.3496e-06, -4.7379e-06,  2.6338e-06,\n",
       "            -3.3253e-06,  3.6400e-07, -1.1381e-06,  1.6778e-06, -5.2520e-06,\n",
       "            -1.7061e-07, -1.4496e-06,  3.6420e-06, -6.0478e-06,  3.4797e-08,\n",
       "            -2.8460e-06, -3.9675e-07,  1.8639e-06,  6.5438e-06, -6.0831e-07,\n",
       "            -5.1760e-06, -2.1598e-06,  9.1578e-07, -7.6533e-06, -3.1469e-06,\n",
       "             3.4540e-06, -3.8514e-06,  5.4553e-06,  1.5422e-06,  2.1407e-06,\n",
       "             8.0088e-06, -2.4148e-06, -2.5428e-06, -1.1682e-06,  3.0035e-06,\n",
       "             7.3402e-06, -4.3298e-06, -9.3881e-07,  2.6595e-07, -2.1520e-06,\n",
       "             4.0104e-06, -3.7041e-06,  4.1620e-06,  2.1043e-06, -1.9007e-07,\n",
       "             6.2629e-07,  4.0723e-08, -8.8370e-07, -4.6335e-06,  3.4988e-06,\n",
       "            -3.0914e-06,  4.9139e-06, -3.1736e-06,  1.6094e-07,  4.1495e-06,\n",
       "             2.0015e-06, -7.1088e-06, -1.0669e-07, -5.3516e-06,  7.2457e-07,\n",
       "            -2.8017e-06,  1.1062e-06, -4.6876e-06, -3.8326e-06, -4.2160e-07,\n",
       "            -3.1107e-06, -1.1157e-06,  9.6945e-07,  5.9225e-06, -4.7119e-06,\n",
       "             1.6687e-07,  2.0418e-06,  2.1288e-06, -6.1535e-06, -2.3606e-06,\n",
       "             8.4275e-06, -3.4604e-06,  2.8272e-06,  5.8944e-06, -1.8135e-07,\n",
       "            -3.4150e-07, -8.2768e-07, -1.1742e-06, -1.9140e-06,  1.5724e-06,\n",
       "             2.0343e-06,  2.1039e-06, -1.7232e-06,  2.9145e-06, -2.5550e-06,\n",
       "            -9.1941e-08,  2.3694e-06, -3.0501e-06, -1.8833e-06,  2.8928e-06,\n",
       "             3.6550e-06, -2.8533e-06,  2.7766e-06,  4.3468e-06, -6.2844e-06,\n",
       "            -1.8384e-06, -4.7717e-06, -2.2528e-06,  6.7317e-06, -8.8959e-08,\n",
       "            -3.0363e-06, -4.7702e-06, -6.6862e-06, -2.2332e-06,  4.7792e-07,\n",
       "            -8.1532e-07, -1.7376e-06, -1.6413e-06, -3.1116e-07, -5.7762e-06,\n",
       "            -1.0782e-06, -3.5583e-07,  7.3294e-06, -4.7583e-06,  1.0531e-06,\n",
       "             1.2096e-06,  1.4508e-07, -8.6479e-06, -9.9205e-07,  6.8804e-07,\n",
       "             6.6739e-06, -5.5356e-06, -5.3755e-06, -3.1895e-06, -4.0532e-06,\n",
       "            -1.0290e-06, -2.0770e-06, -5.3049e-07,  5.8632e-06,  2.6322e-06,\n",
       "            -1.2827e-06, -2.2446e-08,  3.3299e-06,  1.2559e-06, -1.0012e-06,\n",
       "             4.6082e-06,  1.9657e-06, -6.0642e-07, -5.0561e-06,  2.7241e-06,\n",
       "            -3.4427e-07,  2.2425e-06, -3.7431e-06, -1.0227e-06,  3.4046e-06,\n",
       "            -1.5569e-06, -4.9014e-06, -5.0192e-06, -3.1845e-06, -2.4987e-06,\n",
       "             8.0366e-06, -5.3666e-08,  5.4806e-06, -3.9022e-06, -3.8503e-06,\n",
       "            -6.5902e-06, -7.3327e-06,  2.0505e-07, -3.7454e-06,  2.3489e-06,\n",
       "             2.1565e-06, -5.4152e-06,  4.1769e-06, -9.8960e-07,  3.4052e-06,\n",
       "            -1.2887e-06,  7.6108e-07,  4.5118e-07, -6.0467e-06,  6.1286e-06,\n",
       "            -1.5893e-06,  3.1255e-06,  4.6877e-06, -7.3313e-06, -1.4661e-06,\n",
       "             3.0313e-06, -6.4045e-06,  2.8474e-06,  5.3901e-06,  3.9750e-06,\n",
       "             1.5468e-06, -5.4750e-06, -5.2270e-06, -1.0931e-05, -7.7998e-07,\n",
       "            -1.2434e-07, -2.0821e-06,  1.9258e-06,  4.9469e-06, -3.9585e-06,\n",
       "             7.7629e-06, -2.8580e-06, -4.2921e-06, -2.3915e-06,  6.0847e-08,\n",
       "             1.8817e-06, -4.8872e-06, -1.3248e-06, -1.4681e-07,  6.2234e-07,\n",
       "            -6.2560e-07,  2.4328e-06, -2.9552e-06, -9.1811e-07,  1.1423e-06,\n",
       "            -6.9639e-06, -1.5875e-07, -3.5608e-06, -3.9946e-06,  3.9782e-06,\n",
       "            -1.3460e-06, -3.7180e-06, -4.4715e-07, -1.9630e-06,  1.3265e-06,\n",
       "            -3.3734e-06,  4.4849e-07,  5.2785e-06, -2.4974e-06, -3.2397e-07,\n",
       "            -2.6129e-06,  1.0865e-06, -4.0219e-06, -5.3101e-06, -2.0626e-06,\n",
       "            -2.2097e-06,  3.7169e-06,  2.5463e-06,  1.4776e-06,  3.3902e-06,\n",
       "             1.3903e-07,  3.4740e-07, -1.3172e-06, -2.4098e-06,  4.4218e-06,\n",
       "             7.5733e-07, -1.0018e-06], device='cuda:0')},\n",
       "   353: {'momentum_buffer': tensor([[ 1.8682e-07, -2.4838e-07, -6.1031e-08,  ...,  3.7119e-07,\n",
       "              1.0426e-07, -3.9150e-07],\n",
       "            [ 8.0019e-08,  2.0378e-08, -4.0528e-07,  ...,  1.0908e-07,\n",
       "              9.6613e-08, -1.9922e-07],\n",
       "            [-5.7717e-09,  2.6733e-07, -3.0715e-08,  ..., -9.3168e-08,\n",
       "              1.5994e-07,  1.4856e-07],\n",
       "            ...,\n",
       "            [ 7.4868e-08, -2.5185e-07,  4.5576e-07,  ..., -3.7247e-07,\n",
       "              1.1822e-07, -1.8940e-07],\n",
       "            [-2.9589e-07,  3.7910e-07, -6.7590e-09,  ..., -1.6961e-08,\n",
       "              1.0763e-07,  1.0182e-07],\n",
       "            [-4.8978e-08,  2.4676e-07, -6.5788e-08,  ...,  2.8424e-07,\n",
       "              3.9850e-08, -3.6813e-07]], device='cuda:0')},\n",
       "   354: {'momentum_buffer': tensor([-3.0978e-07, -3.3691e-07,  1.1533e-06,  ..., -1.6448e-06,\n",
       "            -1.4250e-07, -1.6622e-07], device='cuda:0')},\n",
       "   355: {'momentum_buffer': tensor([[-3.3370e-08, -1.5027e-07, -1.1363e-07,  ..., -1.6524e-07,\n",
       "             -2.0677e-07,  6.0969e-08],\n",
       "            [ 2.7100e-08,  1.4794e-07, -1.7601e-07,  ...,  2.6722e-07,\n",
       "              1.7144e-07, -2.1951e-07],\n",
       "            [-2.9313e-08,  2.1718e-07,  1.2616e-07,  ..., -2.2863e-07,\n",
       "              8.9807e-08, -1.1935e-08],\n",
       "            ...,\n",
       "            [-5.2214e-08, -1.6676e-07, -6.4208e-08,  ..., -2.8322e-07,\n",
       "             -7.0713e-08, -8.3351e-08],\n",
       "            [ 2.0163e-07, -1.2029e-07,  8.1068e-08,  ..., -4.2234e-08,\n",
       "             -5.8201e-08, -1.8612e-07],\n",
       "            [-1.3937e-07, -1.5947e-07, -1.3008e-07,  ...,  8.4420e-08,\n",
       "             -8.6712e-08, -8.9216e-08]], device='cuda:0')},\n",
       "   356: {'momentum_buffer': tensor([-1.0787e-06,  5.2928e-06, -7.4958e-07,  5.9312e-06, -1.5526e-05,\n",
       "            -5.2728e-06,  3.8678e-06,  2.6160e-06,  6.2619e-06, -7.8762e-06,\n",
       "            -6.4304e-06, -5.9696e-06, -1.8634e-05, -1.2813e-06,  1.3966e-05,\n",
       "            -4.8672e-06,  1.3159e-06,  9.9486e-06,  6.2850e-06, -9.8916e-07,\n",
       "             1.1588e-05, -4.3230e-06, -1.3755e-06,  6.6957e-06,  8.6027e-06,\n",
       "            -5.4715e-07,  4.8098e-06, -1.0594e-05, -5.2943e-06,  4.5831e-06,\n",
       "             7.9629e-06, -2.7648e-06,  1.1292e-06, -3.3132e-06,  1.0069e-05,\n",
       "            -4.6048e-06,  1.7684e-05, -5.5786e-06,  1.4202e-06, -5.4161e-06,\n",
       "            -4.7960e-06,  2.1695e-05,  1.6349e-06,  3.4062e-06,  2.6740e-06,\n",
       "             1.0078e-06, -6.7934e-06, -1.2109e-05,  1.0194e-05,  6.2992e-07,\n",
       "            -1.5497e-05,  1.1247e-06, -3.8619e-06,  6.3873e-06, -9.3508e-06,\n",
       "            -2.2900e-05,  3.0900e-06, -2.9870e-06,  1.3700e-05,  4.2026e-06,\n",
       "             4.4436e-06,  5.7986e-06,  4.2972e-07,  1.5062e-05,  2.2856e-06,\n",
       "             8.1559e-06, -8.8612e-07, -4.7615e-07,  1.1602e-05,  2.1115e-06,\n",
       "            -4.5820e-06,  5.7036e-06, -5.1210e-06,  8.0492e-06, -4.2385e-06,\n",
       "             5.9207e-06,  1.8879e-06, -9.0157e-06,  3.2061e-06, -1.0095e-05,\n",
       "             1.8855e-06,  2.3978e-07, -1.0326e-05,  8.2816e-07,  1.2008e-05,\n",
       "             1.2306e-05, -1.0976e-05, -4.1993e-07, -3.1410e-06, -4.1804e-06,\n",
       "            -1.6988e-06, -3.8913e-07,  3.3124e-06, -3.6225e-06, -4.8255e-06,\n",
       "             2.7020e-06, -1.0241e-05,  6.5678e-07, -2.1395e-06, -4.4097e-06,\n",
       "             7.8967e-06, -4.4764e-06, -1.2016e-07, -1.4518e-05, -1.5312e-05,\n",
       "             4.9223e-07, -5.7083e-06,  9.4631e-07,  5.0419e-06, -8.5377e-06,\n",
       "             7.3743e-06,  1.6935e-05,  2.9475e-06,  1.6657e-05, -8.6107e-07,\n",
       "            -6.0739e-06,  9.0163e-06,  3.2233e-06, -1.4294e-05,  2.0278e-06,\n",
       "             8.7888e-07, -4.4224e-06, -7.0647e-06, -5.6324e-07, -6.8184e-06,\n",
       "             2.9149e-06,  5.1755e-06,  3.2559e-06, -6.7870e-08,  3.7718e-06,\n",
       "             5.0778e-06, -2.9319e-06, -8.4772e-06,  9.0846e-06,  1.7934e-06,\n",
       "            -1.4598e-06,  1.3822e-06,  7.8343e-06,  3.3752e-07, -9.2279e-06,\n",
       "            -1.8554e-05,  5.4738e-06,  1.8859e-05,  1.3500e-06, -7.6393e-06,\n",
       "             1.6934e-06, -1.2209e-06,  2.3661e-06,  1.1980e-06,  1.0477e-05,\n",
       "            -2.7079e-06, -2.4521e-06, -8.9437e-06, -2.8485e-06,  1.8071e-06,\n",
       "            -1.4329e-05, -3.4364e-06, -8.0408e-06, -8.1218e-06, -9.9431e-06,\n",
       "             2.5025e-06, -5.7271e-07, -1.0895e-05, -8.2655e-06, -2.6788e-06,\n",
       "             6.6394e-06,  3.6817e-06, -3.5009e-06,  3.7094e-06, -1.3004e-06,\n",
       "            -6.9370e-06, -3.1459e-06, -9.4067e-06,  6.1614e-06, -5.3091e-06,\n",
       "             4.7028e-06,  8.2699e-06,  7.7594e-06, -3.6323e-06, -1.9660e-06,\n",
       "             4.7628e-06,  1.1323e-05, -6.0056e-06, -1.8994e-05, -1.4889e-05,\n",
       "             8.9961e-06,  9.8393e-06,  6.9020e-06, -2.5642e-06,  8.7922e-06,\n",
       "             2.0542e-06,  3.5980e-06,  5.0155e-06,  1.1775e-06,  1.1124e-05,\n",
       "             6.4977e-06,  4.7355e-06, -8.9062e-06, -5.4684e-06, -1.4531e-06,\n",
       "             2.3988e-06,  1.4490e-05,  5.8319e-06, -2.9420e-06, -1.0844e-05,\n",
       "            -5.7212e-06,  1.1078e-05, -5.2275e-06, -8.6422e-06,  9.1078e-06,\n",
       "             4.2311e-06, -3.3225e-07, -1.3879e-06, -3.3806e-06, -4.4124e-06,\n",
       "             1.2762e-05,  2.1009e-06,  4.5813e-06, -3.6798e-06,  8.2804e-06,\n",
       "             1.2216e-05, -1.1645e-06,  7.9311e-06, -9.8307e-06,  3.9869e-06,\n",
       "            -7.2992e-06, -4.4291e-06, -3.6974e-06, -2.7092e-06,  1.7342e-06,\n",
       "            -3.9747e-06, -8.0517e-06, -4.5569e-06,  4.1082e-07, -5.8622e-06,\n",
       "             2.0560e-05,  3.6480e-06, -1.2702e-05,  5.4339e-06, -7.9128e-06,\n",
       "            -1.2308e-06,  8.0075e-07, -1.4202e-07,  7.6291e-07,  6.9461e-06,\n",
       "            -3.3357e-06,  1.4233e-06, -5.1500e-06, -1.6773e-06,  3.4228e-06,\n",
       "             3.1603e-06,  9.8451e-06,  8.8220e-06,  2.7818e-06, -1.4235e-05,\n",
       "            -2.2666e-05,  1.1585e-06, -1.7807e-06, -1.4905e-06,  7.6472e-06,\n",
       "            -7.1535e-06,  6.5113e-06,  3.7936e-06, -2.3040e-06, -4.0355e-06,\n",
       "             7.1557e-06,  1.4224e-05, -7.5020e-06, -2.4980e-06, -1.6735e-06,\n",
       "             4.2296e-06,  4.3480e-06,  1.6344e-05,  1.5448e-06,  5.4865e-06,\n",
       "             1.0184e-06, -7.0643e-06,  3.2230e-06, -1.4136e-06, -5.0929e-06,\n",
       "            -5.8075e-06,  6.4117e-06,  7.1238e-06,  3.1840e-06, -2.9807e-06,\n",
       "             1.4733e-05,  5.7235e-06,  5.4695e-06, -1.4433e-06,  1.6711e-05,\n",
       "             6.1628e-06, -8.3212e-06, -8.0003e-07,  3.0228e-06,  2.2227e-06,\n",
       "            -8.7905e-06, -7.1488e-06, -3.5683e-06,  4.0359e-06,  4.1693e-06,\n",
       "             4.1442e-06,  3.4662e-06,  3.8421e-06,  1.0115e-06,  1.0821e-07,\n",
       "             1.5604e-06,  2.0007e-06,  6.7798e-06, -1.3567e-05,  9.5207e-06,\n",
       "            -1.7085e-05,  4.3418e-06,  6.3985e-06, -3.7475e-06, -6.8600e-06,\n",
       "             1.0944e-05,  8.8468e-07, -2.6537e-06, -4.1379e-06, -1.7064e-05,\n",
       "            -3.9934e-06, -3.7698e-06, -9.0733e-06, -2.2672e-06,  5.4232e-07,\n",
       "             3.5313e-06,  4.9700e-06,  1.1063e-05,  1.3542e-06, -9.9595e-06,\n",
       "             2.0778e-06,  8.4930e-06,  3.6456e-07,  5.4646e-06,  7.4509e-06,\n",
       "            -1.6434e-06, -1.0091e-05, -8.7795e-06,  5.3057e-06, -6.1342e-07,\n",
       "            -1.1756e-06,  5.2484e-07, -7.1081e-06, -4.2018e-06,  7.6957e-06,\n",
       "            -3.3529e-07,  3.6174e-06, -5.0265e-07, -5.6485e-06, -8.2686e-06,\n",
       "            -3.6755e-06, -3.9584e-06,  1.1722e-05, -1.0179e-06,  9.0109e-06,\n",
       "            -4.0699e-06, -8.1199e-06, -4.1629e-06, -1.4737e-05, -1.8503e-05,\n",
       "             1.0233e-05, -4.4656e-06,  2.7867e-05,  2.0859e-05, -3.0082e-06,\n",
       "             3.9790e-06, -1.2906e-05, -3.9665e-06,  1.0277e-05,  9.0393e-06,\n",
       "            -4.7281e-06, -1.0850e-05, -1.3427e-05,  3.0368e-06,  7.5913e-06,\n",
       "             4.8051e-06, -3.6199e-06, -2.8859e-06, -4.0704e-06,  5.5464e-06,\n",
       "             1.4220e-07, -3.1007e-06,  3.4392e-06,  3.0169e-06, -3.4854e-06,\n",
       "             1.3799e-06,  7.9487e-06, -7.1429e-06,  2.3579e-06,  1.3294e-05,\n",
       "            -8.8370e-07,  4.2712e-06,  1.0232e-06, -3.6499e-06,  1.2813e-05,\n",
       "             4.0988e-06, -1.1060e-05,  1.3025e-05, -1.9920e-07, -1.0763e-05,\n",
       "             1.8205e-06, -2.0607e-07, -1.4731e-05, -5.7061e-06, -7.8014e-06,\n",
       "            -3.7690e-07, -1.0903e-05,  3.8853e-06,  1.0850e-05,  4.9666e-06,\n",
       "             1.2800e-05, -8.1124e-06, -4.4181e-07,  2.1231e-06,  7.2964e-06,\n",
       "            -2.2014e-05,  1.6133e-05, -1.5132e-06, -1.0946e-05, -6.2026e-06,\n",
       "            -5.2783e-06, -2.6983e-06,  4.8361e-06,  5.1940e-06, -5.5868e-06,\n",
       "            -1.2174e-05,  1.4202e-06, -7.4152e-06,  5.3522e-06,  3.5013e-06,\n",
       "             2.0464e-06, -1.8424e-07,  1.8192e-06, -3.2873e-07,  3.9842e-06,\n",
       "             5.1870e-06, -6.5482e-06, -5.9920e-06, -1.4668e-06, -7.4568e-06,\n",
       "            -8.7148e-06,  1.1281e-05,  1.4896e-06, -2.7914e-07, -8.4009e-06,\n",
       "            -4.4897e-06,  4.8395e-06,  4.3775e-06, -1.8849e-06, -3.9381e-06,\n",
       "            -7.6185e-06,  4.3167e-06,  5.5087e-06, -1.3023e-05, -1.4001e-05,\n",
       "            -4.0553e-06,  4.8849e-06,  7.1600e-06, -2.3994e-06, -1.7329e-06,\n",
       "             7.7480e-06,  1.3245e-05,  4.3970e-06,  5.2047e-06,  2.0687e-06,\n",
       "            -3.7157e-06, -1.4113e-06,  7.1085e-06, -6.0202e-06,  7.8884e-06,\n",
       "            -7.1582e-06,  1.4337e-06,  9.2350e-06,  7.5637e-06, -1.4116e-05,\n",
       "            -4.9150e-06, -6.6590e-06,  1.0753e-05,  2.0287e-05, -8.1850e-06,\n",
       "             8.2844e-06,  1.2616e-05,  8.1923e-06, -4.9361e-07, -8.9829e-06,\n",
       "            -3.3409e-08, -7.3661e-06, -1.5790e-06,  5.2180e-06,  1.6992e-05,\n",
       "             7.3814e-06,  2.8484e-06, -1.1774e-05, -4.4020e-06, -4.9768e-06,\n",
       "            -3.6578e-06, -9.1368e-08, -4.0784e-06,  3.7569e-06, -3.2920e-06,\n",
       "            -8.3372e-06,  4.4597e-06,  8.1395e-06,  1.4309e-05, -1.4801e-05,\n",
       "             3.9550e-06, -4.4037e-06,  9.2773e-06, -5.2197e-06, -7.1451e-06,\n",
       "             3.2893e-06,  8.8344e-06], device='cuda:0')}},\n",
       "  'param_groups': [{'lr': 0.3577875948049462,\n",
       "    'momentum': 0.9,\n",
       "    'dampening': 0,\n",
       "    'weight_decay': 1e-06,\n",
       "    'nesterov': True,\n",
       "    'maximize': False,\n",
       "    'foreach': None,\n",
       "    'initial_lr': 0.6,\n",
       "    'params': [0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4,\n",
       "     5,\n",
       "     6,\n",
       "     7,\n",
       "     8,\n",
       "     9,\n",
       "     10,\n",
       "     11,\n",
       "     12,\n",
       "     13,\n",
       "     14,\n",
       "     15,\n",
       "     16,\n",
       "     17,\n",
       "     18,\n",
       "     19,\n",
       "     20,\n",
       "     21,\n",
       "     22,\n",
       "     23,\n",
       "     24,\n",
       "     25,\n",
       "     26,\n",
       "     27,\n",
       "     28,\n",
       "     29,\n",
       "     30,\n",
       "     31,\n",
       "     32,\n",
       "     33,\n",
       "     34,\n",
       "     35,\n",
       "     36,\n",
       "     37,\n",
       "     38,\n",
       "     39,\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44,\n",
       "     45,\n",
       "     46,\n",
       "     47,\n",
       "     48,\n",
       "     49,\n",
       "     50,\n",
       "     51,\n",
       "     52,\n",
       "     53,\n",
       "     54,\n",
       "     55,\n",
       "     56,\n",
       "     57,\n",
       "     58,\n",
       "     59,\n",
       "     60,\n",
       "     61,\n",
       "     62,\n",
       "     63,\n",
       "     64,\n",
       "     65,\n",
       "     66,\n",
       "     67,\n",
       "     68,\n",
       "     69,\n",
       "     70,\n",
       "     71,\n",
       "     72,\n",
       "     73,\n",
       "     74,\n",
       "     75,\n",
       "     76,\n",
       "     77,\n",
       "     78,\n",
       "     79,\n",
       "     80,\n",
       "     81,\n",
       "     82,\n",
       "     83,\n",
       "     84,\n",
       "     85,\n",
       "     86,\n",
       "     87,\n",
       "     88,\n",
       "     89,\n",
       "     90,\n",
       "     91,\n",
       "     92,\n",
       "     93,\n",
       "     94,\n",
       "     95,\n",
       "     96,\n",
       "     97,\n",
       "     98,\n",
       "     99,\n",
       "     100,\n",
       "     101,\n",
       "     102,\n",
       "     103,\n",
       "     104,\n",
       "     105,\n",
       "     106,\n",
       "     107,\n",
       "     108,\n",
       "     109,\n",
       "     110,\n",
       "     111,\n",
       "     112,\n",
       "     113,\n",
       "     114,\n",
       "     115,\n",
       "     116,\n",
       "     117,\n",
       "     118,\n",
       "     119,\n",
       "     120,\n",
       "     121,\n",
       "     122,\n",
       "     123,\n",
       "     124,\n",
       "     125,\n",
       "     126,\n",
       "     127,\n",
       "     128,\n",
       "     129,\n",
       "     130,\n",
       "     131,\n",
       "     132,\n",
       "     133,\n",
       "     134,\n",
       "     135,\n",
       "     136,\n",
       "     137,\n",
       "     138,\n",
       "     139,\n",
       "     140,\n",
       "     141,\n",
       "     142,\n",
       "     143,\n",
       "     144,\n",
       "     145,\n",
       "     146,\n",
       "     147,\n",
       "     148,\n",
       "     149,\n",
       "     150,\n",
       "     151,\n",
       "     152,\n",
       "     153,\n",
       "     154,\n",
       "     155,\n",
       "     156,\n",
       "     157,\n",
       "     158,\n",
       "     159,\n",
       "     160,\n",
       "     161,\n",
       "     162,\n",
       "     163,\n",
       "     164,\n",
       "     165,\n",
       "     166,\n",
       "     167,\n",
       "     168,\n",
       "     169,\n",
       "     170,\n",
       "     171,\n",
       "     172,\n",
       "     173,\n",
       "     174,\n",
       "     175,\n",
       "     176,\n",
       "     177,\n",
       "     178,\n",
       "     179,\n",
       "     180,\n",
       "     181,\n",
       "     182,\n",
       "     183,\n",
       "     184,\n",
       "     185,\n",
       "     186,\n",
       "     187,\n",
       "     188,\n",
       "     189,\n",
       "     190,\n",
       "     191,\n",
       "     192,\n",
       "     193,\n",
       "     194,\n",
       "     195,\n",
       "     196,\n",
       "     197,\n",
       "     198,\n",
       "     199,\n",
       "     200,\n",
       "     201,\n",
       "     202,\n",
       "     203,\n",
       "     204,\n",
       "     205,\n",
       "     206,\n",
       "     207,\n",
       "     208,\n",
       "     209,\n",
       "     210,\n",
       "     211,\n",
       "     212,\n",
       "     213,\n",
       "     214,\n",
       "     215,\n",
       "     216,\n",
       "     217,\n",
       "     218,\n",
       "     219,\n",
       "     220,\n",
       "     221,\n",
       "     222,\n",
       "     223,\n",
       "     224,\n",
       "     225,\n",
       "     226,\n",
       "     227,\n",
       "     228,\n",
       "     229,\n",
       "     230,\n",
       "     231,\n",
       "     232,\n",
       "     233,\n",
       "     234,\n",
       "     235,\n",
       "     236,\n",
       "     237,\n",
       "     238,\n",
       "     239,\n",
       "     240,\n",
       "     241,\n",
       "     242,\n",
       "     243,\n",
       "     244,\n",
       "     245,\n",
       "     246,\n",
       "     247,\n",
       "     248,\n",
       "     249,\n",
       "     250,\n",
       "     251,\n",
       "     252,\n",
       "     253,\n",
       "     254,\n",
       "     255,\n",
       "     256,\n",
       "     257,\n",
       "     258,\n",
       "     259,\n",
       "     260,\n",
       "     261,\n",
       "     262,\n",
       "     263,\n",
       "     264,\n",
       "     265,\n",
       "     266,\n",
       "     267,\n",
       "     268,\n",
       "     269,\n",
       "     270,\n",
       "     271,\n",
       "     272,\n",
       "     273,\n",
       "     274,\n",
       "     275,\n",
       "     276,\n",
       "     277,\n",
       "     278,\n",
       "     279,\n",
       "     280,\n",
       "     281,\n",
       "     282,\n",
       "     283,\n",
       "     284,\n",
       "     285,\n",
       "     286,\n",
       "     287,\n",
       "     288,\n",
       "     289,\n",
       "     290,\n",
       "     291,\n",
       "     292,\n",
       "     293,\n",
       "     294,\n",
       "     295,\n",
       "     296,\n",
       "     297,\n",
       "     298,\n",
       "     299,\n",
       "     300,\n",
       "     301,\n",
       "     302,\n",
       "     303,\n",
       "     304,\n",
       "     305,\n",
       "     306,\n",
       "     307,\n",
       "     308,\n",
       "     309,\n",
       "     310,\n",
       "     311,\n",
       "     312,\n",
       "     313,\n",
       "     314,\n",
       "     315,\n",
       "     316,\n",
       "     317,\n",
       "     318,\n",
       "     319,\n",
       "     320,\n",
       "     321,\n",
       "     322,\n",
       "     323,\n",
       "     324,\n",
       "     325,\n",
       "     326,\n",
       "     327,\n",
       "     328,\n",
       "     329,\n",
       "     330,\n",
       "     331,\n",
       "     332,\n",
       "     333,\n",
       "     334,\n",
       "     335,\n",
       "     336,\n",
       "     337,\n",
       "     338,\n",
       "     339,\n",
       "     340,\n",
       "     341,\n",
       "     342,\n",
       "     343,\n",
       "     344,\n",
       "     345,\n",
       "     346,\n",
       "     347,\n",
       "     348,\n",
       "     349,\n",
       "     350,\n",
       "     351,\n",
       "     352,\n",
       "     353,\n",
       "     354,\n",
       "     355,\n",
       "     356]}]},\n",
       " 'epoch': 10,\n",
       " 'wandb_id': 'wbajq4k6'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "path = '/home/huangjialong/projects/BiomedCLIP-PUNCE/simclr/output-model/new-simclr-puc/biomed_simclr_infonce_filterGC_224_4*256/biomed_simclr_infonce_filterGC_224_4*256_epoch10.pt'\n",
    "tensor = torch.load(path)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "model.cuda()\n",
    "img_path = '/home1/wsi/gc-224/NILM/202000767/1_10.jpg'\n",
    "input = preprocess_val(Image.open(img_path)).unsqueeze(0).to('cuda')\n",
    "img_feature, _, scalar = model(input)\n",
    "img_feature.shape\n",
    "\n",
    "img_feature1 = model.encode_image(input)\n",
    "img_feature1.shape\n",
    "\n",
    "img_feature2 = model.visual.trunk(input)\n",
    "\n",
    "img_feature2 /= img_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0404,  1.7260, -0.8140],\n",
      "        [ 1.3722,  0.5060, -0.4823],\n",
      "        [-0.7853,  0.6681, -0.4439],\n",
      "        [ 0.1888,  0.5986,  0.6458],\n",
      "        [ 0.6306, -1.4668, -0.6798]], requires_grad=True) tensor([0, 2, 2, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.8474, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def fix_seed(seed):\n",
    "    torch.manual_seed(seed) # 为CPU设置随机种子\n",
    "    torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU，为所有GPU设置随机种子\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_seed(2024)\n",
    "# Example of target with class indices\n",
    "input = torch.randn(5, 3, requires_grad=True)\n",
    "target = torch.randint(3, (5,), dtype=torch.int64)\n",
    "print(input, target)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8474)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "input = torch.tensor([[0,  1, -0.8140],\n",
    "        [ 1.3722,  0.5060, -0.4823],\n",
    "        [-0.7853,  0.6681, -0.4439],\n",
    "        [ 0.1888,  0.5986,  0.6458],\n",
    "        [ 0.6306, -1.4668, -0.6798]], dtype=torch.float32)\n",
    "target = torch.tensor([0, 2, 2, 2, 1])\n",
    "target = F.one_hot(target, num_classes=3).type(torch.float32)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "n = -torch.inf\n",
    "input = torch.tensor([[1., n, n],])\n",
    "target1 = torch.tensor([0])\n",
    "target2 = torch.tensor([[2., 0., 0.],])\n",
    "loss1 = F.cross_entropy(input, target1)\n",
    "loss2 = F.cross_entropy(input, target2)\n",
    "print(loss1, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "          Identity-2             [-1, 196, 768]               0\n",
      "        PatchEmbed-3             [-1, 196, 768]               0\n",
      "           Dropout-4             [-1, 197, 768]               0\n",
      "          Identity-5             [-1, 197, 768]               0\n",
      "          Identity-6             [-1, 197, 768]               0\n",
      "         LayerNorm-7             [-1, 197, 768]           1,536\n",
      "            Linear-8            [-1, 197, 2304]       1,771,776\n",
      "          Identity-9          [-1, 12, 197, 64]               0\n",
      "         Identity-10          [-1, 12, 197, 64]               0\n",
      "          Dropout-11         [-1, 12, 197, 197]               0\n",
      "           Linear-12             [-1, 197, 768]         590,592\n",
      "          Dropout-13             [-1, 197, 768]               0\n",
      "        Attention-14             [-1, 197, 768]               0\n",
      "         Identity-15             [-1, 197, 768]               0\n",
      "         Identity-16             [-1, 197, 768]               0\n",
      "        LayerNorm-17             [-1, 197, 768]           1,536\n",
      "           Linear-18            [-1, 197, 3072]       2,362,368\n",
      "             GELU-19            [-1, 197, 3072]               0\n",
      "          Dropout-20            [-1, 197, 3072]               0\n",
      "         Identity-21            [-1, 197, 3072]               0\n",
      "           Linear-22             [-1, 197, 768]       2,360,064\n",
      "          Dropout-23             [-1, 197, 768]               0\n",
      "              Mlp-24             [-1, 197, 768]               0\n",
      "         Identity-25             [-1, 197, 768]               0\n",
      "         Identity-26             [-1, 197, 768]               0\n",
      "            Block-27             [-1, 197, 768]               0\n",
      "        LayerNorm-28             [-1, 197, 768]           1,536\n",
      "           Linear-29            [-1, 197, 2304]       1,771,776\n",
      "         Identity-30          [-1, 12, 197, 64]               0\n",
      "         Identity-31          [-1, 12, 197, 64]               0\n",
      "          Dropout-32         [-1, 12, 197, 197]               0\n",
      "           Linear-33             [-1, 197, 768]         590,592\n",
      "          Dropout-34             [-1, 197, 768]               0\n",
      "        Attention-35             [-1, 197, 768]               0\n",
      "         Identity-36             [-1, 197, 768]               0\n",
      "         Identity-37             [-1, 197, 768]               0\n",
      "        LayerNorm-38             [-1, 197, 768]           1,536\n",
      "           Linear-39            [-1, 197, 3072]       2,362,368\n",
      "             GELU-40            [-1, 197, 3072]               0\n",
      "          Dropout-41            [-1, 197, 3072]               0\n",
      "         Identity-42            [-1, 197, 3072]               0\n",
      "           Linear-43             [-1, 197, 768]       2,360,064\n",
      "          Dropout-44             [-1, 197, 768]               0\n",
      "              Mlp-45             [-1, 197, 768]               0\n",
      "         Identity-46             [-1, 197, 768]               0\n",
      "         Identity-47             [-1, 197, 768]               0\n",
      "            Block-48             [-1, 197, 768]               0\n",
      "        LayerNorm-49             [-1, 197, 768]           1,536\n",
      "           Linear-50            [-1, 197, 2304]       1,771,776\n",
      "         Identity-51          [-1, 12, 197, 64]               0\n",
      "         Identity-52          [-1, 12, 197, 64]               0\n",
      "          Dropout-53         [-1, 12, 197, 197]               0\n",
      "           Linear-54             [-1, 197, 768]         590,592\n",
      "          Dropout-55             [-1, 197, 768]               0\n",
      "        Attention-56             [-1, 197, 768]               0\n",
      "         Identity-57             [-1, 197, 768]               0\n",
      "         Identity-58             [-1, 197, 768]               0\n",
      "        LayerNorm-59             [-1, 197, 768]           1,536\n",
      "           Linear-60            [-1, 197, 3072]       2,362,368\n",
      "             GELU-61            [-1, 197, 3072]               0\n",
      "          Dropout-62            [-1, 197, 3072]               0\n",
      "         Identity-63            [-1, 197, 3072]               0\n",
      "           Linear-64             [-1, 197, 768]       2,360,064\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "              Mlp-66             [-1, 197, 768]               0\n",
      "         Identity-67             [-1, 197, 768]               0\n",
      "         Identity-68             [-1, 197, 768]               0\n",
      "            Block-69             [-1, 197, 768]               0\n",
      "        LayerNorm-70             [-1, 197, 768]           1,536\n",
      "           Linear-71            [-1, 197, 2304]       1,771,776\n",
      "         Identity-72          [-1, 12, 197, 64]               0\n",
      "         Identity-73          [-1, 12, 197, 64]               0\n",
      "          Dropout-74         [-1, 12, 197, 197]               0\n",
      "           Linear-75             [-1, 197, 768]         590,592\n",
      "          Dropout-76             [-1, 197, 768]               0\n",
      "        Attention-77             [-1, 197, 768]               0\n",
      "         Identity-78             [-1, 197, 768]               0\n",
      "         Identity-79             [-1, 197, 768]               0\n",
      "        LayerNorm-80             [-1, 197, 768]           1,536\n",
      "           Linear-81            [-1, 197, 3072]       2,362,368\n",
      "             GELU-82            [-1, 197, 3072]               0\n",
      "          Dropout-83            [-1, 197, 3072]               0\n",
      "         Identity-84            [-1, 197, 3072]               0\n",
      "           Linear-85             [-1, 197, 768]       2,360,064\n",
      "          Dropout-86             [-1, 197, 768]               0\n",
      "              Mlp-87             [-1, 197, 768]               0\n",
      "         Identity-88             [-1, 197, 768]               0\n",
      "         Identity-89             [-1, 197, 768]               0\n",
      "            Block-90             [-1, 197, 768]               0\n",
      "        LayerNorm-91             [-1, 197, 768]           1,536\n",
      "           Linear-92            [-1, 197, 2304]       1,771,776\n",
      "         Identity-93          [-1, 12, 197, 64]               0\n",
      "         Identity-94          [-1, 12, 197, 64]               0\n",
      "          Dropout-95         [-1, 12, 197, 197]               0\n",
      "           Linear-96             [-1, 197, 768]         590,592\n",
      "          Dropout-97             [-1, 197, 768]               0\n",
      "        Attention-98             [-1, 197, 768]               0\n",
      "         Identity-99             [-1, 197, 768]               0\n",
      "        Identity-100             [-1, 197, 768]               0\n",
      "       LayerNorm-101             [-1, 197, 768]           1,536\n",
      "          Linear-102            [-1, 197, 3072]       2,362,368\n",
      "            GELU-103            [-1, 197, 3072]               0\n",
      "         Dropout-104            [-1, 197, 3072]               0\n",
      "        Identity-105            [-1, 197, 3072]               0\n",
      "          Linear-106             [-1, 197, 768]       2,360,064\n",
      "         Dropout-107             [-1, 197, 768]               0\n",
      "             Mlp-108             [-1, 197, 768]               0\n",
      "        Identity-109             [-1, 197, 768]               0\n",
      "        Identity-110             [-1, 197, 768]               0\n",
      "           Block-111             [-1, 197, 768]               0\n",
      "       LayerNorm-112             [-1, 197, 768]           1,536\n",
      "          Linear-113            [-1, 197, 2304]       1,771,776\n",
      "        Identity-114          [-1, 12, 197, 64]               0\n",
      "        Identity-115          [-1, 12, 197, 64]               0\n",
      "         Dropout-116         [-1, 12, 197, 197]               0\n",
      "          Linear-117             [-1, 197, 768]         590,592\n",
      "         Dropout-118             [-1, 197, 768]               0\n",
      "       Attention-119             [-1, 197, 768]               0\n",
      "        Identity-120             [-1, 197, 768]               0\n",
      "        Identity-121             [-1, 197, 768]               0\n",
      "       LayerNorm-122             [-1, 197, 768]           1,536\n",
      "          Linear-123            [-1, 197, 3072]       2,362,368\n",
      "            GELU-124            [-1, 197, 3072]               0\n",
      "         Dropout-125            [-1, 197, 3072]               0\n",
      "        Identity-126            [-1, 197, 3072]               0\n",
      "          Linear-127             [-1, 197, 768]       2,360,064\n",
      "         Dropout-128             [-1, 197, 768]               0\n",
      "             Mlp-129             [-1, 197, 768]               0\n",
      "        Identity-130             [-1, 197, 768]               0\n",
      "        Identity-131             [-1, 197, 768]               0\n",
      "           Block-132             [-1, 197, 768]               0\n",
      "       LayerNorm-133             [-1, 197, 768]           1,536\n",
      "          Linear-134            [-1, 197, 2304]       1,771,776\n",
      "        Identity-135          [-1, 12, 197, 64]               0\n",
      "        Identity-136          [-1, 12, 197, 64]               0\n",
      "         Dropout-137         [-1, 12, 197, 197]               0\n",
      "          Linear-138             [-1, 197, 768]         590,592\n",
      "         Dropout-139             [-1, 197, 768]               0\n",
      "       Attention-140             [-1, 197, 768]               0\n",
      "        Identity-141             [-1, 197, 768]               0\n",
      "        Identity-142             [-1, 197, 768]               0\n",
      "       LayerNorm-143             [-1, 197, 768]           1,536\n",
      "          Linear-144            [-1, 197, 3072]       2,362,368\n",
      "            GELU-145            [-1, 197, 3072]               0\n",
      "         Dropout-146            [-1, 197, 3072]               0\n",
      "        Identity-147            [-1, 197, 3072]               0\n",
      "          Linear-148             [-1, 197, 768]       2,360,064\n",
      "         Dropout-149             [-1, 197, 768]               0\n",
      "             Mlp-150             [-1, 197, 768]               0\n",
      "        Identity-151             [-1, 197, 768]               0\n",
      "        Identity-152             [-1, 197, 768]               0\n",
      "           Block-153             [-1, 197, 768]               0\n",
      "       LayerNorm-154             [-1, 197, 768]           1,536\n",
      "          Linear-155            [-1, 197, 2304]       1,771,776\n",
      "        Identity-156          [-1, 12, 197, 64]               0\n",
      "        Identity-157          [-1, 12, 197, 64]               0\n",
      "         Dropout-158         [-1, 12, 197, 197]               0\n",
      "          Linear-159             [-1, 197, 768]         590,592\n",
      "         Dropout-160             [-1, 197, 768]               0\n",
      "       Attention-161             [-1, 197, 768]               0\n",
      "        Identity-162             [-1, 197, 768]               0\n",
      "        Identity-163             [-1, 197, 768]               0\n",
      "       LayerNorm-164             [-1, 197, 768]           1,536\n",
      "          Linear-165            [-1, 197, 3072]       2,362,368\n",
      "            GELU-166            [-1, 197, 3072]               0\n",
      "         Dropout-167            [-1, 197, 3072]               0\n",
      "        Identity-168            [-1, 197, 3072]               0\n",
      "          Linear-169             [-1, 197, 768]       2,360,064\n",
      "         Dropout-170             [-1, 197, 768]               0\n",
      "             Mlp-171             [-1, 197, 768]               0\n",
      "        Identity-172             [-1, 197, 768]               0\n",
      "        Identity-173             [-1, 197, 768]               0\n",
      "           Block-174             [-1, 197, 768]               0\n",
      "       LayerNorm-175             [-1, 197, 768]           1,536\n",
      "          Linear-176            [-1, 197, 2304]       1,771,776\n",
      "        Identity-177          [-1, 12, 197, 64]               0\n",
      "        Identity-178          [-1, 12, 197, 64]               0\n",
      "         Dropout-179         [-1, 12, 197, 197]               0\n",
      "          Linear-180             [-1, 197, 768]         590,592\n",
      "         Dropout-181             [-1, 197, 768]               0\n",
      "       Attention-182             [-1, 197, 768]               0\n",
      "        Identity-183             [-1, 197, 768]               0\n",
      "        Identity-184             [-1, 197, 768]               0\n",
      "       LayerNorm-185             [-1, 197, 768]           1,536\n",
      "          Linear-186            [-1, 197, 3072]       2,362,368\n",
      "            GELU-187            [-1, 197, 3072]               0\n",
      "         Dropout-188            [-1, 197, 3072]               0\n",
      "        Identity-189            [-1, 197, 3072]               0\n",
      "          Linear-190             [-1, 197, 768]       2,360,064\n",
      "         Dropout-191             [-1, 197, 768]               0\n",
      "             Mlp-192             [-1, 197, 768]               0\n",
      "        Identity-193             [-1, 197, 768]               0\n",
      "        Identity-194             [-1, 197, 768]               0\n",
      "           Block-195             [-1, 197, 768]               0\n",
      "       LayerNorm-196             [-1, 197, 768]           1,536\n",
      "          Linear-197            [-1, 197, 2304]       1,771,776\n",
      "        Identity-198          [-1, 12, 197, 64]               0\n",
      "        Identity-199          [-1, 12, 197, 64]               0\n",
      "         Dropout-200         [-1, 12, 197, 197]               0\n",
      "          Linear-201             [-1, 197, 768]         590,592\n",
      "         Dropout-202             [-1, 197, 768]               0\n",
      "       Attention-203             [-1, 197, 768]               0\n",
      "        Identity-204             [-1, 197, 768]               0\n",
      "        Identity-205             [-1, 197, 768]               0\n",
      "       LayerNorm-206             [-1, 197, 768]           1,536\n",
      "          Linear-207            [-1, 197, 3072]       2,362,368\n",
      "            GELU-208            [-1, 197, 3072]               0\n",
      "         Dropout-209            [-1, 197, 3072]               0\n",
      "        Identity-210            [-1, 197, 3072]               0\n",
      "          Linear-211             [-1, 197, 768]       2,360,064\n",
      "         Dropout-212             [-1, 197, 768]               0\n",
      "             Mlp-213             [-1, 197, 768]               0\n",
      "        Identity-214             [-1, 197, 768]               0\n",
      "        Identity-215             [-1, 197, 768]               0\n",
      "           Block-216             [-1, 197, 768]               0\n",
      "       LayerNorm-217             [-1, 197, 768]           1,536\n",
      "          Linear-218            [-1, 197, 2304]       1,771,776\n",
      "        Identity-219          [-1, 12, 197, 64]               0\n",
      "        Identity-220          [-1, 12, 197, 64]               0\n",
      "         Dropout-221         [-1, 12, 197, 197]               0\n",
      "          Linear-222             [-1, 197, 768]         590,592\n",
      "         Dropout-223             [-1, 197, 768]               0\n",
      "       Attention-224             [-1, 197, 768]               0\n",
      "        Identity-225             [-1, 197, 768]               0\n",
      "        Identity-226             [-1, 197, 768]               0\n",
      "       LayerNorm-227             [-1, 197, 768]           1,536\n",
      "          Linear-228            [-1, 197, 3072]       2,362,368\n",
      "            GELU-229            [-1, 197, 3072]               0\n",
      "         Dropout-230            [-1, 197, 3072]               0\n",
      "        Identity-231            [-1, 197, 3072]               0\n",
      "          Linear-232             [-1, 197, 768]       2,360,064\n",
      "         Dropout-233             [-1, 197, 768]               0\n",
      "             Mlp-234             [-1, 197, 768]               0\n",
      "        Identity-235             [-1, 197, 768]               0\n",
      "        Identity-236             [-1, 197, 768]               0\n",
      "           Block-237             [-1, 197, 768]               0\n",
      "       LayerNorm-238             [-1, 197, 768]           1,536\n",
      "          Linear-239            [-1, 197, 2304]       1,771,776\n",
      "        Identity-240          [-1, 12, 197, 64]               0\n",
      "        Identity-241          [-1, 12, 197, 64]               0\n",
      "         Dropout-242         [-1, 12, 197, 197]               0\n",
      "          Linear-243             [-1, 197, 768]         590,592\n",
      "         Dropout-244             [-1, 197, 768]               0\n",
      "       Attention-245             [-1, 197, 768]               0\n",
      "        Identity-246             [-1, 197, 768]               0\n",
      "        Identity-247             [-1, 197, 768]               0\n",
      "       LayerNorm-248             [-1, 197, 768]           1,536\n",
      "          Linear-249            [-1, 197, 3072]       2,362,368\n",
      "            GELU-250            [-1, 197, 3072]               0\n",
      "         Dropout-251            [-1, 197, 3072]               0\n",
      "        Identity-252            [-1, 197, 3072]               0\n",
      "          Linear-253             [-1, 197, 768]       2,360,064\n",
      "         Dropout-254             [-1, 197, 768]               0\n",
      "             Mlp-255             [-1, 197, 768]               0\n",
      "        Identity-256             [-1, 197, 768]               0\n",
      "        Identity-257             [-1, 197, 768]               0\n",
      "           Block-258             [-1, 197, 768]               0\n",
      "       LayerNorm-259             [-1, 197, 768]           1,536\n",
      "        Identity-260                  [-1, 768]               0\n",
      "         Dropout-261                  [-1, 768]               0\n",
      "        Identity-262                  [-1, 768]               0\n",
      "VisionTransformer-263                  [-1, 768]               0\n",
      "         Dropout-264                  [-1, 768]               0\n",
      "          Linear-265                  [-1, 512]         393,216\n",
      "       TimmModel-266                  [-1, 512]               0\n",
      "================================================================\n",
      "Total params: 86,039,808\n",
      "Trainable params: 86,039,808\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 521.69\n",
      "Params size (MB): 328.22\n",
      "Estimated Total Size (MB): 850.48\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "input_size = (3, 224, 224)\n",
    "summary(model.to('cuda'), input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTextCLIP(\n",
      "  (visual): TimmModel(\n",
      "    (trunk): VisionTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "        (norm): Identity()\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (patch_drop): Identity()\n",
      "      (norm_pre): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (2): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (3): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (4): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (5): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (6): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (7): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (8): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (9): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (10): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (11): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate=none)\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (fc_norm): Identity()\n",
      "      (head_drop): Dropout(p=0.0, inplace=False)\n",
      "      (head): Identity()\n",
      "    )\n",
      "    (head): Sequential(\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (proj): Linear(in_features=768, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (text): HFTextEncoder(\n",
      "    (transformer): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): ClsLastHiddenStatePooler()\n",
      "    (proj): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=640, bias=False)\n",
      "      (1): GELU(approximate=none)\n",
      "      (2): Linear(in_features=640, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trunk.cls_token\n",
      "trunk.pos_embed\n",
      "trunk.patch_embed.proj.weight\n",
      "trunk.patch_embed.proj.bias\n",
      "trunk.blocks.0.norm1.weight\n",
      "trunk.blocks.0.norm1.bias\n",
      "trunk.blocks.0.attn.qkv.weight\n",
      "trunk.blocks.0.attn.qkv.bias\n",
      "trunk.blocks.0.attn.proj.weight\n",
      "trunk.blocks.0.attn.proj.bias\n",
      "trunk.blocks.0.norm2.weight\n",
      "trunk.blocks.0.norm2.bias\n",
      "trunk.blocks.0.mlp.fc1.weight\n",
      "trunk.blocks.0.mlp.fc1.bias\n",
      "trunk.blocks.0.mlp.fc2.weight\n",
      "trunk.blocks.0.mlp.fc2.bias\n",
      "trunk.blocks.1.norm1.weight\n",
      "trunk.blocks.1.norm1.bias\n",
      "trunk.blocks.1.attn.qkv.weight\n",
      "trunk.blocks.1.attn.qkv.bias\n",
      "trunk.blocks.1.attn.proj.weight\n",
      "trunk.blocks.1.attn.proj.bias\n",
      "trunk.blocks.1.norm2.weight\n",
      "trunk.blocks.1.norm2.bias\n",
      "trunk.blocks.1.mlp.fc1.weight\n",
      "trunk.blocks.1.mlp.fc1.bias\n",
      "trunk.blocks.1.mlp.fc2.weight\n",
      "trunk.blocks.1.mlp.fc2.bias\n",
      "trunk.blocks.2.norm1.weight\n",
      "trunk.blocks.2.norm1.bias\n",
      "trunk.blocks.2.attn.qkv.weight\n",
      "trunk.blocks.2.attn.qkv.bias\n",
      "trunk.blocks.2.attn.proj.weight\n",
      "trunk.blocks.2.attn.proj.bias\n",
      "trunk.blocks.2.norm2.weight\n",
      "trunk.blocks.2.norm2.bias\n",
      "trunk.blocks.2.mlp.fc1.weight\n",
      "trunk.blocks.2.mlp.fc1.bias\n",
      "trunk.blocks.2.mlp.fc2.weight\n",
      "trunk.blocks.2.mlp.fc2.bias\n",
      "trunk.blocks.3.norm1.weight\n",
      "trunk.blocks.3.norm1.bias\n",
      "trunk.blocks.3.attn.qkv.weight\n",
      "trunk.blocks.3.attn.qkv.bias\n",
      "trunk.blocks.3.attn.proj.weight\n",
      "trunk.blocks.3.attn.proj.bias\n",
      "trunk.blocks.3.norm2.weight\n",
      "trunk.blocks.3.norm2.bias\n",
      "trunk.blocks.3.mlp.fc1.weight\n",
      "trunk.blocks.3.mlp.fc1.bias\n",
      "trunk.blocks.3.mlp.fc2.weight\n",
      "trunk.blocks.3.mlp.fc2.bias\n",
      "trunk.blocks.4.norm1.weight\n",
      "trunk.blocks.4.norm1.bias\n",
      "trunk.blocks.4.attn.qkv.weight\n",
      "trunk.blocks.4.attn.qkv.bias\n",
      "trunk.blocks.4.attn.proj.weight\n",
      "trunk.blocks.4.attn.proj.bias\n",
      "trunk.blocks.4.norm2.weight\n",
      "trunk.blocks.4.norm2.bias\n",
      "trunk.blocks.4.mlp.fc1.weight\n",
      "trunk.blocks.4.mlp.fc1.bias\n",
      "trunk.blocks.4.mlp.fc2.weight\n",
      "trunk.blocks.4.mlp.fc2.bias\n",
      "trunk.blocks.5.norm1.weight\n",
      "trunk.blocks.5.norm1.bias\n",
      "trunk.blocks.5.attn.qkv.weight\n",
      "trunk.blocks.5.attn.qkv.bias\n",
      "trunk.blocks.5.attn.proj.weight\n",
      "trunk.blocks.5.attn.proj.bias\n",
      "trunk.blocks.5.norm2.weight\n",
      "trunk.blocks.5.norm2.bias\n",
      "trunk.blocks.5.mlp.fc1.weight\n",
      "trunk.blocks.5.mlp.fc1.bias\n",
      "trunk.blocks.5.mlp.fc2.weight\n",
      "trunk.blocks.5.mlp.fc2.bias\n",
      "trunk.blocks.6.norm1.weight\n",
      "trunk.blocks.6.norm1.bias\n",
      "trunk.blocks.6.attn.qkv.weight\n",
      "trunk.blocks.6.attn.qkv.bias\n",
      "trunk.blocks.6.attn.proj.weight\n",
      "trunk.blocks.6.attn.proj.bias\n",
      "trunk.blocks.6.norm2.weight\n",
      "trunk.blocks.6.norm2.bias\n",
      "trunk.blocks.6.mlp.fc1.weight\n",
      "trunk.blocks.6.mlp.fc1.bias\n",
      "trunk.blocks.6.mlp.fc2.weight\n",
      "trunk.blocks.6.mlp.fc2.bias\n",
      "trunk.blocks.7.norm1.weight\n",
      "trunk.blocks.7.norm1.bias\n",
      "trunk.blocks.7.attn.qkv.weight\n",
      "trunk.blocks.7.attn.qkv.bias\n",
      "trunk.blocks.7.attn.proj.weight\n",
      "trunk.blocks.7.attn.proj.bias\n",
      "trunk.blocks.7.norm2.weight\n",
      "trunk.blocks.7.norm2.bias\n",
      "trunk.blocks.7.mlp.fc1.weight\n",
      "trunk.blocks.7.mlp.fc1.bias\n",
      "trunk.blocks.7.mlp.fc2.weight\n",
      "trunk.blocks.7.mlp.fc2.bias\n",
      "trunk.blocks.8.norm1.weight\n",
      "trunk.blocks.8.norm1.bias\n",
      "trunk.blocks.8.attn.qkv.weight\n",
      "trunk.blocks.8.attn.qkv.bias\n",
      "trunk.blocks.8.attn.proj.weight\n",
      "trunk.blocks.8.attn.proj.bias\n",
      "trunk.blocks.8.norm2.weight\n",
      "trunk.blocks.8.norm2.bias\n",
      "trunk.blocks.8.mlp.fc1.weight\n",
      "trunk.blocks.8.mlp.fc1.bias\n",
      "trunk.blocks.8.mlp.fc2.weight\n",
      "trunk.blocks.8.mlp.fc2.bias\n",
      "trunk.blocks.9.norm1.weight\n",
      "trunk.blocks.9.norm1.bias\n",
      "trunk.blocks.9.attn.qkv.weight\n",
      "trunk.blocks.9.attn.qkv.bias\n",
      "trunk.blocks.9.attn.proj.weight\n",
      "trunk.blocks.9.attn.proj.bias\n",
      "trunk.blocks.9.norm2.weight\n",
      "trunk.blocks.9.norm2.bias\n",
      "trunk.blocks.9.mlp.fc1.weight\n",
      "trunk.blocks.9.mlp.fc1.bias\n",
      "trunk.blocks.9.mlp.fc2.weight\n",
      "trunk.blocks.9.mlp.fc2.bias\n",
      "trunk.blocks.10.norm1.weight\n",
      "trunk.blocks.10.norm1.bias\n",
      "trunk.blocks.10.attn.qkv.weight\n",
      "trunk.blocks.10.attn.qkv.bias\n",
      "trunk.blocks.10.attn.proj.weight\n",
      "trunk.blocks.10.attn.proj.bias\n",
      "trunk.blocks.10.norm2.weight\n",
      "trunk.blocks.10.norm2.bias\n",
      "trunk.blocks.10.mlp.fc1.weight\n",
      "trunk.blocks.10.mlp.fc1.bias\n",
      "trunk.blocks.10.mlp.fc2.weight\n",
      "trunk.blocks.10.mlp.fc2.bias\n",
      "trunk.blocks.11.norm1.weight\n",
      "trunk.blocks.11.norm1.bias\n",
      "trunk.blocks.11.attn.qkv.weight\n",
      "trunk.blocks.11.attn.qkv.bias\n",
      "trunk.blocks.11.attn.proj.weight\n",
      "trunk.blocks.11.attn.proj.bias\n",
      "trunk.blocks.11.norm2.weight\n",
      "trunk.blocks.11.norm2.bias\n",
      "trunk.blocks.11.mlp.fc1.weight\n",
      "trunk.blocks.11.mlp.fc1.bias\n",
      "trunk.blocks.11.mlp.fc2.weight\n",
      "trunk.blocks.11.mlp.fc2.bias\n",
      "trunk.norm.weight\n",
      "trunk.norm.bias\n",
      "head.proj.weight\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.visual.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m linear1 \u001b[39m=\u001b[39m LinearAdapter(\u001b[39m10\u001b[39m)\n\u001b[1;32m     17\u001b[0m linear2 \u001b[39m=\u001b[39m LinearAdapter(\u001b[39m10\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m nor \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mnormalize(\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m module \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(linear1, linear2, nor)\n\u001b[1;32m     20\u001b[0m module\n",
      "File \u001b[0;32m~/miniconda3/envs/biomed/lib/python3.8/site-packages/torch/nn/functional.py:4620\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4618\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(normalize, (\u001b[39minput\u001b[39m, out), \u001b[39minput\u001b[39m, p\u001b[39m=\u001b[39mp, dim\u001b[39m=\u001b[39mdim, eps\u001b[39m=\u001b[39meps, out\u001b[39m=\u001b[39mout)\n\u001b[1;32m   4619\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4620\u001b[0m     denom \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(p, dim, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mclamp_min(eps)\u001b[39m.\u001b[39mexpand_as(\u001b[39minput\u001b[39m)\n\u001b[1;32m   4621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39m/\u001b[39m denom\n\u001b[1;32m   4622\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'norm'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearAdapter(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearAdapter, self).__init__()\n",
    "        self.hide_layer = nn.Linear(input_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        x = self.hide_layer(x)\n",
    "        return x\n",
    "\n",
    "linear1 = LinearAdapter(10)\n",
    "linear2 = LinearAdapter(10)\n",
    "nor = F.normalize(10)\n",
    "module = nn.Sequential(linear1, linear2, nor)\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangjialong/miniconda3/envs/biomed/lib/python3.8/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([], size=(1, 0), grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "x = torch.ones(1, 10)\n",
    "linear = nn.Linear(10, 0)\n",
    "y = linear(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "fea_dir1 = '/home/huangjialong/projects/BiomedCLIP-PUNCE/extract-features/result-final-gc-features/biomed-test/pt'\n",
    "fea_dir2 = '/home/huangjialong/projects/BiomedCLIP-PUNCE/extract-features/result-final-gc-features/biomed-test2/pt'\n",
    "fea_dir3 = '/home/huangjialong/projects/BiomedCLIP-PUNCE/extract-features/result-final-gc-features/biomed2/pt'\n",
    "pt_file = 'xCY20005897-NILM.pt'\n",
    "\n",
    "pt_path1 = os.path.join(fea_dir1, pt_file)\n",
    "pt_path2 = os.path.join(fea_dir2, pt_file)\n",
    "pt_path3 = os.path.join(fea_dir3, pt_file)\n",
    "fea1 = torch.load(pt_path1)\n",
    "fea2 = torch.load(pt_path2)\n",
    "fea3 = torch.load(pt_path3)\n",
    "fea2 /= fea3\n",
    "fea2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0756e+00,  4.8968e-01, -1.1921e+00,  7.8138e-01,  5.2103e-01,\n",
       "          -1.8302e+00, -1.1896e-01,  3.3987e-01,  2.6367e-01,  5.0438e-01],\n",
       "         [ 9.6812e-01,  9.8213e-01,  7.0351e-01,  1.1103e+00, -6.4374e-01,\n",
       "           9.4631e-01, -3.7023e+00,  6.7075e-01,  2.3759e-01,  7.2683e-01],\n",
       "         [ 2.0717e+00,  8.7473e-01,  1.4393e+00,  1.4921e+00,  5.6588e+00,\n",
       "           7.5281e-01,  9.3706e-01,  2.1748e+00,  1.4782e+00,  5.8152e-01],\n",
       "         [ 8.9782e-01,  4.5270e-01,  5.1339e-01,  9.2950e-01,  9.0871e-01,\n",
       "           6.8978e-01,  5.1612e-01,  7.1402e-01,  2.0777e+01,  9.7726e-01],\n",
       "         [ 2.0132e+00,  1.1093e+01,  7.7555e-01,  1.2542e-01,  7.6793e-01,\n",
       "           8.1515e-01, -4.7929e-01,  1.8532e+00,  3.7247e-01,  1.3500e+01]],\n",
       "\n",
       "        [[ 1.2070e+00,  7.4351e-01,  5.7571e-01,  1.0943e+00,  1.7656e+00,\n",
       "           6.5129e-01,  7.1894e-01,  1.7322e+00,  8.1733e-01,  7.1680e-01],\n",
       "         [-1.3851e+00,  1.9621e+00,  1.7829e+00,  1.0089e+00,  1.2072e+00,\n",
       "           1.8607e+00,  1.2389e+00,  1.4929e+00,  1.3664e+00,  1.7160e+00],\n",
       "         [ 3.0433e-01,  7.4731e-01,  1.7653e+00,  1.3714e+00,  8.0670e-01,\n",
       "           1.3209e+00,  1.8617e+00,  2.0305e+00,  1.5808e-01, -1.6662e-01],\n",
       "         [ 3.0889e-01,  6.2692e-01, -1.6174e+00,  7.3387e-01,  7.7707e-01,\n",
       "           1.7628e+00,  3.0598e+00, -3.3587e-02,  4.1725e+00,  6.3401e-01],\n",
       "         [ 8.2707e-01,  3.5733e+00,  1.3698e+00,  1.1331e+00,  4.4513e+00,\n",
       "           2.7241e+02,  9.5967e-01,  1.0472e+00,  1.4918e+00,  1.9110e+00]],\n",
       "\n",
       "        [[ 7.3149e-01, -3.9980e-01,  7.2002e-01,  1.3487e+00,  7.3067e-01,\n",
       "           2.0825e+00,  7.4290e-01,  1.3416e+00,  3.6305e-01,  1.2968e+00],\n",
       "         [ 8.8495e-01,  1.3557e+00, -2.3072e+01,  1.2799e+00,  9.8148e-01,\n",
       "          -6.8646e-01,  8.2216e-01,  9.6780e-01,  1.2623e+00,  1.1348e+00],\n",
       "         [ 1.1571e+00,  9.5001e-01,  1.4424e+00, -1.6803e-02,  9.5302e-01,\n",
       "           5.8723e-01,  1.0013e+00,  1.3394e+00,  8.6781e-01,  1.2468e+00],\n",
       "         [ 6.0629e-01,  1.8275e+00,  1.4292e+00,  1.4810e+00,  1.6037e+00,\n",
       "           6.3904e-01,  1.6377e+00,  1.7564e+00,  1.3741e+00,  1.8994e+00],\n",
       "         [ 1.6290e+00,  1.1002e+00,  1.3879e+00,  1.4395e+00,  9.3854e-01,\n",
       "           1.2160e+00,  1.1167e+00,  8.0951e-01,  7.1699e-01,  1.4761e+00]],\n",
       "\n",
       "        [[ 1.0513e+00,  2.7266e+00,  1.2722e+00,  8.8011e-01,  1.1793e+00,\n",
       "           8.6022e-01,  4.4865e-01,  8.3884e-01,  1.0043e+00,  9.0537e-01],\n",
       "         [ 1.5819e+00,  1.3782e+00,  3.9010e+00,  1.5767e+00, -2.6602e-01,\n",
       "           3.3831e+00,  4.0982e+00,  7.0432e-01,  3.7386e+00,  8.1805e+00],\n",
       "         [ 1.0480e+00,  6.5861e-01,  1.1273e+00,  5.7747e-01,  2.0973e-01,\n",
       "           1.1026e+00,  7.8188e-01,  6.9846e-01,  5.3155e+00,  6.9958e-01],\n",
       "         [ 1.2944e+00,  1.2724e+00,  1.4084e+00,  1.5193e+00,  2.0424e+00,\n",
       "           1.5156e+00,  1.5333e+00, -4.9903e-01,  1.2589e+00,  1.3499e+00],\n",
       "         [-3.7000e+01,  2.4873e+00,  9.9741e-01,  4.1906e+00, -8.6480e-02,\n",
       "          -3.9205e-01,  2.0177e+00,  1.0209e+00,  7.9193e-01,  7.7006e-01]],\n",
       "\n",
       "        [[ 1.0751e+00, -2.4585e-01,  6.7300e-02,  7.3891e-01,  2.1863e-01,\n",
       "           3.8139e-01,  8.3405e-01,  2.0702e+00,  1.1020e+00,  1.5364e+00],\n",
       "         [ 6.2775e-01, -8.6566e-01,  4.0387e+00, -9.3119e-01,  9.0940e-02,\n",
       "           6.7181e-01,  9.1091e-02,  5.8365e-01,  9.7077e-01,  1.9207e+00],\n",
       "         [ 1.6781e+00,  1.6415e+00,  9.1583e-01,  1.0941e+00,  2.3223e+00,\n",
       "           2.8307e+00,  6.8286e-01,  2.6097e+00,  1.1563e+00,  4.0947e+00],\n",
       "         [ 3.1012e+00,  9.4309e-01,  1.0076e+00,  9.7545e-01,  9.5464e-01,\n",
       "           8.5739e-01,  9.8153e-01,  9.4879e-01,  9.6960e-01,  9.9274e-01],\n",
       "         [ 6.6344e-01,  5.6681e-01,  1.2913e+00,  1.1747e+00,  1.1296e+00,\n",
       "           9.8215e-01, -6.2222e-01,  7.5490e-01,  6.2889e-01,  7.0103e-01]],\n",
       "\n",
       "        [[ 6.7230e-01,  7.8749e-01,  4.4214e-01,  7.7429e-01, -2.9173e+00,\n",
       "           1.6151e+00,  1.3746e+00,  6.8556e-01,  2.3969e+00,  2.8384e-01],\n",
       "         [ 3.0108e+00,  2.0963e-01,  8.1829e-01,  9.1599e-01,  1.5450e+00,\n",
       "           2.6732e+00,  1.2415e+01,  3.6709e-01,  9.0441e-01,  4.7070e+00],\n",
       "         [ 8.8060e-01,  8.8028e-01,  8.8051e-01,  8.8016e-01,  8.8013e-01,\n",
       "           8.8040e-01,  8.8035e-01,  8.8067e-01,  8.8015e-01,  8.8067e-01],\n",
       "         [ 8.1082e-01,  1.3167e+00,  5.7381e-01,  1.3156e+00,  2.4205e-01,\n",
       "           1.1415e+00,  7.6556e-01,  6.2346e-01,  1.4137e+00,  7.0241e-01],\n",
       "         [ 1.4064e+00,  8.9430e-01,  7.8584e-01,  7.7049e-01,  8.9823e-01,\n",
       "           7.5679e-01,  1.0460e+00,  3.0857e-01, -4.0551e-02,  9.8248e-01]],\n",
       "\n",
       "        [[ 1.0337e+00,  5.1002e+00,  1.1643e+00,  7.0300e-01,  1.2607e+00,\n",
       "           8.2362e-01,  1.5394e+00,  7.7977e-01,  5.6901e+00,  2.0558e+00],\n",
       "         [ 5.7505e-01,  6.0900e-01,  6.8390e-01,  2.0542e+00,  1.5216e+00,\n",
       "           5.5716e-01,  3.5280e-01, -8.2700e-02,  2.3165e-02,  5.8145e+00],\n",
       "         [ 1.4288e+00,  3.2106e+00, -3.9281e-01,  5.7737e-01,  5.3115e-01,\n",
       "          -1.2570e-01,  1.2583e-01,  4.4353e-01,  1.5131e+00,  5.9796e-01],\n",
       "         [-9.5279e-01,  7.0510e-01,  9.4173e-01,  7.3774e-01,  1.5186e+00,\n",
       "           6.7594e-01,  9.1893e-01,  7.2451e-01,  5.6698e-01,  9.2071e-01],\n",
       "         [ 1.4961e+00,  8.8949e-01,  6.9539e-01,  9.9864e-01,  1.4437e+00,\n",
       "           9.3398e-01,  6.2424e-01,  1.2946e+00,  7.4088e-01,  7.6011e-01]],\n",
       "\n",
       "        [[ 8.9806e-01,  8.5105e-01,  8.6043e-01,  8.4288e-01,  9.9045e-01,\n",
       "           8.5082e-01,  9.1574e-01,  4.1839e-01,  9.2433e-01,  8.1632e-01],\n",
       "         [ 1.2675e+00,  1.2000e+00,  7.9152e-01,  5.7393e-01,  5.0759e-01,\n",
       "           1.1757e+00,  9.6363e-01,  3.2098e-01,  9.8752e-01,  1.5821e+00],\n",
       "         [ 8.2840e-01, -1.2046e+00, -9.4462e-01, -1.5374e+00,  1.2345e+00,\n",
       "           2.0689e+00,  2.0768e+00,  1.0184e+00,  1.3052e+02,  1.1056e+00],\n",
       "         [ 2.6807e+00,  8.1704e-01, -5.5828e-01,  1.4801e+00,  6.0229e-01,\n",
       "           6.5473e-01,  2.5808e+01,  1.5045e-01,  8.5761e-01, -7.6999e+00],\n",
       "         [ 8.5241e-01,  8.4013e-01,  8.0331e-01,  9.2963e-01,  7.8623e-01,\n",
       "           8.5213e-01,  7.7391e-01,  8.3849e-01,  8.0024e-01,  8.5881e-01]],\n",
       "\n",
       "        [[ 9.8324e-01,  1.0812e+00,  7.8233e-01,  6.4428e-01,  1.1351e+00,\n",
       "           6.6362e-01,  7.3742e-01,  9.8256e-01,  3.2733e-01,  1.3131e+00],\n",
       "         [ 1.0052e+00,  1.4325e+00,  7.9661e-01,  7.0352e-01,  8.4859e-01,\n",
       "           8.4170e-01,  1.0861e+00,  8.8533e-01,  9.7200e-01,  8.7590e-01],\n",
       "         [ 2.2732e+00,  8.1941e-01,  1.0329e+00,  3.6116e+00, -2.5366e-01,\n",
       "           1.3664e+00,  8.4658e-01,  1.8688e+00, -2.3099e-01,  9.1434e-01],\n",
       "         [ 1.9153e+00,  2.0496e+00, -7.2780e-01,  1.1159e+00, -6.5200e-02,\n",
       "           1.8313e+00,  9.3360e-01,  3.8202e+00,  9.7398e-01,  1.1026e+00],\n",
       "         [ 9.3242e-01,  1.0046e+00,  7.8768e-01,  8.4603e-01,  9.4730e-01,\n",
       "           1.0268e+00,  1.2868e+00,  5.4355e-01,  1.0816e+00,  6.6420e-01]],\n",
       "\n",
       "        [[ 8.7898e-01,  8.6138e-01,  8.8215e-01,  8.3495e-01,  1.0656e+00,\n",
       "           8.5835e-01,  7.8705e-01,  1.1754e+00,  9.3332e-01,  8.9207e-01],\n",
       "         [-4.7969e-02, -6.0334e-01,  3.8483e+00,  8.2099e-01,  6.5836e-01,\n",
       "           8.9778e-01,  5.1330e-01, -7.5601e-01,  1.6018e+00,  2.7956e+00],\n",
       "         [ 6.4137e+00,  1.7246e+00,  2.0787e+00,  1.7575e+00,  6.1420e+00,\n",
       "           2.5158e+00,  1.7703e+00,  1.3284e+00,  2.7675e+00,  1.7536e+00],\n",
       "         [ 1.1572e+00,  1.1690e+00, -5.8691e-01,  5.2975e+00,  3.1684e+00,\n",
       "           3.2709e+00,  2.2399e+00, -2.5330e+00,  1.3168e+01,  3.7095e-01],\n",
       "         [ 5.3892e+00,  1.4610e+00,  1.9503e+00,  1.7560e+00,  1.3405e+00,\n",
       "           1.3430e+00,  1.4556e+00,  1.3354e+00,  1.5123e+00,  1.4544e+00]],\n",
       "\n",
       "        [[ 7.3416e-01,  1.0708e+00,  8.7925e-01,  1.6292e-01,  2.4926e-01,\n",
       "           5.7409e-01,  6.6464e-01,  9.0455e-01,  7.4850e-01,  1.0196e-01],\n",
       "         [ 1.0572e+00,  8.2012e+01,  9.4785e-01,  1.2272e+00,  9.6071e-01,\n",
       "           1.1699e+00,  8.5956e-01,  1.2434e+00,  1.3089e+00,  1.1856e+00],\n",
       "         [ 5.1743e+00,  2.0201e-01,  7.2326e-01,  5.6065e+00,  9.5481e-01,\n",
       "          -1.3008e-01,  9.7536e-01, -2.4330e+00,  4.6143e+00,  1.8521e-02],\n",
       "         [ 1.3812e+00, -6.1913e-01,  7.0502e-02,  3.1578e+00,  6.5330e-01,\n",
       "           8.2746e-01,  2.0875e+00, -1.8871e-01, -6.1477e-02,  8.9197e-01],\n",
       "         [-4.3464e-01, -1.5053e-01,  1.2713e+00,  6.4286e-01,  1.0206e+00,\n",
       "           6.9105e+00,  1.3279e+00,  3.9273e-01,  3.1722e+00,  7.3213e-01]],\n",
       "\n",
       "        [[ 7.4701e-02,  9.3666e-01,  6.0908e-01,  1.7052e+00,  5.4179e-01,\n",
       "           4.1261e-01,  1.0463e+00,  7.4974e+00,  2.4016e+00,  3.4323e-01],\n",
       "         [ 9.1182e-01, -1.1613e+01, -2.8464e+00,  6.1333e-02,  2.5403e-01,\n",
       "          -2.7475e-01, -2.0674e+00,  6.3117e-01,  1.2985e+00,  6.0253e-01],\n",
       "         [ 1.1136e+00,  9.7127e-01,  3.6803e-01,  6.2626e-01,  1.7423e+00,\n",
       "           3.3664e+00,  8.0087e-01,  8.5215e-01,  2.9563e+00,  1.7486e+00],\n",
       "         [ 1.0516e+00,  1.0257e+00,  1.0062e+00,  9.1502e-01,  1.0244e+00,\n",
       "           1.0016e+00,  1.1127e+00,  1.0084e+00,  1.0212e+00,  9.9548e-01],\n",
       "         [ 9.2945e-01,  7.5556e-01,  2.1436e+00,  8.1572e-01,  1.1545e+00,\n",
       "           1.1961e-01,  1.5510e+00,  1.3092e+00,  9.2720e-01,  1.3767e+00]],\n",
       "\n",
       "        [[ 1.6001e+00,  1.1273e+00,  1.0026e+00,  9.6217e-01,  9.5995e-01,\n",
       "           1.5112e+00,  9.4435e-01,  1.2429e+00,  3.3969e-01,  1.2495e+00],\n",
       "         [ 1.1797e+00,  6.3190e-01,  2.6950e-01,  1.0825e+00,  1.0419e+00,\n",
       "           7.9540e-01,  6.2458e-01,  6.9147e-01,  7.0049e-01,  7.9745e-01],\n",
       "         [ 2.0871e+00,  1.7529e+00,  8.5897e-01,  5.3960e-01,  1.0712e+01,\n",
       "           4.4662e-01,  1.9051e-01,  9.0802e-02,  7.8320e-01,  5.0943e-01],\n",
       "         [ 3.1635e+00,  4.6984e-01,  1.3706e-01,  3.2580e+00,  9.2698e-01,\n",
       "           5.0442e-01,  9.3423e-01,  2.4179e+00,  3.6583e+00,  2.1063e+00],\n",
       "         [ 1.2330e+00,  1.2698e+00,  1.3173e+00,  1.2502e+00,  1.2547e+00,\n",
       "           1.2401e+00,  1.2395e+00,  1.2460e+00,  1.2566e+00,  1.2604e+00]],\n",
       "\n",
       "        [[ 1.8770e+00,  6.1321e-01,  9.7591e-01,  3.6340e-02,  1.9166e-01,\n",
       "           6.8426e-01,  1.2986e+00,  5.5516e-01,  3.2029e-01,  1.8586e+00],\n",
       "         [ 9.1249e-01,  2.8653e+00,  9.9373e-01,  2.5900e+00,  3.1578e+00,\n",
       "           3.9013e-02,  1.5175e+00,  1.6558e+00,  1.0168e+00,  1.0333e+00],\n",
       "         [ 3.2779e-01, -1.3987e+00,  9.1357e-02,  9.4798e-01,  2.6998e+00,\n",
       "           2.5555e+00,  8.9761e-01, -1.1217e+00,  2.1879e-01,  1.1164e-01],\n",
       "         [ 2.5544e+00,  1.9510e+00,  1.0255e+00,  1.2233e+00,  1.2526e+00,\n",
       "           1.7696e+00,  1.1273e+00,  1.6738e+00,  2.2942e+00,  2.8261e+00],\n",
       "         [ 8.8558e-01,  9.0169e-01,  7.7100e-01,  7.0731e-01,  7.0869e-01,\n",
       "           6.8091e-01,  6.6657e-01,  6.7149e-01,  6.1483e-01,  6.5087e-01]],\n",
       "\n",
       "        [[ 1.0696e+00,  1.0007e+00,  1.1984e+00,  1.2143e+00,  1.0873e+00,\n",
       "           9.2237e-01,  8.2356e-01,  8.6355e-01,  1.1777e+00,  1.1820e+00],\n",
       "         [-1.2737e+00,  8.1306e-01,  1.2999e+00,  1.3294e+00,  3.1122e+00,\n",
       "           2.0571e+00,  1.0283e+00, -3.2838e-02, -1.2881e+00,  9.9260e+00],\n",
       "         [ 1.0790e+00,  1.3416e+00,  1.9069e+00,  1.2531e+00, -3.1675e+00,\n",
       "           1.2331e+00,  1.6182e+00,  1.6920e+00,  1.9589e+00,  2.8060e+00],\n",
       "         [ 1.4265e+00,  9.4736e-01,  1.1760e+00,  1.6695e+00,  1.1931e+00,\n",
       "           1.4893e+00,  7.7276e-01,  1.5397e+00,  1.0712e+00, -5.7863e-01],\n",
       "         [ 8.4275e-01,  2.0350e+01, -1.3513e+01,  7.3880e-01, -4.3401e-01,\n",
       "          -5.6840e+00,  6.9933e-01,  7.6540e-01,  1.3225e+00,  2.1152e+00]],\n",
       "\n",
       "        [[ 2.6703e+00,  6.8820e-01,  6.1430e-01,  5.2711e+00, -2.2149e+00,\n",
       "           4.1871e+00,  2.6177e+00,  1.6007e+00,  1.5586e+00,  6.2619e-01],\n",
       "         [ 1.3638e+00,  1.0218e+00,  2.0818e-01,  8.0068e-01,  9.8656e-01,\n",
       "           7.5307e-01,  7.0669e-01,  7.9313e-01,  1.1452e+00,  1.3298e+00],\n",
       "         [-1.6835e+00,  9.7934e-01,  8.7709e-01, -8.2661e-01,  1.0518e+01,\n",
       "          -3.9166e-02,  1.9648e+00,  2.0092e+00,  9.3286e-01,  7.6381e-01],\n",
       "         [ 9.3669e-01,  9.9884e-01,  3.8780e+00,  2.0474e+00,  6.9173e-01,\n",
       "           8.5373e-01,  1.3335e+00,  7.5187e-01,  2.5005e+00,  2.9371e-01],\n",
       "         [ 8.9338e-01,  9.8787e-01,  1.0210e+00,  1.2740e+00,  9.0385e-01,\n",
       "           9.0940e-01,  2.0516e+00, -3.3305e-01,  1.0891e+00,  8.8712e-01]],\n",
       "\n",
       "        [[ 2.4085e+00,  1.8428e+00,  8.7271e-01,  1.0736e+00,  9.5846e-01,\n",
       "           5.6248e-01,  2.9656e-01,  9.6665e-01,  8.0164e-01, -1.8032e+01],\n",
       "         [ 3.8904e-01,  1.1397e+00,  9.3040e-01,  7.5962e-01,  1.0086e+00,\n",
       "           7.5136e-01, -1.5510e-01,  1.3127e+00,  7.2525e-01,  6.4009e-01],\n",
       "         [ 1.2228e+00,  5.6936e-01,  6.5611e-01,  5.5910e-01,  6.8682e-01,\n",
       "           1.1003e+00,  1.0919e+00,  8.1421e-01,  2.3513e+01,  7.5460e-01],\n",
       "         [ 5.9371e-01,  1.6524e-01,  1.4189e+00,  6.4180e-01,  7.1155e-01,\n",
       "           3.0907e+00,  3.9267e-01, -1.0121e+00,  6.0542e-01, -4.7880e+00],\n",
       "         [ 9.2855e-01,  1.2763e+00,  3.3730e-01, -1.2441e-01,  6.5862e-01,\n",
       "           3.5060e+00,  7.5468e-01,  1.6508e+00,  1.9541e+00,  7.9324e-01]],\n",
       "\n",
       "        [[ 5.6017e-01,  9.4095e-01,  1.5757e+00,  1.2836e+00,  1.8694e+00,\n",
       "           9.9297e-01,  1.1367e+00,  9.3033e-01,  1.2957e+00,  1.3023e+00],\n",
       "         [ 1.2499e+00,  1.3830e+00,  8.8163e-01,  4.4968e-01,  9.4325e-01,\n",
       "           6.1453e-01,  1.1904e+00,  1.0838e+00,  8.9762e-01,  1.2589e+00],\n",
       "         [ 2.6774e+00,  9.6700e+00,  4.6003e-01,  1.9160e+00,  2.1080e+00,\n",
       "           5.9181e-02, -2.4894e-01,  1.8785e+00,  2.8692e+00,  1.6182e+00],\n",
       "         [ 3.2421e-01,  6.3281e-01,  2.7450e-01,  6.2811e-01,  1.8124e-01,\n",
       "           2.7432e-01,  1.6752e+00,  1.3257e+00,  1.3825e+00,  2.1703e-01],\n",
       "         [ 1.4495e+00, -4.1489e+00,  1.6032e+00,  1.3914e+00,  4.0251e+00,\n",
       "           1.4127e+00,  1.3852e+00,  1.7428e+00,  2.7506e+00,  1.7820e+00]],\n",
       "\n",
       "        [[ 1.0229e+00,  1.6520e+00,  4.4606e-01,  1.1303e+00,  9.1895e-01,\n",
       "           8.7020e-01,  1.0597e+00,  9.8443e-01,  9.2380e-01,  4.7553e-01],\n",
       "         [ 9.9519e-01,  5.9200e-01,  9.9675e-01,  1.0201e+00,  6.1832e-01,\n",
       "           7.0315e-01,  4.1771e-01,  3.9828e-01,  7.3608e-01, -4.3680e+00],\n",
       "         [ 1.4115e+00,  1.3311e+00,  6.1208e-01,  1.3822e+00, -1.1255e+00,\n",
       "           5.5566e-01,  1.6476e+00,  1.0163e+00,  9.6156e-01,  9.4378e-01],\n",
       "         [ 4.7974e-01,  1.7400e+00,  9.6730e+00,  3.9870e-01,  6.9821e-01,\n",
       "           6.3349e+00,  3.0398e-01,  7.3924e-01,  2.0129e+00,  3.7674e-01],\n",
       "         [ 5.5108e-01,  5.3356e-01,  9.8306e-01,  4.1879e-01,  9.2545e-02,\n",
       "           8.6156e-01,  4.9276e-01,  3.8437e-02,  4.3751e-01,  4.2549e-01]],\n",
       "\n",
       "        [[ 1.9235e+00,  1.3126e+00,  7.3300e-01,  7.9056e-01,  7.8872e-01,\n",
       "           7.0984e-01,  2.5030e+00,  1.2866e+00,  6.4108e-01,  1.7774e+00],\n",
       "         [ 1.7312e+00,  1.1851e+00,  9.4556e-01,  1.7032e+00,  9.8116e-01,\n",
       "           1.0570e+00,  1.0914e+00,  1.8880e+00,  2.0183e+00,  1.0639e+00],\n",
       "         [ 1.1024e-01,  7.3279e-01, -5.5883e-01,  7.7526e-01,  1.5026e-01,\n",
       "           1.2881e+00,  1.1942e+00,  6.8664e-01,  7.4788e-01,  5.6955e-01],\n",
       "         [ 7.5680e-01, -4.4902e+01,  4.1971e-01,  1.2976e+01,  6.4754e+00,\n",
       "          -2.4710e+00, -1.6802e-01, -4.0880e+00, -5.6519e+00,  7.1767e-01],\n",
       "         [ 6.7273e-01, -2.6295e+00, -5.5220e-01, -6.7437e+00, -2.6203e+00,\n",
       "          -7.5456e-01, -2.8689e+00,  8.3804e-01,  5.5191e-01, -1.1630e+00]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# NLP Example\n",
    "# NLP Example\n",
    "# NLP Example\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "# Activate module\n",
    "out = layer_norm(embedding)\n",
    "out /= embedding\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangjialong/miniconda3/envs/biomed/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/huangjialong/miniconda3/envs/biomed/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "pretrained = True\n",
    "model_baseline = models.resnet50(pretrained=pretrained)\n",
    "print(model_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "from torch import nn\n",
    "class BiomedclipBackbone(nn.Module):\n",
    "    def __init__(self, without_head: bool = False):\n",
    "        super(BiomedclipBackbone, self).__init__()\n",
    "        \n",
    "        model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "        if without_head:\n",
    "            model = model.visual.trunk\n",
    "        else:\n",
    "            model = model.visual\n",
    "        self.model = model\n",
    "        self.preprocess_val = preprocess_val\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        features /= features.norm(p=2, dim=-1, keepdim=True)\n",
    "        return features\n",
    "\n",
    "model = BiomedclipBackbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x7f941ee09f70>\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(224, 224), interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "preprocess_val = model.preprocess_val\n",
    "print(preprocess_val)\n",
    "\n",
    "transfrom = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor()])\n",
    "transfrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPProcessor:\n",
      "- image_processor: CLIPImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"images\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"do_convert_rgb\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: CLIPTokenizerFast(name_or_path='vinid/plip', vocab_size=49408, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"CLIPProcessor\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CLIPProcessor' object has no attribute 'transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(preprocess_val)\n\u001b[1;32m      4\u001b[0m preprocess_list \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m preprocess_val\u001b[39m.\u001b[39;49mtransforms:\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, transforms\u001b[39m.\u001b[39mNormalize):\n\u001b[1;32m      7\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CLIPProcessor' object has no attribute 'transforms'"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "preprocess_val = model.preprocess_val\n",
    "print(preprocess_val)\n",
    "preprocess_list = []\n",
    "for t in preprocess_val.transforms:\n",
    "    if isinstance(t, transforms.Normalize):\n",
    "        continue\n",
    "    preprocess_list.append(t)\n",
    "preprocess = transforms.Compose(preprocess_list)\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPProcessor\n",
    "import torch\n",
    "preprocess_val = CLIPProcessor.from_pretrained(\"vinid/plip\")\n",
    "data = torch.rand(3,224,224)\n",
    "tmp = preprocess_val(images=data, return_tensors=\"pt\")\n",
    "tmp['pixel_values']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faad65a3910>,\n",
       " <matplotlib.lines.Line2D at 0x7faad65a3970>,\n",
       " <matplotlib.lines.Line2D at 0x7faad65a39a0>,\n",
       " <matplotlib.lines.Line2D at 0x7faad65a3a90>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_1 = np.array([[78.8,77.2,78.4,78.1,78.2,77.5],\n",
    "               [82.7,82.6,83.1,83.5,83.2,82.7],\n",
    "               [84.9,85.2,85.7,86,85.7,85.6],\n",
    "               [86.4,86.6,86.9,86.8,86.75,86.6]])\n",
    "\n",
    "\n",
    "df_1 =  np.array([[96.93, 97.59, 98.84, 97.26],\n",
    "        [97.98, 97.97, 99.14, 97.78],\n",
    "        [98.32, 98.23, 99.17, 98.39],\n",
    "        [97.92, 98.35, 99.18, 98.88]])\n",
    "plt.plot(df_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

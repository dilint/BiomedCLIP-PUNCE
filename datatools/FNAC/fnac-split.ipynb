{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分数据集并重新创建一个目录存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# 设置目录路径\n",
    "DATA_PATH = \"D:/Project/biomed-clip-puNCE/output/FNAC\"\n",
    "OUTPUT_PATH = \"..\\\\..\\\\output\\\\FNAC-split\"\n",
    "\n",
    "# 设置划分比例\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# 创建保存划分结果的目录\n",
    "train_dir = os.path.join(OUTPUT_PATH, \"train\")\n",
    "val_dir = os.path.join(OUTPUT_PATH, \"val\")\n",
    "test_dir = os.path.join(OUTPUT_PATH, \"test\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# 获取子目录列表\n",
    "subdirectories = [\"B\", \"M\"]\n",
    "\n",
    "# 遍历每个子目录\n",
    "for subdirectory in subdirectories:\n",
    "    subdir_path = os.path.join(DATA_PATH, subdirectory)\n",
    "    \n",
    "    # 获取子目录下的所有文件\n",
    "    file_list = os.listdir(subdir_path)\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    # 计算划分边界\n",
    "    num_files = len(file_list)\n",
    "    num_train = int(train_ratio * num_files)\n",
    "    num_val = int(val_ratio * num_files)\n",
    "    \n",
    "    # 划分文件并复制到对应的目录\n",
    "    train_files = file_list[:num_train]\n",
    "    val_files = file_list[num_train:num_train+num_val]\n",
    "    test_files = file_list[num_train+num_val:]\n",
    "    \n",
    "    for file in train_files:\n",
    "        src_path = os.path.join(subdir_path, file)\n",
    "        dst_path = os.path.join(train_dir, subdirectory, file)\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        \n",
    "    for file in val_files:\n",
    "        src_path = os.path.join(subdir_path, file)\n",
    "        dst_path = os.path.join(val_dir, subdirectory, file)\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        \n",
    "    for file in test_files:\n",
    "        src_path = os.path.join(subdir_path, file)\n",
    "        dst_path = os.path.join(test_dir, subdirectory, file)\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        shutil.copy(src_path, dst_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成W3标注文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# **一般只需要修改这个 就可以在不同机器上运行\n",
    "# **所有路径都不要使用单个反斜杠\n",
    "# Benign 对应 1，malignant 对应 0\n",
    "\n",
    "DATA_ROOT = \"D:/Dataset/\"\n",
    "config_name = '../../settings/environment.json'\n",
    "if os.path.exists(config_name):\n",
    "    with open(config_name, 'r') as file:\n",
    "        info = json.load(file)\n",
    "        DATA_ROOT = info[\"FNAC_DATA_ROOT\"]\n",
    "\n",
    "input_dir = DATA_ROOT+'FNAC-CROP/base-data/'\n",
    "output_dir = DATA_ROOT+'FNAC-CROP/annotations/w3/'\n",
    "\n",
    "def get_all_wsi(input_dir):\n",
    "    \"\"\"获取所有wsi路径\"\"\"\n",
    "    wsi_set = set()\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            # 获取所有图片的上一层路径名称\n",
    "            category = 1 if os.path.basename(os.path.dirname(root))==\"B\" else 0\n",
    "            wsi_name = os.path.basename(root)\n",
    "            wsi_set.add((wsi_name, category))\n",
    "\n",
    "def split_w3_dataset(input_dir, output_dir):\n",
    "    # 创建输出目录\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 获取所有wsi文件的相对路径\n",
    "    wsi_set = get_all_wsi(input_dir)\n",
    "\n",
    "    # 打乱图片文件列表\n",
    "    wsi_set = list(wsi_set)\n",
    "    random.shuffle(wsi_set)\n",
    " \n",
    "    # 计算训练、验证和测试集的数量\n",
    "    num_wsi = len(wsi_set)\n",
    "    num_train = int(0.8 * num_wsi)\n",
    "    num_val = int(0.1 * num_wsi)\n",
    "\n",
    "    # 划分数据集并保存到相应文件\n",
    "    with open(os.path.join(output_dir, 'train.txt'), 'w') as train_file:\n",
    "        for wsi_name, category in wsi_set[:num_train]:\n",
    "            train_file.write(f\"{wsi_name} {category}\\n\")\n",
    "\n",
    "    with open(os.path.join(output_dir, 'val.txt'), 'w') as val_file:\n",
    "        for wsi_name, category in wsi_set[num_train:num_train + num_val]:\n",
    "            val_file.write(f\"{wsi_name} {category}\\n\")\n",
    "\n",
    "    with open(os.path.join(output_dir, 'test.txt'), 'w') as test_file:\n",
    "        for wsi_name, category in wsi_set[num_train + num_val:]:\n",
    "            test_file.write(f\"{wsi_name} {category}\\n\")\n",
    "    print(\"W3 for FNAC-CROP split and saved successfully.\")\n",
    "\n",
    "# split_w3_dataset(input_dir, output_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计w3标注信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt中的各类WSI数量如下：\n",
      "类别0:    91\n",
      "类别1:    78\n",
      "全部 :   169\n",
      "val.txt中的各类WSI数量如下：\n",
      "类别0:    11\n",
      "类别1:    10\n",
      "全部 :    21\n",
      "test.txt中的各类WSI数量如下：\n",
      "类别0:    11\n",
      "类别1:    11\n",
      "全部 :    22\n",
      "  \n",
      "总和 :   212\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_ROOT = \"D:/Dataset/\"\n",
    "config_name = '../../settings/environment.json'\n",
    "if os.path.exists(config_name):\n",
    "    with open(config_name, 'r') as file:\n",
    "        info = json.load(file)\n",
    "        DATA_ROOT = info[\"FNAC_DATA_ROOT\"]\n",
    "\n",
    "ann_dir = DATA_ROOT+'FNAC-CROP/annotations/w3/'\n",
    "files=['train.txt', 'val.txt', 'test.txt']\n",
    "\n",
    "def info_ann(file_name):\n",
    "    print(f\"{file_name}中的各类WSI数量如下：\")\n",
    "    items = []\n",
    "    with open(ann_dir+file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            items.append(line.strip().split())\n",
    "    \n",
    "    labels = [int(data[1]) for data in items]\n",
    "    max_label = max(labels)\n",
    "    counts = np.zeros(max_label+1, dtype=np.uint16)\n",
    "    \n",
    "    # 统计标注为 0 和 1 的个数\n",
    "    for item in items:\n",
    "        label = int(item[1])\n",
    "        counts[label] += 1\n",
    "\n",
    "    # 输出统计结果\n",
    "    for i, count in enumerate(counts):\n",
    "        print('类别{}: {:>5}'.format(i, count))\n",
    "    print('全部 : {:>5}'.format(sum(counts)))\n",
    "    return sum(counts)\n",
    "\n",
    "def main():\n",
    "    sum_all = 0    \n",
    "    for file in files:\n",
    "        sum_all += info_ann(file)\n",
    "    print(\"  \")\n",
    "    print('总和 : {:>5}'.format(sum_all))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成W1标注文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# **所有路径都不要使用单个反斜杠\n",
    "# Benign 对应 1，malignant 对应 0\n",
    "DATA_ROOT = \"D:/Dataset/\"\n",
    "config_name = '../../settings/environment.json'\n",
    "if os.path.exists(config_name):\n",
    "    with open(config_name, 'r') as file:\n",
    "        info = json.load(file)\n",
    "        DATA_ROOT = info[\"FNAC_DATA_ROOT\"]\n",
    "        \n",
    "input_dir = DATA_ROOT+'FNAC-CROP/base-data/'\n",
    "w3_dir = DATA_ROOT+'FNAC-CROP/annotations/w3/'\n",
    "output_dir = DATA_ROOT+'FNAC-CROP/annotations/w1/'\n",
    "\n",
    "files=['train.txt', 'val.txt', 'test.txt']\n",
    "\n",
    "def w1_split_dataset(input_dir, w3_dir, file_name, output_dir):\n",
    "    # 根据w3(wsi数据)划分w1(patch数据)\n",
    "    os.makedirs(output_dir, exist_ok=True)    \n",
    "    w3_ann_file = w3_dir + file_name\n",
    "\n",
    "    items=[]\n",
    "    with open(w3_ann_file, 'r') as file:\n",
    "        for line in file:\n",
    "            items.append(line)\n",
    "    wsis = [item.split(\" \")[0] for item in items]\n",
    "    labels = [item.split(\" \")[1] for item in items]\n",
    "    \n",
    "    wsi_dirs = []\n",
    "    wsi_labels = []\n",
    "    image_files = []\n",
    "    # 找到对应wsi的目录\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for dir in dirs:\n",
    "            if dir in wsis:\n",
    "                wsi_labels.append(labels[wsis.index(dir)])\n",
    "                wsi_dirs.append(os.path.join(root, dir))\n",
    "    \n",
    "    # 将图片读取\n",
    "    for i, dir in enumerate(wsi_dirs):\n",
    "        image_list = os.listdir(dir)\n",
    "        for file in image_list:\n",
    "            if file.lower().endswith('.jpg') or file.lower().endswith('.png'):\n",
    "                # 确定图片类别，假设子目录名称为类别名称\n",
    "                category = int(wsi_labels[i])\n",
    "                image_path = os.path.relpath(os.path.join(dir, file), input_dir)\n",
    "                image_files.append((image_path, category))\n",
    "\n",
    "    # 保存到相应文件\n",
    "    with open(os.path.join(output_dir, file_name), 'w') as train_file:\n",
    "        for image_file, category in image_files:\n",
    "            train_file.write(f\"{image_file} {category}\\n\")\n",
    "\n",
    "for file_name in files:\n",
    "    w1_split_dataset(input_dir, w3_dir, file_name, output_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计w1标注信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt中的各类WSI数量如下：\n",
      "类别0:  15015\n",
      "类别1:  12870\n",
      "全部 :  27885\n",
      "val.txt中的各类WSI数量如下：\n",
      "类别0:   1815\n",
      "类别1:   1650\n",
      "全部 :   3465\n",
      "test.txt中的各类WSI数量如下：\n",
      "类别0:   1815\n",
      "类别1:   1815\n",
      "全部 :   3630\n",
      "  \n",
      "总和 :  34980\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "DATA_ROOT = \"D:/Dataset/\"\n",
    "config_name = '../../settings/environment.json'\n",
    "if os.path.exists(config_name):\n",
    "    with open(config_name, 'r') as file:\n",
    "        info = json.load(file)\n",
    "        DATA_ROOT = info[\"FNAC_DATA_ROOT\"]\n",
    "        \n",
    "ann_dir = DATA_ROOT+'FNAC-CROP/annotations/w1/'\n",
    "files=['train.txt', 'val.txt', 'test.txt']\n",
    "\n",
    "def info_ann(file_name):\n",
    "    print(f\"{file_name}中的各类WSI数量如下：\")\n",
    "    items = []\n",
    "    with open(ann_dir+file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            items.append(line.strip().split())\n",
    "    \n",
    "    labels = [int(data[1]) for data in items]\n",
    "    max_label = max(labels)\n",
    "    counts = np.zeros(max_label+1, dtype=np.uint16)\n",
    "    \n",
    "    # 统计标注为 0 和 1 的个数\n",
    "    for item in items:\n",
    "        label = int(item[1])\n",
    "        counts[label] += 1\n",
    "\n",
    "    # 输出统计结果\n",
    "    for i, count in enumerate(counts):\n",
    "        print('类别{}: {:>6}'.format(i, count))\n",
    "    print('全部 : {:>6}'.format(sum(counts)))\n",
    "    return sum(counts)\n",
    "\n",
    "def main():\n",
    "    sum_all = 0    \n",
    "    for file in files:\n",
    "        sum_all += info_ann(file)\n",
    "    print(\"  \")\n",
    "    print('总和 : {:>6}'.format(sum_all))\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
